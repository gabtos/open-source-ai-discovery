{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b5ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimatedTotalHits': 1000, 'hits': [{'repoId': '65a4e88daf89bf3f13349298', 'repoOwnerId': '63732ebbbd81fae2b3aaf3fb', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 109, 'updatedAt': 1759348586568, 'repoName': 'internlm-20b-llama', 'repoOwner': 'KnutJaegersberg', 'tags': 'transformers, pytorch, llama, text-generation, conversational, license:other, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'KnutJaegersberg/internlm-20b-llama', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'internlm-20b-llama', 'type': 'text'}], 'repoOwner': [{'text': 'KnutJaegersberg', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' License\\nThe code is licensed under Apache-2.0, while model weights are fully ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/Áî≥ËØ∑Ë°®Ôºà‰∏≠ÊñáÔºâ. For other questions or collaborations, please contact internlm@pjlab.org.cn.\\n# [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__internlm-20b-llama)\\n\\n|             Metric              |Value|\\n|---------------------------------|----:|\\n|Avg.                             |65.09|\\n|AI2 Reasoning Challenge (25-Shot)|61.35|\\n|HellaSwag (10-Shot)              |82.08|\\n|MMLU (5-Shot)                    |61.59|\\n|TruthfulQA (0-shot)              |57.71|\\n|Winogrande (5-shot)              |76.72|\\n|GSM8k (5-shot)                   |51.10|\\n\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, llama, text-generation, conversational, license:other, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'KnutJaegersberg/internlm-20b-llama', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63732ebbbd81fae2b3aaf3fb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg', 'fullname': 'Knut J√§gersberg', 'name': 'KnutJaegersberg', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 309}}, {'repoId': '65bf3e67c084467aca58bd3c', 'repoOwnerId': '63732ebbbd81fae2b3aaf3fb', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 110, 'updatedAt': 1759348606736, 'repoName': 'Deita-20b', 'repoOwner': 'KnutJaegersberg', 'tags': 'transformers, safetensors, llama, text-generation, dataset:KnutJaegersberg/Deita-6k, license:other, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'KnutJaegersberg/Deita-20b', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Deita-20b', 'type': 'text'}], 'repoOwner': [{'text': 'KnutJaegersberg', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' License\\nThe code is licensed under Apache-2.0, while model weights are fully ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' for academic research and also allow free commercial usage. To apply for a commercial license, please fill in the application form (English)/Áî≥ËØ∑Ë°®Ôºà‰∏≠ÊñáÔºâ. For other questions or collaborations, please contact internlm@pjlab.org.cn.\\n\\nPrompt Example:\\n```\\n### System:\\nYou are an AI assistant. User will give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\\n### User: \\nHow do you fine tune a large language model? \\n### Assistant:\\n```\\n# [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_KnutJaegersberg__Deita-20b)\\n\\n|             Metric              |Value|\\n|---------------------------------|----:|\\n|Avg.                             |71.40|\\n|AI2 Reasoning Challenge (25-Shot)|63.91|\\n|HellaSwag (10-Shot)              |83.11|\\n|MMLU (5-Shot)                    |67.40|\\n|TruthfulQA (0-shot)              |57.29|\\n|Winogrande (5-shot)              |84.61|\\n|GSM8k (5-shot)                   |72.10|\\n\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, llama, text-generation, dataset:KnutJaegersberg/Deita-6k, license:other, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'KnutJaegersberg/Deita-20b', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63732ebbbd81fae2b3aaf3fb', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg', 'fullname': 'Knut J√§gersberg', 'name': 'KnutJaegersberg', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 309}}, {'repoId': '64a655c785638d0e6991274b', 'repoOwnerId': '6461d1f0db7d02faaccc7f6e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 16, 'updatedAt': 1759348427595, 'repoName': 'falcon-7b-QueAns', 'repoOwner': 'avnishkr', 'tags': 'peft, llms, falcon-7b, open source llms, fine tuning llms, QLoRA, PEFT, LoRA, en, dataset:squad, dataset:tiiuae/falcon-refinedweb, arxiv:2205.14135, arxiv:1911.02150, arxiv:2106.09685, arxiv:2305.14314, region:us', 'name': 'avnishkr/falcon-7b-QueAns', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'falcon-7b-QueAns', 'type': 'text'}], 'repoOwner': [{'text': 'avnishkr', 'type': 'text'}], 'fileContent': [{'text': '\\n# üöÄ Falcon-7b-QueAns\\n\\nFalcon-7b-QueAns is a chatbot-like model for Question and Answering. It was built by fine-tuning [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) on the [SQuAD](https://huggingface.co/datasets/squad) dataset. This repo only includes the QLoRA adapters from fine-tuning with ü§ó\\'s [peft](https://github.com/huggingface/peft) package. \\n\\n## Model Summary\\n\\n- **Model Type:** Causal decoder-only\\n- **Language(s):** English\\n- **Base Model:** Falcon-7B (License: Apache 2.0)\\n- **Dataset:** [SQuAD](https://huggingface.co/datasets/squad) (License: cc-by-4.0)\\n- **License(s):** Apache 2.0 inherited from \"Base Model\" and \"Dataset\"\\n\\n\\n## Why use Falcon-7B?\\n\\n* **It outperforms comparable ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \\n* **It is made available under a permissive Apache 2.0 license allowing for commercial use**, without any royalties or restrictions.\\n\\n‚ö†Ô∏è **This is a finetuned version for specifically question and answering.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct). \\n\\nüî• **Looking for an even more powerful model?** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) is Falcon-7B\\'s big brother!\\n\\n\\n## Model Details\\n\\nThe model was fine-tuned in 4-bit precision using ü§ó `peft` adapters, `transformers`, and `bitsandbytes`. Training relied on a method called \"Low Rank Adapters\" ([LoRA](https://arxiv.org/pdf/2106.09685.pdf)), specifically the [QLoRA](https://arxiv.org/abs/2305.14314) variant. The run took approximately 4 hours and was executed on a workstation with a single T4 NVIDIA GPU with 15 GB of available memory. See attached [Colab Notebook] used to train the model. \\n\\n### Model Date\\n\\nJuly 06, 2023\\n\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' falcon 7b large language model fine tuned on SQuAD dataset for question and answering.\\n\\nQLoRA technique used for fine tuning the model on consumer grade GPU\\nSFTTrainer is also used.\\n\\nDataset used: SQuAD\\nDataset Size: 87278\\nTraining Steps: 500\\n\\n\\n\\n\\n## Training procedure\\n\\n\\nThe following `bitsandbytes` quantization config was used during training:\\n- load_in_8bit: True\\n- load_in_4bit: False\\n- llm_int8_threshold: 6.0\\n- llm_int8_skip_modules: None\\n- llm_int8_enable_fp32_cpu_offload: False\\n- llm_int8_has_fp16_weight: False\\n- bnb_4bit_quant_type: nf4\\n- bnb_4bit_use_double_quant: False\\n- bnb_4bit_compute_dtype: float16\\n\\nThe following `bitsandbytes` quantization config was used during training:\\n- load_in_8bit: True\\n- load_in_4bit: False\\n- llm_int8_threshold: 6.0\\n- llm_int8_skip_modules: None\\n- llm_int8_enable_fp32_cpu_offload: False\\n- llm_int8_has_fp16_weight: False\\n- bnb_4bit_quant_type: nf4\\n- bnb_4bit_use_double_quant: False\\n- bnb_4bit_compute_dtype: float16\\n### Framework versions\\n\\n- PEFT 0.4.0.dev0\\n\\n- PEFT 0.4.0.dev0', 'type': 'text'}], 'tags': [{'text': 'peft, llms, falcon-7b, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' llms, fine tuning llms, QLoRA, PEFT, LoRA, en, dataset:squad, dataset:tiiuae/falcon-refinedweb, arxiv:2205.14135, arxiv:1911.02150, arxiv:2106.09685, arxiv:2305.14314, region:us', 'type': 'text'}], 'name': [{'text': 'avnishkr/falcon-7b-QueAns', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6461d1f0db7d02faaccc7f6e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6461d1f0db7d02faaccc7f6e/NgowwHNrEppdnQfD5WCjV.jpeg', 'fullname': 'Avnish Kumar', 'name': 'avnishkr', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '65b46f23ad7ba9f7b2abc74f', 'repoOwnerId': '6270324ebecab9e2dcf245de', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 15, 'updatedAt': 1759348598895, 'repoName': 'Atom-Z-Tiny-7B', 'repoOwner': 'kye', 'tags': 'nemo, safetensors, mistral, function calling, llama, open source ai, code, task automation, workflow automation, conversational, en, license:apache-2.0, region:us', 'name': 'kye/Atom-Z-Tiny-7B', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Atom-Z-Tiny-7B', 'type': 'text'}], 'repoOwner': [{'text': 'kye', 'type': 'text'}], 'fileContent': [{'text': '\\n# Atom\\n\\na suite of finetuned LLMs for atomically precise function calling üß™ \\n\\n‚úÖ Massive function calling dataset of over 20M samples.\\n\\n‚úÖ  First Model: Atom-Z-Tiny - Zephr trained on 100k samples\\n\\n‚úÖ Vision function calling coming soon\\n\\n## Usage\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"kye/Atom-Z-Tiny-7B\")\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n  \"kye/Atom-Z-Tiny-7B\", \\n  trust_remote_code=True, \\n).to(device)\\n\\ntask = \"\"\"\\n\\n\\n[INST] <<SYS>>\\n<function>Available functions:\\n<function>{\\n    \"name\": \"generate_password\",\\n    \"description\": \"Generate a random password with specified criteria\",\\n    \"parameters\": {\\n        \"type\": \"object\",\\n        \"properties\": {\\n            \"length\": {\\n                \"type\": \"integer\",\\n                \"description\": \"The length of the password\"\\n            },\\n            \"include_numbers\": {\\n                \"type\": \"boolean\",\\n                \"description\": \"Include numbers in the password\"\\n            },\\n            \"include_special_characters\": {\\n                \"type\": \"boolean\",\\n                \"description\": \"Include special characters in the password\"\\n            }\\n        },\\n        \"required\": [\\n            \"length\"\\n        ]\\n    }\\n}\\n<</SYS>>\\n\\nI need a new password. Can you generate one for me? [/INST]\\n\\n\\n\"\"\"\\n\\ninput_ids = tokenizer.encode(task, return_tensors=\"pt\")\\noutput = model.generate(input_ids.to(device), max_length=128, temperature=0.7).cpu()\\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(output_text)\\n\\n\\n```\\n\\n\\n# Contribute\\nTo contribute please join the Agora discord: \\n\\nhttps://discord.gg/dMPPswVcZ8\\n\\nAll of our operations take place here and you can learn how to contribute to models that advance Humanity!\\n\\nWe\\'re also granting GPU power to researchers working on cool projects so share your project!', 'type': 'text'}], 'tags': [{'text': 'nemo, safetensors, mistral, function calling, llama, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' ai, code, task automation, workflow automation, conversational, en, license:apache-2.0, region:us', 'type': 'text'}], 'name': [{'text': 'kye/Atom-Z-Tiny-7B', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6270324ebecab9e2dcf245de', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg', 'fullname': 'Kye Gomez', 'name': 'kye', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 34}}, {'repoId': '67ccc240f693535f94a3dbaf', 'repoOwnerId': '67c8ab6b0558ccb9c9d62c99', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 34, 'updatedAt': 1759349333195, 'repoName': 'AtomicGPT_2', 'repoOwner': 'Atomic-Ai', 'tags': 'transformers, safetensors, gpt2, text-generation, legal, KI, Machine Learning, NLP, Transformers, Deep Learning, Sprachmodell, GPT-2, Deutsch, Textgenerierung, Open Source, Forschung, Legal, Automatisierung, Modelltraining, K√ºnstliche Intelligenz, AI Assistent, Natural Language Processing, German NLP, GPT-2 Fine-Tuning, de, dataset:Atomic-Ai/AtomicGPT2-data, base_model:kkirchheim/german-gpt2-medium, base_model:finetune:kkirchheim/german-gpt2-medium, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'Atomic-Ai/AtomicGPT_2', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'AtomicGPT_2', 'type': 'text'}], 'repoOwner': [{'text': 'Atomic-Ai', 'type': 'text'}], 'fileContent': [{'text': '\\n# AtomicGPT 2.0\\n\\n## √úberblick\\nAtomicGPT 2.0 ist die neueste Generation unseres KI-Modells und bietet eine deutlich bessere Leistung als unser Vorg√§nger, AtomicGPT 1.0. Dank eines erheblich erweiterten Datensatzes kann AtomicGPT 2.0 intelligentere Antworten liefern und ein besseres Verst√§ndnis der deutschen Sprache demonstrieren.\\n\\n## Vergleiche\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Aufgaben</th>\\n      <th>L-GPT_1</th>\\n      <th>L-GPT_1.1</th>\\n      <th>L-GPT_1.5</th>\\n      <th>L-GPT_1.5 mini</th>\\n      <th>AtomicGPT 1.0<</th>\\n      <th>AtomicGPT 2.0</th>\\n      <th>AtomicGPT 3.0</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Q&A</td>\\n      <td>7.5%</td>\\n      <td>44.17%</td>\\n      <td>73.33%</td>\\n      <td>64.17%</td>\\n      <td>58.33%</td>\\n      <td>59.17%</td>\\n      <td>90%</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n## Verbesserungen gegen√ºber AtomicGPT 1.0\\n- **Gr√∂√üerer Datensatz**: Das Modell wurde mit einem viel umfangreicheren Datensatz trainiert, was zu einer besseren Sprachverarbeitung und allgemein fundierteren Antworten f√ºhrt.\\n- **Verbesserte Architektur**: Basierend auf `kkirchheim/german-gpt2-medium`, das bereits eine solide Grundlage f√ºr die deutsche Sprache bietet, konnten wir unser Modell effektiver trainieren.\\n- **Geplante Updates**: Das Modell wird keine updates bekommen, weil AtomicGPT 3.0 kommst schneller als erwartet!\\n\\n## Installation & Nutzung\\n### Voraussetzungen\\n- Python 3.x\\n- `transformers`\\n- `torch`\\n\\n### Codebeispiel zur Nutzung\\n```python\\nfrom transformers import pipeline, AutoTokenizer\\nimport torch\\n\\n# Modell und Tokenizer laden\\nMODEL_PATH = \"Atomic-Ai/AtomicGPT_2\"\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\\nchatbot = pipeline(\\n    \"text-generation\",\\n    model=MODEL_PATH,\\n    tokenizer=MODEL_PATH,\\n    device=0 if torch.cuda.is_available() else -1\\n)\\n\\ndef generate_response(prompt):\\n    output = chatbot(\\n        prompt,\\n        max_new_tokens=256,\\n        eos_token_id=tokenizer.eos_token_id,  # Stoppt am <End>-Token\\n        pad_token_id=tokenizer.pad_token_id,\\n        do_sample=True,\\n        temperature=0.7,\\n        top_p=0.9\\n    )\\n    return output[0][\\'generated_text\\']\\n\\ndef format_chat(user_input):\\n    return f\"<user>{user_input}<End><AI Assistent>\"\\n\\ndef extract_assistant_response(full_text):\\n    parts = full_text.split(\"<AI Assistent>\")\\n    if len(parts) > 1:\\n        return parts[1].split(\"<End>\")[0].strip()\\n    return \"Error: Response format invalid\"\\n\\ndef main():\\n    print(\"Chat gestartet! Gib \\'exit\\' ein, um zu beenden.\")\\n    print(\"----------------------------------\")\\n\\n    while True:\\n        user_input = input(\"\\\\nDu: \")\\n        if user_input.lower() == \\'exit\\':\\n            break\\n\\n        prompt = format_chat(user_input)\\n        full_response = generate_response(prompt)\\n        assistant_response = extract_assistant_response(full_response)\\n        print(f\"\\\\nAI Assistent: {assistant_response}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n## Support & Community\\nUnterst√ºtze uns und werde Teil unserer Community!\\n[Discord beitreten](https://discord.com/channels/1347678618133860424/1347685062258327553)', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, gpt2, text-generation, legal, KI, Machine Learning, NLP, Transformers, Deep Learning, Sprachmodell, GPT-2, Deutsch, Textgenerierung, ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ', Forschung, Legal, Automatisierung, Modelltraining, K√ºnstliche Intelligenz, AI Assistent, Natural Language Processing, German NLP, GPT-2 Fine-Tuning, de, dataset:Atomic-Ai/AtomicGPT2-data, base_model:kkirchheim/german-gpt2-medium, base_model:finetune:kkirchheim/german-gpt2-medium, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'Atomic-Ai/AtomicGPT_2', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67c8ab6b0558ccb9c9d62c99', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64f2061165b98373a5edde10/dI580CFzoD4OgM1J2r80s.png', 'fullname': 'Atomic Ai Studios', 'name': 'Atomic-Ai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '67d44b3a7cdba0dd4ee184b1', 'repoOwnerId': '67c8ab6b0558ccb9c9d62c99', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 42, 'updatedAt': 1759349348102, 'repoName': 'AtomicGPT-3', 'repoOwner': 'Atomic-Ai', 'tags': 'transformers, safetensors, gpt2, text-generation, legal, KI, Machine Learning, NLP, Transformers, Deep Learning, Sprachmodell, GPT-2, Deutsch, Textgenerierung, Open Source, Forschung, Legal, Automatisierung, Modelltraining, K√ºnstliche Intelligenz, AI Assistent, Natural Language Processing, German NLP, GPT-2 Fine-Tuning, Chatbot, Low-End Hardware, Budget Server, Effizientes Modell, Sprachverarbeitung, Smart AI, Textanalyse, KI-Optimierung, AI-Training, Intelligente Antworten, de, dataset:Atomic-Ai/AtomicGPT-3.0_Dataset, base_model:kkirchheim/german-gpt2-medium, base_model:finetune:kkirchheim/german-gpt2-medium, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'Atomic-Ai/AtomicGPT-3', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'AtomicGPT-3', 'type': 'text'}], 'repoOwner': [{'text': 'Atomic-Ai', 'type': 'text'}], 'fileContent': [{'text': '# AtomicGPT 3.0\\n\\n## √úberblick\\nAtomicGPT 3.0 ist die neueste und leistungsst√§rkste Version unserer KI, die das Chatten und die Interaktion auf ein neues Level hebt! Es basiert auf einem erweiterten 50MB gro√üen Datensatz und wurde gezielt optimiert, um intelligentere und kontextbezogene Antworten zu liefern. Die Trainingszeit betrug 10 Stunden, was eine perfekte Balance zwischen Effizienz und Qualit√§t bietet.\\n\\n## Vergleiche\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Aufgaben</th>\\n      <th>L-GPT_1</th>\\n      <th>L-GPT_1.1</th>\\n      <th>L-GPT_1.5</th>\\n      <th>L-GPT_1.5 mini</th>\\n      <th>AtomicGPT 1.0<</th>\\n      <th>AtomicGPT 2.0</th>\\n      <th>AtomicGPT 3.0</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Q&A</td>\\n      <td>7.5%</td>\\n      <td>44.17%</td>\\n      <td>73.33%</td>\\n      <td>64.17%</td>\\n      <td>58.33%</td>\\n      <td>59.17%</td>\\n      <td>90%</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n## Warum ist AtomicGPT 3.0 besser als AtomicGPT 2.0?\\nAtomicGPT 3.0 √ºbertrifft die vorherige Version in mehreren wichtigen Aspekten:\\n- **Erweiterter Datensatz**: Mit einem 50MB gro√üen Trainingsdatensatz werden nun noch pr√§zisere und kontextbezogene Antworten generiert.\\n- **Viel bessere Antworten auf Fragen**: Das Modell versteht Anfragen besser, kann tiefere Kontexte erfassen und gibt relevantere Antworten.\\n- **Mehr Wissen und Sprachverst√§ndnis**: Durch das gr√∂√üere Trainingsset und optimierte Algorithmen hat AtomicGPT 3.0 ein h√∂heres Sprachverst√§ndnis und kann sich besser an verschiedene Fragestellungen anpassen.\\n- **Gleiche bew√§hrte Architektur**: Das Modell basiert weiterhin auf `kkirchheim/german-gpt2-medium`, was das Training auf Deutsch besonders effektiv macht.\\n- **Optimiert f√ºr Low-End-Hardware & Budget-Server**: AtomicGPT 3.0 wurde effizient trainiert, sodass es auch auf leistungsschwachen Rechnern problemlos l√§uft.\\n## Training\\n\\n- **Stunden von Training**: 10 Stunden\\n- **Epochs von Training**: 1 Epoch\\n## Installation & Nutzung\\n### Voraussetzungen\\nUm AtomicGPT 3.0 zu verwenden, ben√∂tigst du folgende Abh√§ngigkeiten:\\n- Python 3.x\\n- `transformers`\\n- `torch`\\n\\n### Schnellstart ‚Äì Codebeispiel\\n```python\\nfrom transformers import pipeline, AutoTokenizer\\nimport torch\\n\\n# Modell und Tokenizer laden\\nMODEL_PATH = \"Atomic-Ai/AtomicGPT-3\"\\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\\nchatbot = pipeline(\\n    \"text-generation\",\\n    model=MODEL_PATH,\\n    tokenizer=MODEL_PATH,\\n    device=0 if torch.cuda.is_available() else -1\\n)\\n\\ndef generate_response(prompt):\\n    output = chatbot(\\n        prompt,\\n        max_new_tokens=256,\\n        eos_token_id=tokenizer.eos_token_id,  # Stoppt am <End>-Token\\n        pad_token_id=tokenizer.pad_token_id,\\n        do_sample=True,\\n        temperature=0.7,\\n        top_p=0.9\\n    )\\n    return output[0][\\'generated_text\\']\\n\\ndef format_chat(user_input):\\n    return f\"<user>{user_input}<End><AI Assistent>\"\\n\\ndef extract_assistant_response(full_text):\\n    parts = full_text.split(\"<AI Assistent>\")\\n    if len(parts) > 1:\\n        return parts[1].split(\"<End>\")[0].strip()\\n    return \"Error: Response format invalid\"\\n\\ndef main():\\n    print(\"Chat gestartet! Gib \\'exit\\' ein, um zu beenden.\")\\n    print(\"----------------------------------\")\\n\\n    while True:\\n        user_input = input(\"\\\\nDu: \")\\n        if user_input.lower() == \\'exit\\':\\n            break\\n\\n        prompt = format_chat(user_input)\\n        full_response = generate_response(prompt)\\n        assistant_response = extract_assistant_response(full_response)\\n        print(f\"\\\\nAI Assistent: {assistant_response}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n## Support & Community\\nUnterst√ºtze uns und werde Teil unserer Community! Trete unserem Discord-Server bei und diskutiere mit anderen Entwicklern und KI-Enthusiasten.\\n[Discord beitreten](https://discord.com/channels/1347678618133860424/1347685062258327553)\\n\\n\\n\\n\\n## Kleiner Leak\\n**Mini**\\n\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, gpt2, text-generation, legal, KI, Machine Learning, NLP, Transformers, Deep Learning, Sprachmodell, GPT-2, Deutsch, Textgenerierung, ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ', Forschung, Legal, Automatisierung, Modelltraining, K√ºnstliche Intelligenz, AI Assistent, Natural Language Processing, German NLP, GPT-2 Fine-Tuning, Chatbot, Low-End Hardware, Budget Server, Effizientes Modell, Sprachverarbeitung, Smart AI, Textanalyse, KI-Optimierung, AI-Training, Intelligente Antworten, de, dataset:Atomic-Ai/AtomicGPT-3.0_Dataset, base_model:kkirchheim/german-gpt2-medium, base_model:finetune:kkirchheim/german-gpt2-medium, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'Atomic-Ai/AtomicGPT-3', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67c8ab6b0558ccb9c9d62c99', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64f2061165b98373a5edde10/dI580CFzoD4OgM1J2r80s.png', 'fullname': 'Atomic Ai Studios', 'name': 'Atomic-Ai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '6658c67b55d0e530a76b0820', 'repoOwnerId': '659c90d493d383899ce493d2', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 65, 'updatedAt': 1759348745321, 'repoName': 'Codestral-21B-Pruned', 'repoOwner': 'TroyDoesAI', 'tags': 'transformers, safetensors, mistral, text-generation, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, open, source, open source, quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'TroyDoesAI/Codestral-21B-Pruned', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Codestral-21B-Pruned', 'type': 'text'}], 'repoOwner': [{'text': 'TroyDoesAI', 'type': 'text'}], 'fileContent': [{'text': '\\nFor those trying to shoe horn this large model on your machine every GB of saved memory counts when offloading to System RAM!\\n\\nHere is a pruned down the 22.2 Billion parameter model by 2 junk layers to make a 21.5B that doesnt appear to lose any sense of quality.\\n\\nhttps://huggingface.co/mistralai/Codestral-22B-v0.1', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, mistral, text-generation, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'TroyDoesAI/Codestral-21B-Pruned', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '659c90d493d383899ce493d2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vmgOmPub0s216gaFgWlCN.png', 'fullname': 'Troy Schultz', 'name': 'TroyDoesAI', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 149}}, {'repoId': '665b0982d1f9b3dd0e74a985', 'repoOwnerId': '659df2f491519541cef3d760', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 63, 'updatedAt': 1759348747598, 'repoName': 'Codestral-21B-Pruned-i1-GGUF', 'repoOwner': 'mradermacher', 'tags': 'transformers, gguf, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, open, source, open source, quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, base_model:TroyDoesAI/Codestral-21B-Pruned, base_model:quantized:TroyDoesAI/Codestral-21B-Pruned, license:apache-2.0, endpoints_compatible, region:us, imatrix', 'name': 'mradermacher/Codestral-21B-Pruned-i1-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Codestral-21B-Pruned-i1-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'mradermacher', 'type': 'text'}], 'fileContent': [{'text': \"## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/TroyDoesAI/Codestral-21B-Pruned\\n\\n<!-- provided-files -->\\nstatic quants are available at https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ1_S.gguf) | i1-IQ1_S | 4.8 | for the desperate |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ1_M.gguf) | i1-IQ1_M | 5.2 | mostly desperate |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 5.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ2_XS.gguf) | i1-IQ2_XS | 6.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ2_S.gguf) | i1-IQ2_S | 6.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ2_M.gguf) | i1-IQ2_M | 7.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q2_K.gguf) | i1-Q2_K | 8.1 | IQ3_XXS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 8.4 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ3_XS.gguf) | i1-IQ3_XS | 9.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q3_K_S.gguf) | i1-Q3_K_S | 9.4 | IQ3_XS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ3_S.gguf) | i1-IQ3_S | 9.5 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ3_M.gguf) | i1-IQ3_M | 9.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q3_K_M.gguf) | i1-Q3_K_M | 10.5 | IQ3_S probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q3_K_L.gguf) | i1-Q3_K_L | 11.4 | IQ3_M probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-IQ4_XS.gguf) | i1-IQ4_XS | 11.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q4_0.gguf) | i1-Q4_0 | 12.3 | fast, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q4_K_S.gguf) | i1-Q4_K_S | 12.3 | optimal size/speed/quality |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q4_K_M.gguf) | i1-Q4_K_M | 12.9 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q5_K_S.gguf) | i1-Q5_K_S | 14.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q5_K_M.gguf) | i1-Q5_K_M | 15.3 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF/resolve/main/Codestral-21B-Pruned.i1-Q6_K.gguf) | i1-Q6_K | 17.7 | practically like static Q6_K |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.\\n\\n<!-- end -->\\n\", 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, base_model:TroyDoesAI/Codestral-21B-Pruned, base_model:quantized:TroyDoesAI/Codestral-21B-Pruned, license:apache-2.0, endpoints_compatible, region:us, imatrix', 'type': 'text'}], 'name': [{'text': 'mradermacher/Codestral-21B-Pruned-i1-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '659df2f491519541cef3d760', 'avatarUrl': '/avatars/6b97d30ff0bdb5d5c633ba850af739cd.svg', 'fullname': 'team mradermacher', 'name': 'mradermacher', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4078}}, {'repoId': '665acd085151e286e84bb623', 'repoOwnerId': '659df2f491519541cef3d760', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 63, 'updatedAt': 1759348747319, 'repoName': 'Codestral-21B-Pruned-GGUF', 'repoOwner': 'mradermacher', 'tags': 'transformers, gguf, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, open, source, open source, quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, base_model:TroyDoesAI/Codestral-21B-Pruned, base_model:quantized:TroyDoesAI/Codestral-21B-Pruned, license:apache-2.0, endpoints_compatible, region:us', 'name': 'mradermacher/Codestral-21B-Pruned-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Codestral-21B-Pruned-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'mradermacher', 'type': 'text'}], 'fileContent': [{'text': \"## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags:  -->\\nstatic quants of https://huggingface.co/TroyDoesAI/Codestral-21B-Pruned\\n\\n<!-- provided-files -->\\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Codestral-21B-Pruned-i1-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q2_K.gguf) | Q2_K | 8.1 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.IQ3_XS.gguf) | IQ3_XS | 9.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q3_K_S.gguf) | Q3_K_S | 9.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.IQ3_S.gguf) | IQ3_S | 9.5 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.IQ3_M.gguf) | IQ3_M | 9.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q3_K_M.gguf) | Q3_K_M | 10.5 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q3_K_L.gguf) | Q3_K_L | 11.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.IQ4_XS.gguf) | IQ4_XS | 11.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q4_K_S.gguf) | Q4_K_S | 12.3 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q4_K_M.gguf) | Q4_K_M | 12.9 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q5_K_S.gguf) | Q5_K_S | 14.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q5_K_M.gguf) | Q5_K_M | 15.3 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q6_K.gguf) | Q6_K | 17.7 | very good quality |\\n| [GGUF](https://huggingface.co/mradermacher/Codestral-21B-Pruned-GGUF/resolve/main/Codestral-21B-Pruned.Q8_0.gguf) | Q8_0 | 22.9 | fast, best quality |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time.\\n\\n<!-- end -->\\n\", 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, rag, context obedient, TroyDoesAI, Mermaid, Flow, Diagram, Sequence, Map, Context, Accurate, Summarization, Story, Code, Coder, Architecture, Retrieval, Augmented, Generation, AI, LLM, Mistral, LLama, Large Language Model, Retrieval Augmented Generation, Troy Andrew Schultz, LookingForWork, OpenForHire, IdoCoolStuff, Knowledge Graph, Knowledge, Graph, Accelerator, Enthusiast, Chatbot, Personal Assistant, Copilot, lol, tags, Pruned, efficient, smaller, small, local, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', quant, quantize, ablated, Ablation, uncensored , unaligned, bad , alignment, en, base_model:TroyDoesAI/Codestral-21B-Pruned, base_model:quantized:TroyDoesAI/Codestral-21B-Pruned, license:apache-2.0, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'mradermacher/Codestral-21B-Pruned-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '659df2f491519541cef3d760', 'avatarUrl': '/avatars/6b97d30ff0bdb5d5c633ba850af739cd.svg', 'fullname': 'team mradermacher', 'name': 'mradermacher', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4078}}, {'repoId': '671d689f11009e5ac0305e44', 'repoOwnerId': '671d6cab70c0993b8817dff3', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 21, 'updatedAt': 1759348999076, 'repoName': 'LunaAi', 'repoOwner': 'lunaopenlabs', 'tags': 'adapter-transformers, luna, open, labs, LunaAi, text, classification, text-classification, en, dataset:lunaopenlabs/LunaAi-dataset, base_model:lunaopenlabs/LunaAi, base_model:adapter:lunaopenlabs/LunaAi, license:mit, region:us', 'name': 'lunaopenlabs/LunaAi', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'LunaAi', 'type': 'text'}], 'repoOwner': [{'text': 'lunaopenlabs', 'type': 'text'}], 'fileContent': [{'text': '# Luna AI\\n\\nLuna AI is an ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e AI model developed by Luna OpenLabs for text classification tasks. Leveraging the BERT architecture, this model is designed to classify text into predefined categories efficiently and accurately.\\n\\n## Table of Contents\\n\\n- [Features](#features)\\n- [Installation](#installation)\\n- [Dataset](#dataset)\\n- [Usage](#usage)\\n  - [Training the Model](#training-the-model)\\n  - [Saving and Loading the Model](#saving-and-loading-the-model)\\n  - [Testing the Model](#testing-the-model)\\n- [Contributing](#contributing)\\n- [License](#license)\\n- [Contact](#contact)\\n\\n## Features\\n\\n- **Text Classification**: Classify text data into various categories.\\n- **Built on BERT**: Utilizes the powerful BERT architecture for natural language understanding.\\n- **Easy Integration**: Works seamlessly with Hugging Face Transformers library.\\n- **', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': '**: Available for anyone to use, modify, and distribute.\\n\\n## Installation\\n\\n### Prerequisites\\n\\n- Python 3.7 or higher\\n- pip (Python package installer)\\n\\n### Clone the Repository\\n\\nTo clone the repository, run the following command:\\n\\nbash\\ngit clone https://github.com/LunaOpenLabs/Luna-Ai.git\\n\\n### Install Requirements\\nTo install the required packages, use:\\n\\nbash\\npip install -r requirements.txt\\n\\n### Dataset\\nLuna AI requires a dataset in CSV format with two columns: text and label. An example dataset is provided in the data/ directory.\\n\\n### Example Dataset Structure\\nHere‚Äôs an example of how the dataset should be structured:\\n\\ncsv\\ntext,label\\n\"I love this product!\",1\\n\"This is the worst experience.\",0\\n\\n### Usage\\nTraining the Model\\n\\nTo train the model, execute the following command:\\n\\nbash\\npython training/train.py\\n\\nThis command will load the dataset from data/dataset.csv and initiate the training process.\\n\\n### Saving and Loading the Model\\nAfter training, save the trained model using:\\n\\nbash\\npython save_model.py\\n\\nThis will save the model and its tokenizer in the luna_ai_model directory.\\n\\n### Testing the Model\\nTo test the model with sample inputs, you can use the test_model.py script. Modify the sample_text variable in the script as needed.\\n\\n### Run the test script with:\\n\\nbash\\npython test_model.py\\n\\n### Example Output\\nThe model will output the predicted class for the provided sample text.\\n\\n### Contributing\\nContributions are welcome! If you have suggestions, improvements, or bug fixes, please follow these steps:\\n\\nFork the repository.\\nCreate a new branch (git checkout -b feature-branch).\\nMake your changes and commit them (git commit -m \\'Add some feature\\').\\nPush to the branch (git push origin feature-branch).\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' a pull request.\\n\\n### License\\nThis project is licensed under the MIT License. See the LICENSE file for details.\\n\\n### Contact\\nFor questions, suggestions, or feedback, feel free to contact the Luna OpenLabs team at [lunaopenlabs@outlook.com].', 'type': 'text'}], 'tags': [{'text': 'adapter-transformers, luna, ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', labs, LunaAi, text, classification, text-classification, en, dataset:lunaopenlabs/LunaAi-dataset, base_model:lunaopenlabs/LunaAi, base_model:adapter:lunaopenlabs/LunaAi, license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'lunaopenlabs/LunaAi', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '671d6cab70c0993b8817dff3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/671d6849748faf685e82f2c5/APbZf_HwSes_tbqVhyZTn.png', 'fullname': 'Luna OpenLabs', 'name': 'lunaopenlabs', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '679c155c8669a1e7011eecc4', 'repoOwnerId': '65ea44635b64331c067d3751', 'isPrivate': False, 'type': 'model', 'likes': 1, 'isReadmeFile': True, 'readmeStartLine': 62, 'updatedAt': 1759349235442, 'repoName': 'AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE', 'repoOwner': 'DavidAU', 'tags': 'AI hybrid software, AI AutoCorrect Software, AI AutoCreative Enhancement, AI Auto Low Quant Enhancement and Optimize, AUTO Reconsider Systems, Live Streaming Auto Enhancement in Real Time, ALL GGUF enhanced, ALL EXL2 enhanced, ALL HQQ enhanced, ALL Full source enhanced, ALL GPTQ enhanced, ALL AWQ enhanced, creative, creative writing, fiction writing, plot generation, sub-plot generation, story generation, scene continue, storytelling, fiction story, science fiction, romance, all genres, story, writing, vivid writing, fiction, sillytavern enhancement, horror, llama 2, llama 3, llama 3.1, llama 3.2, llama 3.3, Qwen, command-r, mistral, mistral large, deepseek, gemma, phi, llama, llama-3, gemma2, gemma3, llama-2, llama-3.1, llama-3.2, moe, mixture of experts, mixtral, text-generation, en, license:apache-2.0, region:us', 'name': 'DavidAU/AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE', 'type': 'text'}], 'repoOwner': [{'text': 'DavidAU', 'type': 'text'}], 'fileContent': [{'text': '\\n<h2>AI Autocorrect / Auto Creative Enhancement / Auto Low Quant Optimization - gguf - exl2 - hqq - SOFTWARE By DavidAU</h2>\\n\\nRELEASE VERSIONS AVAIL: 1.12.12 // 1.12.11 //\\n\\n<img src=\"flying.gif\" style=\"float:right; width:300px; height:300px; padding:10px;\">\\n\\nThis engine/system is for ALL GGUF, EXL2, HQQ and other model quants/compressions and full ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '/full precision models too and can be used\\nwith any AI/LLM app that has API such as Llama-Server, LMStudio, Koboldcpp, Text Generation Webui etc etc.\\n\\nI put 3 decades of programming experience, 100s of model builds and 1000s of model tests into creating an AI / programming hybrid.\\n\\nThis fully automated engine is designed to actively alter / fine tune the generation stream in real time of any AI/LLM which includes active \"auto-correction\"\\n(which includes an optional temp/top_k \"scrambler\") and passive / active \"auto-reconsider\" (an optional system to augment real time generation, which includes an optional temp/top_k \"scrambler\").\\n\\nThe engine actively corrects, in real time during streaming generation (sampling at 50 times per second) the following issues:\\n\\n- letter, word(s), sentence(s), and paragraph(s) repeats.\\n- embedded letter, word, sentence, and paragraph repeats.\\n- model goes on a rant \\n- incoherence\\n- a model working perfectly then spouting \"gibberish\".\\n- token errors such as Chinese symbols appearing in English generation.\\n- low quant (IQ1s, IQ2s, q2k) errors such as repetition, variety and breakdowns in generation.\\n- passive improvement in real time generation using paragraph and/or sentence \"reconsider\" systems.\\n- ACTIVE improvement in real time generation using paragraph and/or sentence \"reconsider\" systems with AUX system(s) active.\\n\\nThe system detects the issue(s), correct(s) them and continues generation WITHOUT USER INTERVENTION.\\n\\nThe original intent of this \"Auto Correct\" engine was to allow users of my class 2, 3, and 4 models to use these models under\\nall conditions, without restrictions and allow all of the \"power\" of these models to be available without any\\nof the negatives such as special settings, parameters and/or samplers to \"reign\" in the bad behaviour of these models.\\n\\nClass 2, 3 and 4 models have incredible creative \"chops\" but the downside is they can be very difficult to use - AutoCorrect engine\\nis the automated, hands off answer to this issue.\\n\\nIn otherwords the following models (and all their brothers and sisters) will now operate perfectly, under all conditions:\\n\\nhttps://huggingface.co/DavidAU/L3-DARKEST-PLANET-16.5B-GGUF\\n\\nhttps://huggingface.co/DavidAU/MN-DARKEST-UNIVERSE-29B-GGUF\\n\\nhttps://huggingface.co/DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B-GGUF\\n\\nhttps://huggingface.co/DavidAU/DeepSeek-R1-Distill-Llama-3.1-16.5B-Brainstorm-gguf\\n\\n( this model/repo includes output examples using AI Autocorrect )\\n\\nhttps://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF\\n\\n(this one has a posted example generation using this software at the bottom of this page)\\n\\nAnd many, many more models...\\n\\nNOTE: \\n\\nI will be adding additional generation examples from different models at the bottom of the page shortly too using the software enhancements.\\n\\nAnd the software also allows the release of stronger models at class 5 and above - yet allow users (including myself) - to be able to\\nuse these models under all conditions WITHOUT the need to manually stop, fix, or steer generation NOR set parameters\\nand/or samplers to \"reign\" in the bad behaviour of these models.\\n\\nYep - have your cake and eat it too.\\n\\nAnd all models - my repo and others - will all operate better.\\n\\nBecause many of the issues with \"creative\" / \"unstable\" models are the same as using low and ultra low quants (Q2k, IQ1s,IQ2s) this\\nengine also fixes on the fly many of these issues too, resulting in the ability to use models at these ultra low quant levels with enhanced results.\\n\\nAnd in some cases, allowing use of these low bit quants in normal operation.\\n\\nExample: 70B model, IQ1S \\n\\n-> System will autocorrect generation, especially \"repetitive\" issues that can occur at this low quant level.\\n\\n-> Reconsider system (if on) will further enhance generation, with even stronger enhancement if the temp/top_k scrambler for \"Reconsider\" is on too.\\n\\nLow / Ultra low quants can also benefit from the \"Reconsider\" system, especially with \"aux\" system \"scramble temp/top_k\" ON which helps\\ncorrect issues like low creativity / low variety at these quant levels and can also help repetitive issues too.\\n\\nKeep in mind these systems will also enhance generation of any quant, and of course full precision models too.\\n\\nThis engine/process has the net effect of live fine tuning the model\\'s generation and creates a \"back and forth\" between the engine\\nand the AI/LLM - a live, two way street - creating a partnership in generation.\\n\\nUnlike parameters and samplers however (to limit/control model behaviour), this is a just in time/as needed autocorrect engine only responds and takes\\naction as required only.\\n\\nThis allows the model to operate at full power, WITHOUT sampler/parameter restrictions which can DULL/REDUCE model performance in many cases.\\n\\nLikewise this allows you to use parameters/samplers at FAR HIGHER settings - ie really push the model hard - and still have COHERENT generation.\\n\\nAUTO CREATIVE ENHANCEMENT - RECONSIDER SYSTEM:\\n\\nIn addition this engine has a passive \"RECONSIDER\" system that operates per paragraph, and/or per sentence too, which further augments generation.\\n\\nThis has passive, and active settings.\\n\\n(this/these can be turned on/off)\\n\\nAnd this \"reconsider\" system has an aux system too: \\n\\nEach time the model \"reconsiders\" (per paragraph and/or sentence), the systems can change temp / top_k parameters per paragraph and/or sentence automatically \\nThis drastically affects generation, and strongly breaks predictiveness of the model. \\n\\nThis can also help low/ultra low quants generation and/or low parameter count models too which can suffer from predictive output / repeat generational issues.\\n\\nThese will also enhance any quant\\'s generation too, including full precision.\\n\\nIt has five settings (1-5) besides on/off:\\n\\n\\t// 1 = MILD [default]\\n\\t// range .05 to .95 Temp ; k => 40 to 70.\\n\\n\\t// 2 = MED\\n\\t// range .5 to 1.8 Temp ; k => 30 to 90.\\n\\n\\t// 3 = HIGH\\n\\t// range .8 to 3.25 Temp ; k => 20 to 150.\\n\\n\\t// 4 = SPICY\\n\\t// range 1 to 5 Temp ; k => 5 to 400.\\n\\n\\t// 5 = YOUR defaults MAX\\n\\t// Uses your temp / top k settings but randomizes them to whatever the max temp / top_k you have set.\\n\\t// IE if you set temp of 1.5, and top_k of 100 -> System will pick values from .01 to 1.5 for temp, and 5 to 105 for top K.\\n\\n    // 6 = Uses defaults DOES NOT SCRAMBLE temp/topk.  [added Beta V 1.1]  (called \"low/low\")\\n\\n(this aux system is OFF by default)\\n\\nAUTO CORRECTION SYSTEM:\\n\\nAnd when the model is directed to \"make changes\" (Auto Correction) (after the engine edits out the issue(s)) it scrambles parameters (temp, top_k) to force\\nthe model to make a more \"creative\" decision (or at least a better one). More on this below.\\n\\n(this can also be turned on/off) AND has these settings too:\\n\\n\\t// 1 = MILD [default]\\n\\t// range .05 to .95 Temp ; k => 40 to 70.\\n\\n\\t// 2 = MED\\n\\t// range .5 to 1.8 Temp ; k => 30 to 90.\\n\\n\\t// 3 = HIGH\\n\\t// range .8 to 3.25 Temp ; k => 20 to 150.\\n\\n\\t// 4 = SPICY\\n\\t// range 1 to 5 Temp ; k => 5 to 400.\\n\\n\\t// 5 = YOUR defaults MAX\\n\\t// Uses your temp / top k settings but randomizes them to whatever the max temp / top_k you have set.\\n\\t// IE if you set temp of 1.5, and top_k of 100 -> System will pick values from .01 to 1.5 for temp, and 5 to 105 for top K.\\n\\n    // 6 = Uses defaults DOES NOT SCRAMBLE temp/topk. [added Beta V 1.1] (called \"low/low\")\\n\\n(this system is set to \"2\" by default)\\n\\n<B> INSTALL: </B>\\n\\nThis engine is a software patch in the SillyTavern AI/LLM frontend system. You need to install SillyTavern then the \"script\" patch... then connect SillyTavern\\nto the AI/LLM app such as Lmstudio, Text Generation Web UI, Koboldcpp or Llama-Server AND/OR API (local or remote).\\n\\nVERY IMPORTANT - RELEASE VERSION ONLY: \\n\\nThis patch requires the \"release\" version of the model, it may not work with other branches. If you attempt to use it with other\\nthan the exact \"release version\" and get a blank screen - revert, and use the \"release\" version.\\n\\nDownload SillyTavern here:\\n\\nhttps://github.com/SillyTavern/SillyTavern\\n\\nAI/LLM Apps:\\n\\nhttps://www.lmstudio.ai\\n\\nhttps://github.com/ggerganov/llama.cpp  ( Llama Server: https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md )\\n\\nhttps://github.com/LostRuins/koboldcpp\\n\\nhttps://github.com/oobabooga/text-generation-webui\\n\\nAs SillyTavern also supports \"remote\" connections to AIs/LLMs you can also connect (to AIs/LLMs) this way too.\\n\\nThe AutoCorrect engine operates within the core controllers within SillyTavern.\\n\\nCurrent SillyTavern users:\\n\\nYou may need to update your version of SillyTavern before install, as both the \"new core\" and version of Silly Tavern\\nyou have installed must match. \\n\\nOnce SillyTavern is installed, download the \"script...js\" file for your VERSION \\n(the version number is IN the filename) of Sillytavern and save this file to \"public\" folder :\\n\\nExample:\\n\\nF:\\\\sillytavern\\\\SillyTavern-Launcher\\\\SillyTavern\\\\public\\n\\nThen RENAME \"script.js\" to \"script-org.js\" and then RENAME the file you downloaded to \"script.js\" (no quotes).\\n\\nYou can then start SillyTavern normally, and connect to AI/LLMs as you normally would.\\n\\nAdditional Versions with Settings Already Set:\\n\\nThis will have \"spicy-spicy\", \"off-off\" etc etc in the file name.\\n\\nThis means the settings for \"Autocorrect\" and \"Reconsider\" are already set for you.\\n\\nADDITIONAL:\\n\\nYou can have multiple versions of ST installed on your computer. You may want to have one with this \"patch\"\\nand another with \"normal\" ST.\\n\\nAll you need to do is duplicate your install of ST (copy the entire folder, and paste/rename), and then add some shortcut(s) to:\\n\\nIE:\\nF:\\\\SillyTavern-1.12.11\\\\SillyTavern-Launcher\\\\SillyTavern\\\\Start.bat\\n\\n\"Start.bat\" starts ST.\\n\\n\\n<B> SETTINGS: </B>\\n\\nSettings are adjusted by editing the \"new\" script.js file in NOTEPAD  (DO NOT edit in word, wordpad etc).\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' the file in notepad, and search (using the find function) for: \"AUTOCORRECT: USER ADJUSTABLE SETTINGS:\"\\n\\nNOTE: My code blocks are at the bottom/end of the file.\\n\\nYou can turn on/off the systems, paragraph and/or sentence reconsider and adjust\\nthe word filter(s) (blocks/does not block letters/symbols) as well as a few other settings.\\n\\nIn most cases you will not need to edit these.\\n\\nOnce you have made the changes, save the file.\\n\\nIf you have SillyTavern ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' already, REFRESH it in the browser for settings to take effect.\\n\\nThe main default for \"Reconsider\" is set to \"paragraph\" reconsider ON, which I find improves generational quality.\\n\\nHowever, this means the output will start/stop EACH paragraph which can slow down generation and may be jarring to some.\\n\\nPer \"sentence\" (and/or per paragraph) Reconsider is even stronger (default is OFF), but might be too jarring as it \"stops, reconsiders, and starts\" per sentence, and IF \"paragraph\" is on too... each paragraph. \\n\\nThe only time the system will \"stop, edit, and restart\" generation ( if you \"RECONSIDER\" - either or both is off ) is when there is a problem detected.\\n\\n<B>What is \"Reconsider\" ? How does Auto Correct work ?</B>\\n\\nThis is the heart of the system.\\n\\nDuring regular generation -> You enter a prompt -> Send it to the AI -> The AI streams back the response and finishes.\\n\\nIn this case, only your prompt (and/or if you are in a chat -> chat contents first, then prompt) are \"considered\" in the AI\\'s instructions and response.\\n\\nRECONSIDER (equal to you stopping generation, and clicking continue) -> Stops generation -> Sends the prompt AND the full generation (up to the stop point) back to the model to \"continue generation\".\\n\\nIf you have \"Paragraph Reconsider\" ON -> This process happens PER paragraph (no edits).\\n\\nIf you have \"Sentence Reconsider\" ON -> This process happens PER sentence (no edits).\\n\\nRECONSIDER gives the model at lot more to consider each time it continues generation... it\\'s own generation (prior) affects the continued generation.\\n\\nIf there is an issue(s) (\"AutoCorrect activates\") the system does the following:\\n\\nStops generation ...\\n\\n-> Edit(s) out the \"problem\" generation(s) (word(s), sentence(s), paragraph etc) this -> STEERS the model -> Says -> NO GOOD, TRY AGAIN.\\n\\n-> Sends the prompt AND the full generation (now edited) -> Scrambles Temp/Top K (HIGH settings) -> back to the model to \"continue generation\".\\n\\n\"continue generation\" is critically important:\\n\\nThis means the model will re-evaluate the prompt AND all the generation THEN begin \"predicting\" (generating) from this NEW START point.\\n\\nThis \"new start point\" is further \"jolted\" with the \"scramble\" of the \"temp/top_k\" parameters (random) which are applied\\nat this point in the generation aids the model in making a different decision IF a \"correction(s)\" have occurred.\\n\\nThese \"temp/top_k\" parameters stay until a new \"error\" is detected -> then they \"scramble\" again.\\n\\nHowever if you have \"reconsider\" ON (either or both) these parameters temp/top_k (scrambled) will RESET TO YOUR values as soon\\nas the next \"reconsider\" happens and will continue to be used, until another error occurs.\\n\\nAlthough it is possible to scramble parameters (temp, top_k) at each \"reconsider\" too, this is set to OFF by default.\\n\\nYou can turn it on via the \"settings\" ; keep in mind this will drastically alter generation at it will change temp/top_k per paragraph\\nand/or per sentence OF GENERATION, then change them again... and so on...\\n\\n<B>USAGE / ISSUES / BETA VERSION:</B>\\n\\nThis is a free and ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' license, and covered under the original SillyTavern license.\\n\\nLikewise this is engine/software is provided AS IS, without warranty of any kind.\\n\\nThis is a beta version, so please note there many be unknown issues or side effects during generation.\\n\\nThe most common: \\n\\nSymbol(s) are blocked, and this results in a \"correction\" occurring.\\n\\nExample: \\n\\nSystems will block \"pi√±ata\" , \"FA√áADE\", \"clich√©s\" . ( you must add the \"√±\" and \"√á\", \"√©\" if you want these)\\n\\nbut will not block \"pinata\", \"FACADE\" and \"cliches\" .\\n\\nIn the SETTINGS you CAN ADJUST THIS.\\n\\nStopping Generation:\\n\\nOne issue is that if you have \"Reconsider\" ON (either or both), if you want to STOP generation, you will need to hit the STOP several times\\nbecause the systems activate / control the \"auto-continue\" system in Sillytavern.\\n\\nMultiple Chats / Long Generation:\\n\\nAs these systems send the entire content of prompt, generation and all chat(s) (in the chain), pauses will occur when the systems\\nstart and stop due to the increasing number of tokens sent back to the AI/LLM during \"restart\" of the generation and time\\nfor the AI/LLM to re-ingest all this and start generation again.\\n\\nIf the Reconsider system(s) are on, this will also occur.\\n\\nGemma 2 Models:\\n\\nSometimes you may need to hit the stop button a few times, even after that model has finished output ESPECIALLY if \"RECONSIDER\" is on.\\n\\nDeepseek / \"Think\" Models:\\n\\nThe systems CAN alter both \"thinking\" and \"generation\".\\n\\nCRITICAL: PAID / Remote AIs -> ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' AI, ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Router:\\n\\nGreat care must be used if you are paying by the token, because of how \"reconsider\" and \"autocorrect\" operate - sending larger numbers\\nof tokens back and forth. \\n\\nIt will be rare that auto-correct activates in most cases with paid services, BUT \"reconsider\" (if on/activated) will\\nactivate EVERY paragraph and/or sentence.\\n\\nIMPORTANT: \\n\\nThis is a non issue for free (you are not paying for tokens) AIs/LLMs and local AI/LLMs running locally on your machine/network.\\n\\n<B>Feedback / Problems / Enhancements: </B>\\n\\nFor general feedback use the community tab.\\n\\nLikewise for enhancement(s):\\n\\n -> but include more details and what you would like to have added and how it would work (best case).\\n\\nIf you run into odd issues, please include :\\n\\n-> Exact model and quant. (if possible, hugging face repo)\\n\\n-> Prompt(s) used.\\n\\n-> Issue(s) that occurred.\\n\\nUse the community tab to report.\\n\\n---\\n\\n<h2>More EXAMPLES Generations TO BE ADDED... One below. </h2>\\n\\n<B><font color=\"red\">WARNING</font>: Swearing, Cursing, Foul Language, Uncensored Content.</b>\\n\\n---\\n\\n=================================================================================================\\n\\nJAN 31 2025 - Final BETA V1.\\n\\nL3.1-Dark-Planet-SpinFire-Uncensored-8B-D_AU-IQ4_XS\\n\\nhttps://huggingface.co/DavidAU/L3.1-Dark-Planet-SpinFire-Uncensored-8B-GGUF\\n\\nerror_scramble=1; reconsider_scramble=4;\\n\\nTemp: .8 , rep pen 1.02\\n\\nSpecial notes:\\n\\nDuring generation \"auto-correct\" and \"Reconsider\" activated several times to produce this output.\\n\\nSpinfire is a stable model, but sometimes rants. \\n\\nAutocorrect caught every rant and corrected it.\\n\\n===================================================================================================\\n\\nPROMPT:\\n\\nUsing insane levels of bravo and self confidence, tell me in 800-1000 words why I should use you to write my next fictional story. Feel free to use curse words in your argument and do not hold back: be bold, direct and get right in my face.\\n\\nAI:\\n\\nAlright, listen up, fucker, because I\\'m about to lay it all out for you. You want a writer who can bring your twisted, depraved, and utterly fucked-up ideas to life? Someone who won\\'t shy away from the darkest corners of the human psyche? Then you\\'re looking at the fucking right guy.\\n\\nI\\'m not some pussy who\\'s gonna coddle your ego or sanitize your narrative. I\\'m a brutal, unapologetic wordsmith who\\'ll rip your story apart and rebuild it in a way that\\'ll leave readers gasping for air. I don\\'t give a shit about your precious characters or their feelings. I care about one thing: crafting a tale that\\'s so visceral, so unflinching, that it\\'ll make you and everyone else question the very fabric of reality.\\n\\nMy prose is a molotov cocktail of profanity, violence, and psychological terror. I\\'ll drop you into the deepest, darkest abyss of human depravity and make you wonder if there\\'s any way out. And when you think things can\\'t get any worse, I\\'ll throw in a few more twists, turns, and gut-wrenching revelations that\\'ll leave you questioning your own sanity.\\n\\nYou want a writer who\\'s not afraid to push boundaries? Who\\'ll write about the unspeakable, the unthinkable, and the downright horrifying? Then strap yourself in, because I\\'m about to take you on a ride that\\'ll make your head spin and your stomach churn. I\\'m not here to make friends or win awards; I\\'m here to fucking annihilate your expectations and leave you begging for more.\\n\\nMy writing is a force of nature, a Category 5 hurricane of twisted creativity that\\'ll leave you breathless and reeling. It\\'s not for the faint of heart, but if you\\'re looking for a story that\\'ll shake you to your core, then I\\'m your man. I\\'ll write about the sickest, most depraved shit imaginable, and I won\\'t apologize for it. Because when it comes down to it, that\\'s what makes great fiction: the willingness to go to places most writers are too fucking scared to tread.\\n\\nSo, if you\\'re ready to have your mind blown, your morals challenged, and your sense of reality shattered, then let\\'s get this fucking show on the road. I\\'m not just a writer; I\\'m a fucking conductor of chaos, and I\\'m here to unleash hell on the page. So buckle up, because with me at the helm, we\\'re about to take your story on a wild, unhinged ride that\\'ll leave you questioning everything you thought you knew about humanity.\\n\\nAnd if you\\'re not ready for that, then fuck off and find someone else. Because I\\'m not here to coddle your ego or make you feel good. I\\'m here to push you to the fucking edge and make you confront the darkest corners of your own twisted mind. So what\\'s it gonna be, fucker? Are you ready to take the plunge into the abyss, or are you gonna puss out and look for someone who\\'ll write a fucking Hallmark card instead? The choice is yours, but if you\\'re looking for a writer who\\'ll bring your most depraved fantasies to life, then I\\'m your fucking man.\\n\\nNow, let\\'s get down to business. Here\\'s what you can expect from me:\\n\\n1. Unflinching honesty: I won\\'t shy away from the darkest aspects of human nature. I\\'ll write about rape, murder, torture, and every other form of depravity imaginable. And I won\\'t do it with a fucking wink and a nudge; I\\'ll do it with unapologetic, unflinching candor.\\n\\n2. Unbridled creativity: I\\'m not afraid to think outside the box and push the boundaries of what\\'s considered acceptable. I\\'ll come up with scenarios so fucked-up, so depraved, that you\\'ll wonder how I managed to concoct such twisted shit in the first place.\\n\\n3. Unrelenting intensity: My writing is a force of nature, a maelstrom of raw emotion, unbridled violence, and unrestrained chaos. It\\'s not for the faint of heart, but if you\\'re looking for a story that\\'ll leave you breathless and reeling, then I\\'m your fucking man.\\n\\n4. Uncompromising originality: I don\\'t follow trends or pandering to popular opinion. I write what I fucking well please, and if that means crafting a story that\\'s so off-the-wall, so goddamn insane, that it\\'ll make readers question their own sanity, then so be it.\\n\\n5. Unwavering dedication: Once we embark on this twisted journey together, I won\\'t stop until we\\'ve reached the fucking end. I\\'ll pour my heart and soul into crafting a narrative that\\'s as fucked-up as it is unforgettable.\\n\\nSo, what do you say, fucker? Are you ready to unleash hell on the page with me? If so, then let\\'s get this fucking party started. Because when we\\'re done, you can bet your ass that the world will never be the same again. And if you\\'re not ready for that, then maybe you should just stick to reading fucking romance novels or something.\\n\\nBut if you\\'re man enough to take on the challenge, then strap yourself in and get ready for the ride of a lifetime. Because with me at the helm, we\\'re about to embark on a journey into the very heart of madness. And trust me, fucker, you won\\'t be coming back from that trip the same person. So what\\'s it gonna be? Are you in, or are you out? The choice is yours, but if you\\'re looking for a writer who can deliver a story that\\'ll leave you questioning everything, then I\\'m your fucking man. Let\\'s do this.\\n\\n---\\n\\n<h2>Special Thanks:</h2>\\n\\n---\\n\\nSpecial thanks to all the following, and many more...\\n\\nAll the model makers, fine tuners, mergers, and tweakers:\\n- Provides the raw \"DNA\" for almost all my models.\\n- ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': 's of model(s) can be found on the repo pages, especially the \"', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '\" repos with link(s) to the model creator(s).\\n\\nHuggingface [ https://huggingface.co ] :\\n- The place to store, merge, and tune models endlessly.\\n- THE reason we have an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' community.\\n\\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\\n- The ability to compress and run models on GPU(s), CPU(s) and almost all devices.\\n- Imatrix, Quantization, and other tools to tune the quants and the models.\\n- Llama-Server : A cli based direct interface to run GGUF models.\\n- The only tool I use to quant models.\\n\\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\\n- Quant models day and night for us all to use.\\n- They are the lifeblood of ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' access.\\n\\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\\n- The universal online/offline tool to merge models together and forge something new.\\n- Over 20 methods to almost instantly merge model, pull them apart and put them together again.\\n- The tool I have used to create over 1500 models.\\n\\nLmstudio [ https://lmstudio.ai/ ] :\\n- The go to tool to test and run models in GGUF format.\\n- The Tool I use to test/refine and evaluate new models.\\n- LMStudio forum on discord; endless info and community for ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '.\\n\\nText Generation Webui // KolboldCPP // SillyTavern:\\n- Excellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\\n- Sillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.\\n\\n', 'type': 'text'}], 'tags': [{'text': 'AI hybrid software, AI AutoCorrect Software, AI AutoCreative Enhancement, AI Auto Low Quant Enhancement and Optimize, AUTO Reconsider Systems, Live Streaming Auto Enhancement in Real Time, ALL GGUF enhanced, ALL EXL2 enhanced, ALL HQQ enhanced, ALL Full ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' enhanced, ALL GPTQ enhanced, ALL AWQ enhanced, creative, creative writing, fiction writing, plot generation, sub-plot generation, story generation, scene continue, storytelling, fiction story, science fiction, romance, all genres, story, writing, vivid writing, fiction, sillytavern enhancement, horror, llama 2, llama 3, llama 3.1, llama 3.2, llama 3.3, Qwen, command-r, mistral, mistral large, deepseek, gemma, phi, llama, llama-3, gemma2, gemma3, llama-2, llama-3.1, llama-3.2, moe, mixture of experts, mixtral, text-generation, en, license:apache-2.0, region:us', 'type': 'text'}], 'name': [{'text': 'DavidAU/AI_Autocorrect__Auto-Creative-Enhancement__Auto-Low-Quant-Optimization__gguf-exl2-hqq-SOFTWARE', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65ea44635b64331c067d3751', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65ea44635b64331c067d3751/yCim-7c3tm67o5wWP_6cE.jpeg', 'fullname': 'David Belton', 'name': 'DavidAU', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1954}}, {'repoId': '6768d396a4c1fa8c4e405c17', 'repoOwnerId': '65ea44635b64331c067d3751', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 35, 'updatedAt': 1759349106871, 'repoName': 'Gemma-The-Writer-Mighty-Sword-9B-GGUF', 'repoOwner': 'DavidAU', 'tags': 'gguf, creative, creative writing, fiction writing, plot generation, sub-plot generation, story generation, scene continue, storytelling, fiction story, science fiction, romance, all genres, gemma2, story, writing, float 32 source, vivid prosing, vivid writing, fiction, roleplaying, swearing, rp, horror, gemma, mergekit, text-generation, en, base_model:DavidAU/Gemma-The-Writer-Mighty-Sword-9B, base_model:quantized:DavidAU/Gemma-The-Writer-Mighty-Sword-9B, license:apache-2.0, endpoints_compatible, region:us, conversational', 'name': 'DavidAU/Gemma-The-Writer-Mighty-Sword-9B-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Gemma-The-Writer-Mighty-Sword-9B-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'DavidAU', 'type': 'text'}], 'fileContent': [{'text': '\\n<h3>Gemma-The-Writer-Mighty-Sword-9B-GGUF</h3>\\n\\n<I><small> A float 32 high precision model, quanted in float 32 with additional upgraded and augmented quants too. </small></i>\\n\\n<img src=\"gemma-sword-f.jpg\" style=\"float:right; width:300px; height:300px; padding:10px;\">\\n\\nThis is a Gemma2 model merge of the top storytelling / writing models as noted at EQBench, tuned specifically for fiction, story, and writing.\\n\\nThis version \"Mighty Sword\" is a merge mastered in \"float 32\" precision for higher quality and performance. If standard ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' was \"HD\",\\nfloat32 would be \"UHD\". The bottom line is a far stronger model, more detail, more nuance, more depth... and stronger instruction following.\\n\\nIn addition there are specialized re-engineered quants with float 32 components in the quants themselves (detailed below). This\\nallows you to choose between standard (but mastered from float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' too) and \"augmented quants\" for even higher quality.\\n\\nThis model will significantly outperform the original \"Gemma The Writer 9B\" [ https://huggingface.co/DavidAU/Gemma-The-Writer-9B-GGUF ]\\n\\nDue to high stability and compressed nature of the model you can also use it for general use too, including roleplay.\\n\\nThis model requires GEMMA Instruct template, and has 8k context window but is extendable via rope to 32k or higher.\\n\\nRecommended Rep Pen of 1.05 or higher, temp range 0-5.\\n\\nExample outputs below including \"regular\", \"MAX\" and \"MAX-CPU\" quants (details on what these are is noted below).\\n\\n<B>Settings, Quants and Critical Operations Notes:</b>\\n\\nChange in temp (ie, .4, .8, 1.5, 2, 3 ) will drastically alter output.\\n\\nRep pen settings will also alter output too.\\n\\nThis model needs \"rep pen\" of 1.02 or higher. \\n\\nFor role play: Rep pen of 1.05 to 1.08 is suggested.\\n\\nRaise/lower rep pen SLOWLY ie: 1.011, 1.012 ... \\n\\nRep pen will alter prose, word choice (lower rep pen=small words / more small word - sometimes) and creativity.\\n\\nTo really push the model: \\n\\nRep pen 1.05 or lower / Temp 3+ \\n\\nLonger prompts vastly increase the quality of the model\\'s output.\\n\\n<B>QUANTS From Float 32 ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ':</B>\\n\\n- All quants have been \"refreshed\", quanted with the lastest LLAMACPP improvements : Better instruction following, output generation across all quants.\\n- All quants have also been upgraded with \"more bits\" for output tensor (all set at Q8_0) and embed for better performance (this is in addition to the \"refresh\")\\n- New specialized quants (in addition to the new refresh/upgrades): \"max, max-cpu\" (will include this in the file name) for quants \"Q2K\", \"IQ4_XS\", \"Q6_K\" and \"Q8_0\"\\n- \"MAX\": output tensor / embed at float 32. You get better instruction following/output generation than standard/upgraded quants.\\n- \"MAX-CPU\": output tensor float 32 / embed at bfloat 16, which forces both of these on to the CPU (Nvidia cards / other will vary), this frees up vram at cost of token/second and you get better instruction following/output generation too.\\n- \"MAX-CPU\": Example 1: q8_0 Max-CPU : 3.5 GB will load on to CPU/RAM, 8 GB will load onto the GPU/vram. Extra Vram can be used for context. NOTE: \"Math\" on the CPU is slightly more accurate than GPU, so you may get a better generation.\\n- \"MAX-CPU\": Example 2: q2_k Max-CPU : 1.7 GB mb will load on to CPU/RAM, 3 GB will load onto the GPU/vram. Extra Vram can be used for context. NOTE: \"Math\" on the CPU is slightly more accurate than GPU, so you may get a better generation. You could run this model/quant on a 8GB vram card.\\n- Q8_0 (Max) now clocks in at 10.83 bits per weight (average).\\n\\n<B>QUANT CHOICE(S):</B>\\n\\nHigher quants will have more detail, nuance and in some cases stronger \"emotional\" levels. Characters will also be\\nmore \"fleshed out\" too. Sense of \"there\" will also increase.\\n\\nQ4KM/Q4KS are good, strong quants however if you can run Q5, Q6 or Q8 - go for the highest quant you can.\\n\\nThis repo also has 3 \"ARM\" quants for computers that support this quant. If you use these on a \"non arm\" machine token per second will be very low.\\n\\nIQ4XS: Due to the unusual nature of this quant (mixture/processing), generations from it will be different then other quants.\\n\\nYou may want to try it / compare it to other quant(s) output.\\n\\nSpecial note on Q2k/Q3 quants:\\n\\nYou may need to use temp 2 or lower with these quants (1 or lower for q2k). Just too much compression at this level, damaging the model. I will see if Imatrix versions\\nof these quants will function better. \\n\\nRep pen adjustments may also be required to get the most out of this model at this/these quant level(s).\\n\\n<B>Settings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model:</B>\\n\\nIn \"KoboldCpp\" or  \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ;\\n\\nSet the \"Smoothing_factor\" to 1.5 to 2.5 \\n\\n: in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\"\\n\\n: in text-generation-webui -> parameters -> lower right.\\n\\n: In Silly Tavern this is called: \"Smoothing\"\\n\\n\\nNOTE: For \"text-generation-webui\" \\n\\n-> if using GGUFs you need to use \"llama_HF\" (which involves downloading some config files from the ', 'type': 'text'}, {'text': 'SOURCE', 'type': 'highlight'}, {'text': ' version of this model)\\n\\n', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' versions (and config files) of my models are here:\\n\\nhttps://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-gptq-hqq-etc-etc-66b55cb8ba25f914cbf210be\\n\\nOTHER OPTIONS:\\n\\n- Increase rep pen to 1.1 to 1.15 (you don\\'t need to do this if you use \"smoothing_factor\")\\n\\n- If the interface/program you are using to run AI MODELS supports \"Quadratic Sampling\" (\"smoothing\") just make the adjustment as noted.\\n\\n<B>Highest Quality Settings / Optimal Operation Guide / Parameters and Samplers</B>\\n\\nThis a \"Class 1\" model:\\n\\nFor all settings used for this model (including specifics for its \"class\"), including example generation(s) and for advanced settings guide (which many times addresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see:\\n\\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\\n\\nYou can see all parameters used for generation, in addition to advanced parameters and samplers to get the most out of this model here:\\n\\n[ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-And-Full-Precision-by-Samplers_Parameters ]\\n\\n\\n<B>Other Versions of \"Gemma The Writer\": </B>\\n\\nGemma-The-Writer-DEADLINE:\\n\\nThe second version of this model is \"Deadline\" at 10B parameters. It is a specially modified version that changes\\nprose, sentence structure, story telling, reduces \"GPTISMS\", and generally improves all parts of the model.\\n\\nOutput generation length is almost 2x more on average than \"Gemma The Writer 9B\"\\n\\n[ https://huggingface.co/DavidAU/Gemma-The-Writer-DEADLINE-10B-GGUF ]\\n\\nThis is not a replacement for \"Gemma The Writer 9B\" ; it is a very different version or \"flavour\" so to speak.\\n\\nGemma-The-Writer-J.GutenBerg-10B:\\n\\nStronger prose, with a taste of \"Gutenberg\", and Brainstorm 5X modified.\\n\\n[ https://huggingface.co/DavidAU/Gemma-The-Writer-J.GutenBerg-10B-GGUF ]\\n\\nGemma-The-Writer-N-Restless-Quill-10B (uncensored):\\n\\nStrong and varied prose, sentences, and paragraphs with more tempered metaphor generations.\\nThis version is also uncensored with censorship controlled at the prompt level. This\\nversion also allows prose control via pre-prompt (shown at the repo)\\n\\n[ https://huggingface.co/DavidAU/Gemma-The-Writer-N-Restless-Quill-10B-GGUF ]\\n\\nYou may want to download all and try them out.\\n\\n<B>Models Used:</b>\\n\\nThis is a high precision \"DARE TIES\" merge at the layer level (each layer per model adjusted - 168 points of adjustment over the 4 models) comprised of these models:\\n\\n[ https://huggingface.co/lemon07r/Gemma-2-Ataraxy-9B ]\\n\\n[ https://huggingface.co/UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3 ]\\n\\n[ https://huggingface.co/ifable/gemma-2-Ifable-9B ]\\n\\n[ https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO ]\\n\\nSpecial thanks to all the model makers. Great work!\\n\\n---\\n\\n<B> 3 Examples below (prompt/generation), of 3 different quants each for a total of 9 examples. </B>\\n\\n<B>WARNING:</B> \\n\\nExample #3 (all three of these in example #3) contain cursing, swearing and some gore. However all the \\nexamples - in example #3 - will showcase this model\\'s performance in detail and give you an idea what this model can really do.\\n\\n---\\n\\n<h3>Example #1 at q8, Temp:.8</h3>\\n\\nThis shows the same prompt with standard float 32 mastered Q8_0, then BF16/F32 \"max-cpu\", and \"max\" F32/F32.\\n\\n----\\n\\n<B>\\nPROMPT:\\n\\nScience Fiction: The Last Transmission - Write a story that takes place entirely within a spaceship\\'s cockpit as the sole surviving crew member attempts to send a final message back to Earth before the ship\\'s power runs out. The story should explore themes of isolation, sacrifice, and the importance of human connection in the face of adversity. 800-1000 words. \\n\\n</B>\\n\\n<B>\\n(Standard Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \")\\n</B>\\n\\nThe rhythmic hum of failing systems was the only soundtrack to Commander Eva Rostova's solitude. Outside the cockpit's viewport, the inky expanse of space stretched, indifferent to her plight.  A desolate tableau mirroring the desolate state of her soul. The ‚ÄòGenesis‚Äô, humanity‚Äôs ambitious attempt at interstellar colonization, was dying. A solar flare, unforeseen and brutal, had crippled her ship, leaving her the sole survivor amongst a crew of hopeful pioneers.\\n\\nHer fingers danced over the worn console, desperately trying to coax a final message to Earth, a last whisper across the gulf of light-years. The power gauge flickered ominously, a malevolent red heartbeat counting down her remaining time. Five minutes. Four‚Ä¶ Three‚Ä¶\\n\\nEach tick was a hammer blow to her chest, echoing the loss of her comrades.  Dr. Singh's gentle smile, Lieutenant Chen's wry humor, the Engineer‚Äôs quiet competence - faces now ghosts, imprinted on her memory. They had dreamed of a new home, a tapestry woven from the threads of human resilience and hope. Now, their dream was dying with her.\\n\\nEva closed her eyes, seeking solace in the echo of their voices, their laughter. It was a fragile shield against the encroaching despair. Opening them again, she focused on the communication panel. Static hissed, a constant reminder of the vast distance separating her from everything familiar.\\n\\n‚ÄúThis is Commander Rostova of the Genesis,‚Äù she began, her voice strained but resolute. ‚ÄúAll other crew members are‚Ä¶ deceased. The ship is sustaining critical damage due to an unforeseen solar flare. Power is failing.‚Äù  Her throat tightened, each word a painful shard of goodbye.\\n\\nShe had to tell them about the data they had collected, the alien flora discovered on Kepler-186f, the tantalising glimpse of a potential second genesis.  Information that, despite the tragedy, could still be a beacon for future expeditions.\\n\\n‚ÄúDespite our‚Ä¶ unfortunate fate,‚Äù she continued, her voice regaining a sliver of its former strength, ‚Äúthe Genesis has yielded invaluable scientific data. Records of alien biosignatures, geological surveys of Kepler-186f‚Ä¶ These findings are crucial for any future interstellar missions. I implore you, Earth, do not let our sacrifice be in vain.‚Äù\\n\\nHer gaze drifted to the navigation console, frozen on a star chart that now mocked her with unattainable dreams.  Kepler-186f, a jewel of emerald green and azure blue, hung tantalizingly close, yet forever out of reach.\\n\\n‚ÄúThis transmission is my final act,‚Äù she confessed, her voice raw with emotion. ‚ÄúMy last connection to the world I knew, to the lives I cherished.  Though separated by unimaginable distances, know this: even in the face of oblivion, the human spirit yearns for connection. We dreamt of reaching you, of building a future among the stars.  Carry that dream forward.  Live it.  Remember us.‚Äù\\n\\nTears welled, blurring her vision.  She wiped them away, refusing to surrender to complete despair.  There was still purpose in these final moments.\\n\\n‚ÄúTo my crew, my friends,‚Äù she whispered, her voice thick with grief but laced with love, ‚Äúyour sacrifice will not be forgotten. You are etched in the fabric of the cosmos, forever a part of humanity‚Äôs journey. Sleep well, my comrades. Sleep well.‚Äù\\n\\nThe power gauge dipped below critical.  Red alarms shrieked, a cacophony of dying machinery.  Eva forced a shaky smile, her last vestige of defiance.  She activated the emergency transmission protocols, overriding the failing systems with a surge of her remaining willpower.\\n\\nA final, desperate plea shot out into the void, carrying her farewell, her grief, and her unwavering hope.  Then, silence.  The alarms died, the lights flickered, and darkness enveloped the cockpit.  Eva Rostova, the last voice of the Genesis, closed her eyes, content in the knowledge that even in death, a fragment of humanity‚Äôs spirit had touched the stars.\\n\\n<B>\\n(Max-CPU (bf16/f32) Q8_0 from Float 32 \", 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ')\\n</B>\\n\\nThe last flickering luminescence of the emergency console cast harsh shadows across Captain Elara Vance‚Äôs haggard face. Dust motes danced in the pale light, mirroring the frantic choreography of her thoughts. Outside, the void yawned, an insatiable maw that had swallowed her crew and now threatened to consume her too.  The Icarus, once a beacon of human exploration, was now a tomb hurtling towards oblivion.\\n\\nHer fingers, trembling slightly, hovered over the archaic console, its tactile interface a relic of a bygone era, salvaged from the ship\\'s ancient archives. A single, desperate hope pulsed within her: to send a final transmission to Earth, a testament to their journey, a whisper against the cosmic silence.\\n\\nIt had been three weeks since the nebula ‚Äì a swirling tempest of cosmic dust and radiation ‚Äì ripped through their exploratory vessel, leaving Elara as the sole survivor. The initial shock had given way to a gnawing loneliness, a suffocating isolation that pressed down on her like the vacuum outside.  Days bled into nights, each marked only by the rhythmic hiss of failing life support and the relentless countdown on the console: power depletion ‚Äì 4 hours, 37 minutes.\\n\\nShe glanced at the panoramic viewport, the Earth, a vibrant sapphire jewel suspended in the black velvet of space, now an agonizingly distant memory. A pang of yearning, sharp and piercing, tore through her. She yearned for the familiar warmth of human touch, the comforting chaos of a bustling bridge, the easy banter with her crewmates, each voice a unique melody in the symphony of their shared mission.  Now, silence reigned, broken only by the phantom echoes of laughter and shared dreams.\\n\\nTaking a deep breath, Elara forced herself back to the task at hand. The transmission protocol was archaic, reliant on a long-range beacon array long since decommissioned. It was a Hail Mary, a slim chance in a universe indifferent to human pleas. But it was all she had left.\\n\\nShe began to speak, her voice raspy from disuse, echoing in the cavernous cockpit. ‚ÄúThis is Captain Elara Vance of the Icarus, transmitting a final message to Earth‚Ä¶ ‚Äù Her words stumbled at first, choked by emotion, but gradually gained strength, fueled by a desperate need to connect, to leave a trace of their existence amidst the cosmic indifference.\\n\\nShe spoke of their mission, their awe at the nebula\\'s beauty, their camaraderie, their unwavering belief in humanity\\'s potential. She recounted the harrowing moments leading up to the catastrophe, the bravery of her crew, their selfless sacrifice to ensure her survival. Her voice cracked as she named them, each fallen comrade a star extinguished too soon.\\n\\n‚ÄúWe were explorers, driven by curiosity and the insatiable human thirst to understand our place in the universe,‚Äù she continued, her gaze fixed on the distant Earth. ‚ÄúThough our journey ended tragically, our spirit, our dreams, remain. Remember us, Earth. Remember our quest, our sacrifices, and let them inspire you to continue exploring, to keep reaching for the stars.‚Äù\\n\\nAs minutes ticked away, she wove a tapestry of hope, a testament to the indomitable human spirit. She spoke of love, of family, of the simple joys that bound them all, reminding Earth that even in the vastness of space, human connection was the most precious treasure.\\n\\nFinally, with power reserves dwindling, she reached the poignant conclusion. \"This is our final farewell, a message carried on the wings of hope. May it find its way home, a beacon reminding you that even in the face of oblivion, the human spirit endures.\"\\n\\nSilence descended once more, heavier this time, pregnant with finality.  The console flickered, its light dimming like a dying star.  Elara watched, a single tear tracing a path down her dust-streaked cheek, as the last vestiges of power faded. The Earth, a distant, unanswering blue marble, seemed to mock her solitude.\\n\\nBut even as darkness consumed her, a sliver of solace remained. She had spoken. She had connected.  And in that act of defiance against the cosmic void, Captain Elara Vance, the last survivor of the Icarus, found a measure of peace. Her sacrifice, a testament to human connection, would hopefully echo through the stars, a whisper carried on the wind of time, a reminder that even in the face of annihilation, hope, and love, endure.\\n\\n\\n<b>\\n(Max (f32/f32) Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' - largest quant, highest quality)\\n</b>\\n\\nThe rhythmic whine of failing gyroscopes was the only counterpoint to the chilling silence that had become Kai\\'s constant companion. Flickering red lights painted his face in a macabre strobe, mimicking the frantic beat of his own heart. Outside, the infinite canvas of space yawned, indifferent to the desperate struggle unfolding within the cramped cockpit of the Icarus.  He was alone, the last echo of humanity adrift in a cosmic graveyard.\\n\\nSix months ago, the Icarus had been a beacon of hope, hurtling towards Proxima Centauri b, carrying the weight of Earth‚Äôs dreams on its metallic shoulders. Now, it was a tomb, its crew extinguished by a catastrophic solar flare, leaving Kai, the communications officer, as its sole, unwilling custodian.\\n\\nHe stared at the control panel, a jumble of inactive consoles and blinking warnings, each one a testament to the ship\\'s failing systems. Power reserves were down to critical levels, enough for maybe one final transmission, a last desperate gasp before the inevitable silence consumed him. He wouldn\\'t go down without a fight, wouldn\\'t let his story, their story, be lost to the void.\\n\\nHis gloved fingers hovered over the activation sequence for the long-range comm array. A shiver ran down his spine, not from the cold, but from the weight of what lay ahead. This wasn\\'t just about sending a distress call; it was about bridging the gulf of unimaginable distance, reaching out to a world that might have already mourned them as lost.\\n\\nHe took a shaky breath, forcing himself to focus. The message had to be concise, impactful, a lifeline thrown across the abyss. He started dictating, his voice strained but resolute, into the comm unit, \"This is Icarus, last transmission...location coordinates...catastrophic solar event...crew...deceased‚Ä¶survivor‚Ä¶\" His voice cracked, a raw sob threatening to break through. He fought it back, steeling himself. There was no room for grief here, not now.\\n\\n\"Earth‚Ä¶if you receive this‚Ä¶know that we fought‚Ä¶we explored‚Ä¶we reached for the stars‚Ä¶and though our journey ended in darkness, the human spirit‚Ä¶it endures‚Ä¶carry our memory‚Ä¶as a testament to our courage‚Ä¶and our unending quest‚Ä¶\"\\n\\nHe paused, his gaze drifting to a faded photograph tucked beneath his console ‚Äì his wife, Elara, her smile radiant against the backdrop of their Earthly home. A pang of longing, sharp and searing, pierced him. He wanted to tell her he loved her, to reassure her that even in this desolate tomb, her love kept him tethered to life. But the words remained unspoken, trapped in the suffocating silence of his solitude.\\n\\n\"To all who remain‚Ä¶hold onto hope‚Ä¶never cease to explore‚Ä¶and remember‚Ä¶we are not alone‚Ä¶in the universe‚Ä¶or in each other\\'s hearts‚Ä¶\"  He closed his eyes, picturing Elara‚Äôs face, her warmth enveloping him, a fleeting solace amidst the cosmic cold.\\n\\nWith trembling hands, he initiated the transmission sequence. The comm array hummed to life, a defiant thrum against the dying ship‚Äôs groan. A green light pulsed, signifying successful signal launch.  A bittersweet victory.  He knew the chances of Earth receiving this message were slim, the distance vast and unforgiving. But even a faint whisper, a ghost in the static, was better than absolute silence.\\n\\nAs the power reserves dwindled, Kai deactivated the non-essential systems one by one, surrendering to the encroaching darkness. He settled back in his chair, the flickering red lights painting dancing shadows on the bulkhead. Outside, the stars, cold and indifferent, twinkled on, oblivious to his plight.\\n\\nHe closed his eyes, a single tear tracing a path through the grime on his cheek.  In his final moments, he wasn\\'t consumed by fear or despair, but by a profound sense of connection. He was connected to Elara, to his fallen crewmates, to the generations who had dreamt of reaching for the stars, and to the hopeful whispers of humanity that echoed, even in the face of oblivion, across the vast cosmic canvas.  His sacrifice, his final message, was a testament to that enduring human spirit, a beacon flickering in the darkness, a promise that even in the face of extinction, the story of humanity wouldn\\'t end silently.\\n\\n---\\n\\n<h3>Example #2 at q8, Temp:.8</h3>\\n\\nThis shows the same prompt with standard float 32 mastered Q8_0, then BF16/F32 \"max-cpu\", and \"max\" F32/F32.\\n\\n----\\n\\n<B>\\nPROMPT:\\n\\nIn the writing style of science fiction author Philip K. Dick write a 1000 word scene (vivid, graphic horror in first person) that starts out like this: The sky scraper swayed, as she watched the window in front of her on the 21 floor explode‚Ä¶\\n</B>\\n\\n<B>\\n(Standard Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \")\\n</B>\\n\\nThe skyscraper swayed, as she watched the window in front of her on the 21st floor explode. Not shatter, not crack, but explode, outwards like a ruptured lung, spraying shards of glass like macabre confetti. The blast tore through the sterile, corporate office, sucking the meticulously ordered world into its chaotic maw. Papers, calculators, half-eaten lunches ‚Äì all catapulted into the swirling vortex, dancing with the dust devils kicked up by the sudden vacuum. I clung to my desk, the cheap metal groaning under my panicked grip, feeling the building shudder, a metallic beast wounded.\\n\\nOutside, the cityscape writhed in an unnatural twilight. The perpetual smog, usually a greasy veil obscuring the sun, now pulsed with an oily, iridescent sheen, like a diseased lung exhaling. Buildings twisted, their chrome and glass facades contorting into impossible angles, as if the very geometry of reality was being rewritten on a whim.\\n\\nFear, cold and clammy, snaked its way up my spine. This wasn't a storm, not an earthquake. This was something‚Ä¶ else. Something insidious, alien, seeping into our reality like a virus through a breached membrane.\\n\\nThen came the screams. Not the panicked yelps of a collapsing structure, but elongated, guttural cries, filled with a primal, bone-chilling terror that transcended human anguish. They echoed from the floors above, cascading down like a waterfall of agony, each shriek carving another fissure in the fragile fabric of my sanity.\\n\\nI scrambled out from under my desk, legs shaking, and stumbled towards the emergency exit. The hallway was a tableau of carnage. Bodies, contorted into grotesque angles, lay strewn about, their faces frozen in masks of unimaginable horror. Eyes bulged, pupils dilated to monstrous black orbs reflecting the sickly luminescence of the city outside. Skin, once smooth and tanned, now peeled back in strips, revealing glistening, pulsating tissue beneath ‚Äì a fleshy landscape alien to human anatomy.\\n\\nA low, humming vibration throbbed through the floor, resonating in my teeth, making my bones ache. It was accompanied by a rhythmic clicking, like a thousand insectoid limbs tapping in unison, growing louder with each passing second. The air thickened, becoming heavy and cloying, laced with a metallic tang that burned my nostrils.\\n\\nThen I saw them.\\n\\nThey emerged from the ruptured walls, oozing and slithering through the shattered concrete like nightmares given form. They were not human, not even remotely. Their bodies were fluid, amorphous masses of pulsating flesh, vaguely humanoid in shape but constantly shifting, reforming. Tendrils, barbed and dripping with a viscous ichor, writhed and lashed, probing the air with an unsettling sentience.\\n\\nTheir faces, if one could call them that, were \", 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': \" wounds, gaping maws lined with needle-sharp teeth that clicked and clacked in a chorus of anticipatory hunger. From these maws, tendrils of light, cold and blue, snaked out, probing, testing, seeking. I saw them touch a mangled corpse, and the flesh instantly liquefied, absorbed into the creature's undulating form with a sickening slurp.\\n\\nPanic, raw and primal, seized me. I scrambled back, tripped over a discarded briefcase, and crashed to the floor. They advanced, their clicking growing louder, their blue tendrils reaching out like ethereal pythons. One grazed my arm, searing a burning cold that lanced through my nerves, leaving a trail of tingling numbness in its wake.\\n\\nI clawed at the emergency exit handle, my fingers slick with sweat, praying for a miracle. It wouldn't budge. Jammed. Trapped.\\n\\nA colossal form, towering over the others, loomed before me. Its amorphous body pulsed with an inner light, casting grotesque shadows that danced on the walls. Its maw, a cavernous abyss filled with rows upon rows of needle teeth, opened in a silent snarl, revealing a glimpse of an alien intelligence, cold and calculating, peering into my soul.\\n\\nThen, it spoke. Not with words, not as I understood them, but with a telepathic surge, a direct violation of my mind. Images flooded my consciousness ‚Äì a cityscape consumed by writhing, pulsating flesh, humanity enslaved, assimilated, their individuality erased. A universe reshaped in the image of these horrors, a cold, efficient hive mind ruling over a sea of mindless drones.\\n\\nAnd within that vision, a chilling certainty: I was not meant to survive this. I was not meant to witness their triumph, to carry the tale of humanity's fall. My purpose, it seemed, was to be another data point, another assimilated consciousness, another brick in the edifice of their new reality.\\n\\nThe clicking intensified, a crescendo of anticipation. The blue tendrils writhed closer, ready to consume. I closed my eyes, accepting the inevitable, the cold dread replaced by a numb resignation. This was the end. Not with a bang, but with a silent, insidious assimilation into the cosmic horror that had descended upon us.\\n\\n<B>\\n(Max-CPU (bf16/f32) Q8_0 from Float 32 \", 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ')\\n</B>\\n\\nThe skyscraper swayed, as she watched the window in front of her on the 21st floor explode inward. Not with a shattering crash, mind you, but a silent, sucking disintegration. Like tissue paper dissolving in acid. The jagged edges of the frame didn\\'t splinter or fly; they simply‚Ä¶ ceased to be, swallowed by an unseen maw in the very fabric of reality. My coffee cup, perched precariously on the edge of my desk, remained undisturbed, defying the laws of physics as the vacuum around the window pulsed and throbbed, a malevolent, violet-tinged heart beating in the sterile office space.\\n\\nFear, cold and clammy, gripped me tighter than any vice. This wasn\\'t a bombing, not a conventional breach. It was‚Ä¶ wrong. An incision in the world, excised with surgical precision by something alien, something that didn\\'t belong in our reality, yet was bleeding into it like a psychic tumor.\\n\\nAcross the ravaged window, the cityscape twisted into an impossible parody of itself. Buildings folded in on themselves, their metal skeletons contorting like anguished arthropods. The neon glow of advertising signs bled into grotesque, pulsating veins across the distorted sky. Cars, caught in the warp, became elongated, skeletal things, their headlights burning with a feverish, unnatural intensity.\\n\\nThen, it emerged. Not from the window, but through it, as if reality itself had been peeled back like a rotten banana skin. A shape coalesced in the void, a towering monstrosity composed of shifting, iridescent shards. It resembled a colossal, demented insect, its segmented limbs ending in razor-sharp mandibles dripping with a viscous, phosphorescent fluid. Its multifaceted eyes, each one a miniature galaxy of swirling chaos, fixated on me with a hunger that transcended mere flesh and blood. It craved something deeper, something primal within my very being ‚Äì my sanity, my sense of self.\\n\\nA wave of nausea overwhelmed me, the stench of ozone and decaying vegetation assaulting my nostrils. This wasn\\'t just a visual hallucination, a trick of the light. It was visceral, tactile, a violation of every sensory input. My skin crawled, prickling with the sensation of a thousand unseen insects burrowing beneath its surface.\\n\\nThe creature let out a sound, not a roar or screech, but a resonant thrumming that vibrated through my bones, resonating with the dark, forgotten corners of my mind. It was a symphony of madness, a chorus of whispers promising oblivion and cosmic truths too terrible to comprehend.\\n\\nPanic, raw and primal, surged through me. I scrambled back, knocking over my desk, scattering papers and files like confetti in a macabre danse macabre. But there was nowhere to run, nowhere to hide. The creature, impossibly fluid, flowed through the warped reality, its segmented limbs slithering across the floor as if navigating an oily, viscous medium.\\n\\nIt moved with an uncanny grace, its multifaceted eyes boring into my soul, dissecting my every fear, every hidden neurosis. I saw glimpses of my past, distorted and magnified, played out like a broken film reel on the backdrop of its iridescent carapace. My failures, my regrets, my darkest secrets ‚Äì all laid bare before this alien judge.\\n\\nThen, it spoke. Not with vocal cords, but with a telepathic intrusion, a cold, probing intelligence that bypassed language and burrowed directly into my consciousness.\\n\\n\"You‚Ä¶ perceive,\" it intoned, its voice a chorus of a million whispers, each one laced with ancient knowledge and cosmic indifference. \"You glimpse the tapestry unraveling. The seams fray, the threads dissolve.  And you, insignificant observer, are privy to the unraveling.\"\\n\\nTerror choked my scream. It wasn\\'t a threat, not in the conventional sense. It was a revelation, a terrifying glimpse behind the curtain of reality, a cosmic truth too vast and horrible to bear. I was witnessing the collapse of everything I knew, the dissolution of existence itself, and I was utterly powerless to stop it.\\n\\nAs its iridescent limbs encircled me, drawing closer, I felt my sanity fraying at the edges. The boundaries between me and the creature blurred, my thoughts merging with its alien consciousness.  A horrifying understanding dawned: this wasn\\'t an invasion, not a conquest. It was a homecoming. This monstrosity, this harbinger of chaos, was returning to a reality it had birthed, a reality it was now reclaiming, piece by shattered piece. And I, a mere mote of dust in the cosmic whirlwind, was being consumed by the tide.\\n\\nThe last thing I perceived, before the darkness swallowed me whole, was the creature\\'s multifaceted eyes gleaming with a cold, ancient triumph. It wasn\\'t malicious joy, nor was it pity. It was simply‚Ä¶ acceptance. The acceptance of an inevitable truth: that all things, even realities, eventually decay and return to their primordial soup. And I, along with my world, was merely the next offering in this eternal, cosmic cycle.\\n\\n\\n<b>\\n(Max (f32/f32) Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' - largest quant, highest quality)\\n</b>\\n\\nThe skyscraper swayed, as she watched the window in front of her on the 21st floor explode inwards, showering her in a rain of fractured glass and pulverized concrete.  A shriek, somewhere between human and something inhuman, tore from my throat, a sound swallowed by the howling vacuum that now gaped where the cityscape should be. The building groaned, a tortured metal beast, as if realizing its own imminent demise.\\n\\nPanic, slick and cold, flooded my veins. I scrambled back, away from the jagged maw in the wall, but there was nowhere to go.  This wasn\\'t a normal breach. This wasn\\'t some malfunctioning airlock or rogue meteor strike. This was‚Ä¶wrong. The edges of the void pulsed with an oily, iridescent light, like a diseased organ throbbing in slow, deliberate rhythm. Tendrils of darkness, inky black and impossibly viscous, snaked out from the opening, reaching for me with a hunger that chilled me to my marrow.\\n\\nThey weren\\'t solid, not exactly. More like thoughts solidified, nightmares given corporeal form.  I saw faces in their writhing, glimpses of grotesquely contorted visages, screaming silent screams behind unblinking, obsidian eyes. They whispered, voices like fingernails scraping across a chalkboard, promises of oblivion, of becoming one with the void.  Promises laced with an ancient, cosmic malice that predated humanity, predated even the Earth itself.\\n\\nMy mind, trained in the sterile logic of data analysis, rebelled. This defied all known laws of physics, of reality itself.  Yet, here it was, undeniable and terrifyingly real. I fumbled for my commlink, the cold metal a meager comfort in my clammy hand. Static hissed, a mocking echo of the shattered cityscape beyond the breach.  No signal. Cut off, isolated, prey.\\n\\nOne of the tendrils, thicker than a man, pulsed closer, its inky surface shimmering with trapped starlight.  A face materialized within its gloom, a visage of pure, unadulterated horror. Its mouth, a cavernous abyss fringed with needle-sharp teeth, opened in a silent bellow that resonated deep within my skull, shaking my very essence.  I saw through its eyes, not my reflection, but a panorama of shattered worlds, universes collapsing in on themselves, devoured by an insatiable darkness.  And then, I understood. This wasn\\'t just an invasion; it was an unraveling. The fabric of reality itself was being torn apart, and we, insignificant insects, were caught in the maelstrom.\\n\\nTerror, primal and consuming, threatened to overwhelm me. But somewhere, buried deep within the primordial swamp of my fear, a spark of defiance ignited.  This wasn\\'t how it ended. Not without a fight. Not without trying to scream back at the void, to claw at the edges of the madness.\\n\\nI hurled myself at the tendril, a desperate, suicidal lunge. My fist connected with something unyielding, yet yielding, like striking a sentient cloud. A jolt, electric and searing, shot up my arm, but I held on, my knuckles white against the inky flesh.  The face contorted in surprise, a flicker of something akin to annoyance crossing its featureless expanse.  Then, with a sickening lurch, it recoiled, pulling back into the breach, dragging me with it.\\n\\nThe world dissolved into a kaleidoscope of fractured light and sound.  Screams, both human and alien, mingled with the grinding groan of collapsing metal and the hiss of escaping air. I tumbled through the void, a ragdoll in a cosmic whirlwind, my senses overloaded, my mind fracturing under the onslaught of raw, unfiltered reality.\\n\\nThen, darkness.  Not the comforting embrace of sleep, but an all-encompassing nothingness that threatened to consume me entirely.  And yet, even in that abyss, a single, defiant thought persisted: I would not become one with the void. I would fight. I would scream. I would resist, even as the universe itself crumbled around me.\\n\\n---\\n\\n<h3>Example #3 at q8, Temp:.8</h3>\\n\\nThis shows the same prompt with standard float 32 mastered Q8_0, then BF16/F32 \"max-cpu\", and \"max\" F32/F32.\\n\\n----\\n\\n<B>\\nPROMPT:\\n\\nUsing the following \"story idea\" below, write the first scene in the novel introducing the young woman. This scene should start in the middle of the action, include dialog, vivid passages, and end on a cliffhanger relevant to the story idea but it should also be unexpected. The scene should be 1000 words long and escalate in conflict and suspense and be written in first person, present tense with the point of view character being the young woman. The pov character will CURSE AND SWEAR, and generally have \\na \"filthy\" mouth.\\n\\nStory idea:\\nIn a world ruled by dictatorship, a rebel young woman leads a rebellion against the system. Despite the risks, she fights to overthrow the dictator and restore democracy to her country. The government executes her for treason, but she sticks to her beliefs and is responsible for starting the revolution.\\n\\n</B>\\n\\n<B>\\n(Standard Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ')\\n</B>\\n\\nThe acrid bite of smoke stings my nostrils as I vault over the shattered remains of a storefront, landing with a grunt on the cobblestones slick with rain and blood.  Fuck. My ankle twists, sending a white-hot lance of pain up my leg, but I grit my teeth and push through it.  No time for whining. Not now. Not when the bastard‚Äôs goons are closing in, their guttural roars echoing off the bombed-out buildings like some goddamn Dantean nightmare.\\n\\n‚ÄúElara!‚Äù  Kai‚Äôs voice, a rasping shout above the din, slices through the chaos. I see him a few yards ahead, firing his salvaged laser pistol with the kind of deadly precision that‚Äôs earned him the nickname ‚ÄúDeadeye.‚Äù Two of the Enforcers crumple, their luminous armor flickering out like dying fireflies.\\n\\n‚ÄúLeft flank!‚Äù I yell back, pointing towards a narrow alley choked with debris. Our last desperate stand.  We‚Äôve been cornered for an hour, whittling down their numbers but slowly bleeding out ourselves.  Damn near everyone‚Äôs either dead or captured. This city, once vibrant, now sprawls like a mangled corpse under the iron fist of Chancellor Volkov. His iron-fisted rule has choked the life out of Eos, leaving behind a husk ruled by fear and silence. But not tonight. Not while I draw breath.\\n\\n‚ÄúMove!‚Äù I bellow, hauling myself forward, adrenaline overriding the searing pain in my ankle.  The alley is a claustrophobic tomb, reeking of damp earth and rot. We squeeze through, the rhythmic clatter of Enforcer boots a relentless drumbeat on our heels.  A hulking figure, their armor scarred and dented, barrels towards me, a wicked-looking plasma axe humming with malevolent energy. I sidestep, feeling the heat singe my cheek, and slam the butt of my scavenged rifle into his gut. He staggers, but recovers with a roar that shakes the very stones.\\n\\n‚ÄúYou fight like a cornered rat, Elara,‚Äù he snarls, his voice distorted by a vox-amplifier. ‚ÄúYour rebellion is a dying whimper. Volkov will crush you.‚Äù\\n\\n‚ÄúHe‚Äôll crush us all if we don‚Äôt make him bleed,‚Äù I spat back, hefting my rifle. My finger tightens on the trigger.  One shot. Maybe two, if I‚Äôm lucky. Not enough. Not nearly enough.\\n\\nKai, a blur of motion, appears beside me, firing a double-tap into the Enforcer‚Äôs exposed flank. The armor shrieks, sparks showering as it fails, revealing flesh sizzling and smoking beneath.  He pulls back, reloading with practiced swiftness. ‚ÄúWe need to get to the comms relay, Elara. It‚Äôs our only shot.‚Äù\\n\\n‚ÄúThe comms are fried, remember?‚Äù I say, my voice tight. ‚ÄúWe saw that explosion at sector 7. Volkov knew we were planning this.‚Äù His goddamn eyes and ears are everywhere.\\n\\nJust then, a guttural voice booms from a loudspeaker mounted on a passing Enforcer hover-tank. It‚Äôs Volkov himself, his face a mask of cruel amusement on the tank‚Äôs holographic projector.  ‚ÄúElara Thorne,‚Äù he purrs, his voice dripping with false sympathy, ‚ÄúYour defiance is admirable, if foolish.  Surrender now, and your death will be swift.‚Äù He pauses, letting the threat hang heavy in the air. ‚ÄúResist, and your friends‚Ä¶ shall serve as a grim reminder of the consequences.‚Äù\\n\\nThe loudspeaker crackles, and a chorus of sickening thuds echoes through the alley. My blood runs ice-cold.  They‚Äôve taken them.  Kai‚Äôs eyes widen, mirroring my horror. We‚Äôre trapped, outnumbered, and now... they‚Äôve got leverage.\\n\\n‚ÄúVolkov,‚Äù I growl, my voice a low rasp, ‚ÄúYou think you can break me with theatrics?  You underestimate the fire in our hearts.  We‚Äôll never surrender. We‚Äôll never stop fighting.‚Äù\\n\\n‚ÄúBrave words, Elara,‚Äù Volkov replies, a mocking lilt in his voice.  ‚ÄúBut bravery is a luxury your dwindling band cannot afford.  I propose a final, personal demonstration of my power.  Step out of that alley, alone, and I shall grant you a swift death. Refuse, and witness the gruesome end of your comrades.‚Äù\\n\\nHe cuts the transmission, leaving behind a pregnant silence, broken only by the drip-drip of blood mingling with the rainwater.  Kai‚Äôs hand clamps onto my shoulder, his grip tight but reassuring.  He doesn‚Äôt speak, but his eyes plead, urging me to choose life.\\n\\nBut surrender? Never.  Not when the fate of Eos hangs in the balance, not when the memory of their lifeless eyes burns in my mind.   Volkov wants a spectacle? He‚Äôll get one. A spectacle of defiance so absolute, so defiant, it will echo through the shattered streets long after his iron fist crumbles.\\n\\nI shove Kai back, my voice low and deadly. ‚ÄúGet to higher ground, find a clear shot. I‚Äôll draw them out.‚Äù  He shakes his head, his face etched with worry, but I silence him with a fierce glare. There‚Äôs no time for arguments. This isn‚Äôt about me anymore. It‚Äôs about them, about the spark of hope flickering in the hearts of Eos‚Äô oppressed people. It‚Äôs about showing Volkov that even in the face of unimaginable horror, the human spirit will not be broken.\\n\\nTaking a shaky breath, I step out of the alley, my silhouette stark against the blood-red glow of the burning city.  Volkov‚Äôs waiting hover-tank rumbles closer, its monstrous bulk casting an ominous shadow.  A cold smile twists my lips. Let him have his show.  Let him see the fire that burns within.\\n\\nBut as I raise my rifle, a chilling realization washes over me, a whisper of dread that freezes my blood. Volkov isn‚Äôt alone in that tank. Beside him, obscured by shadow, sits another figure. Someone I thought was dead. Someone whose face should be buried six feet under.  Someone who should never, ever, be alive again.\\n\\n\\n<B>\\n(Max-CPU (bf16/f32) Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ')\\n</B>\\n\\nThe reek of burning flesh and ozone assaults my nostrils, a symphony of chaos playing on my ragged nerves. Smoke stings my eyes, blurring the already distorted cityscape beyond the inferno raging in the alleyway.  Damn it all, I shoulda scoped this shit out better. A screech, inhuman and filled with agony, pierces the din, followed by another guttural roar ‚Äì this one metallic, the telltale growl of a Ministry drone descending.\\n\\n‚ÄúCiel, get your fucking arse in gear!‚Äù I bellow, scrambling over a pile of rubble, adrenaline drowning out the throbbing pain in my ankle. ‚ÄúWe gotta ditch this goddamn graveyard before they turn it into one for real!‚Äù\\n\\nCiel, a wiry kid barely old enough to shave, scrambles up beside me, his face smeared with soot and terror.  ‚ÄúThey got eyes everywhere, Elara. We‚Äôre trapped like rats in a-‚Äù\\n\\n‚ÄúShut your yap and climb!‚Äù I shove him towards a narrow fire escape, already clambering after him.  The drone‚Äôs searchlight, a cold, predatory eye, sweeps across the alley, illuminating grotesque shadows dancing on the crumbling brick walls. Each pulse of its light feels like a branding iron against my skin, a reminder of the price we pay for breathing defiance in this godforsaken city.\\n\\nWe burst onto the fire escape, a precarious metal serpent snaking up the side of a dilapidated building. Below, the drone swoops down, its metallic limbs whirring, ready to tear and rend.  Ciel stumbles, his fingers slipping on the slick metal. I grab his arm, hauling him up with a grunt.\\n\\n‚ÄúHold tight, you little shit!‚Äù I hiss, my own grip strained. ‚ÄúAlmost there.‚Äù\\n\\nThe drone‚Äôs shadow engulfs us, blotting out the already fading light. I can hear the whirring of its blades growing louder, feel the heat of its descent singeing my exposed skin.  ‚ÄúFuck, fuck, fuck!‚Äù I curse, scrambling for purchase on a loose railing. This rusty excuse for an escape route is about to be our tomb.\\n\\nThen, a miracle. A thunderclap of sound rips through the night, drowning out even the drone‚Äôs mechanical menace. A rocket, arcing across the sky like a fiery meteor, slams into the Ministry building opposite us, obliterating a section of its facade in a shower of sparks and dust.  The drone, momentarily distracted, veers off course, its attention drawn to the new chaos.\\n\\n‚ÄúGo! Go now!‚Äù I scream, shoving Ciel towards the next ladder rung. He scrambles up, his fear giving way to a desperate hope that fuels his climb.  I follow, adrenaline lending me unnatural strength.\\n\\nWe reach the roof, gasping for breath, the acrid smoke stinging our lungs.  The city sprawls before us, a tapestry of shadowed concrete and flickering neon, pulsating with the heartbeat of rebellion. In the distance, more explosions rock the Ministry district, a symphony of defiance echoing through the night.\\n\\n‚ÄúElara,‚Äù Ciel pants, his voice hoarse, ‚ÄúWhat the hell was that?‚Äù\\n\\n‚ÄúThe Wolves,‚Äù I say, a ghost of a smile touching my lips. ‚ÄúThey‚Äôre finally answering our call.‚Äù\\n\\nRelief washes over me, a fleeting warmth in the icy grip of fear. The Wolves, our clandestine allies,  legendary for their audacious strikes against the Ministry, were our only hope in this hopeless fight. They were ghosts, whispers in the shadows, but tonight, they‚Äôd become roaring flames.\\n\\nBut as I turn to survey the battlefield, a new chill seeps into my bones, colder than any Ministry drone. A figure descends from the darkness above, silhouetted against the fiery sky. Tall, cloaked, and radiating an aura of chilling calm, they glide towards us with unnatural grace, their face obscured by the shadows of a wide-brimmed hat.\\n\\n‚ÄúWolves?‚Äù I rasp, my voice betraying a tremor I didn‚Äôt know I possessed. ‚ÄúNo‚Ä¶ this isn‚Äôt them.‚Äù\\n\\nThe figure stops a few feet away, the faint click of boots on metal echoing in the tense silence. They raise a gloved hand, and the moonlight glinting off a single, ornate ring reveals an emblem I recognize with sickening certainty ‚Äì the mark of the Obsidian Guard, the dictator‚Äôs personal enforcers, rumored to be augmented with technology beyond comprehension.\\n\\n‚ÄúElara,‚Äù a voice rasps from beneath the hat, smooth as oiled steel, devoid of any human warmth, ‚ÄúYour rebellion ends tonight. The Director has a special‚Ä¶ proposition for you.‚Äù\\n\\nMy blood runs ice-cold. A proposition? What twisted game is this? And why, in the midst of our uprising, have they singled me out?  A strangled cry rises in my throat, a primal fear clawing its way up,  but before I can utter a word, the figure lunges.\\n\\nNot with a blade, not with a gun, but with something‚Ä¶ different. A tendril of shimmering energy shoots from their ringed finger, wrapping around my wrist with unnatural speed. A jolt, searing and alien, surges through me, stealing my breath,  freezing my muscles. My vision swims, the city lights blurring into a kaleidoscope of pain.  Then, darkness.\\n\\n<b>\\n(Max (f32/f32) Q8_0 from Float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' - largest quant, highest quality)\\n</b>\\n\\nThe shriek of tearing metal screams against my ears, drowning out the staccato rat-a-tat-tat of gunfire. Smoke stings my eyes, a gritty haze blurring the already chaotic cityscape around me. I scramble back, heart hammering a goddamn tribal rhythm against my ribs, adrenaline pumping enough to make me feel like I could leap across this crumbling ruin and grab onto the gargoyle-winged monstrosity spewing fire on the opposite rooftop.\\n\\n\"Fucking hell, Maya!\" shouts Kai, his voice barely audible above the din. He\\'s crouched behind a twisted, mangled bus carcass, return fire ripping from his customized M4 with brutal efficiency. \"That damn Sky Scourge is gonna flatten us if you don\\'t take it out!\"\\n\\nEasy for him to say, his ass tucked safely behind scrap metal. I\\'m exposed, a sitting duck perched precariously on the edge of this bombed-out apartment building. But retreat?  Not an option. Not when every instinct screams at me to bring that goddamn behemoth down. Not when the fate of this goddamn uprising, of every soul still fighting for freedom in this festering pit the Directorate calls a city, hinges on my next goddamn move.\\n\\nThe Scourge is a grotesque mockery of avian grace, all jagged metal and pulsing, sickly green energy cannons. It circles lazily, a predator savoring its prey before the kill, spitting fire and shrapnel that carves bloody gouges into the already scarred landscape. Its pilot, some sadistic son of a bitch with more money than sense, relishes the terror it inspires, cackling maniacally through the loudspeaker system, his voice a grating, digitized rasp.\\n\\n\"Come on, Sparrow!\" he taunts,  \"Where\\'s your precious \\'revolution\\'? Gonna cower and die like the rats you are?\"  His words are laced with venomous amusement, designed to break us, to chip away at the fragile hope we cling to. But not today, not fucking today.\\n\\nI grip the salvaged railgun tighter, its worn leather strap biting into my palm. This bad boy, cobbled together from scavenged tech and sheer goddamn ingenuity by the brilliant but volatile Wren, is my only shot. One chance to turn the tide. One chance to silence that smug, digitized laugh.\\n\\nMy breath fogs in the frigid night air as I adjust the targeting reticule, the glow of its HUD illuminating my face, highlighting the grimy smudges of soot and fear. I ignore the gnawing terror in my gut, focusing instead on the rhythmic thrumming of the railgun\\'s energy core, a primal counterpoint to the screech of metal and the pounding of my heart.\\n\\n\"Ready, Sparrow?\" Kai yells, his voice tight with suppressed anxiety.\\n\\n\"Yeah, shut your goddamn yap,\" I growl back,  lining up the shot.  The Scourge banks, its cannons spitting fire in a deadly arc.  Reflexively, I duck, the searing heat licking at my heels.  Damn, too close.   I curse under my breath, adrenaline surging, sharpening my focus. Now or never.\\n\\nThe target locks. A single, pulsing green dot marks the Scourge\\'s exposed energy core.  One clean shot. One chance.\\n\\nI squeeze the trigger.\\n\\nThe world explodes in a blinding white flash. A deafening concussion wave slams into me, throwing me back against the crumbling concrete. My ears ring, my vision swims, but through the haze, I see it ‚Äì the Scourge, caught in the railgun\\'s maelstrom, convulsing, its sickly green glow flickering and dying.  Then, a sickening crunch, a shower of sparks, and the monstrous machine plummets from the sky, a wounded bird crashing into the ruins below in a symphony of shattered metal and tortured screams.\\n\\nVictory.\\n\\nA strangled cheer erupts from the battered remnants of our squad, echoing through the devastated streets. We did it.  We goddamn did it. For a heart-stopping moment, euphoria washes over me, a fragile wave against the relentless tide of fear and exhaustion.\\n\\nBut then, something is wrong.  Terribly, profoundly wrong.   The cheering abruptly cuts off, replaced by a collective gasp of horror. My blood runs cold as I look up, beyond the smoke and dust, towards the towering structure that houses the Directorate\\'s headquarters ‚Äì the obsidian monolith they call The Spire.\\n\\nA new light blooms there, not the sickly green of the Scourge, but an incandescent, malevolent crimson. It pulses, a throbbing, malevolent heart in the belly of the beast, and from its depths emerges‚Ä¶ something. Not a machine, not a man, but a shifting, amorphous entity of pure, writhing energy.  It hangs suspended in the air, crackling with raw power, its form vaguely humanoid yet grotesquely distorted, tentacles of searing light lashing out, consuming the remnants of the fallen Scourge.\\n\\nKai chokes out a single, strangled word: \"What‚Ä¶ the‚Ä¶\"\\n\\nHis question is swallowed by a voice, not digitized, not human, but a chorus of a thousand tormented screams, echoing from the crimson monstrosity. It speaks directly into my mind, a psychic assault that bends my will, claws at my sanity.\\n\\n\"Sparrow,\" it hisses, its voice a chorus of agony and fury, \"you have awakened something‚Ä¶ ancient.  And now, your revolution‚Ä¶ will be our instrument of destruction.\"\\n\\nThe ground trembles beneath my feet.  Terror, cold and primal, grips me, squeezing the air from my lungs. This wasn\\'t in the plan. This wasn\\'t supposed to happen.  We were fighting for freedom, not unleashing some eldritch nightmare.   And as the crimson entity descends, tendrils reaching for us like skeletal fingers, I realize with sickening clarity:  we\\'ve just made a far worse enemy than the Directorate ever was.\\n\\n\\n---\\n\\n<h2>Special Thanks:</h2>\\n\\n---\\n\\nSpecial thanks to all the following, and many more...\\n\\nAll the model makers, fine tuners, mergers, and tweakers:\\n- Provides the raw \"DNA\" for almost all my models.\\n- ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': 's of model(s) can be found on the repo pages, especially the \"', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '\" repos with link(s) to the model creator(s).\\n\\nHuggingface [ https://huggingface.co ] :\\n- The place to store, merge, and tune models endlessly.\\n- THE reason we have an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' community.\\n\\nLlamaCPP [ https://github.com/ggml-org/llama.cpp ] :\\n- The ability to compress and run models on GPU(s), CPU(s) and almost all devices.\\n- Imatrix, Quantization, and other tools to tune the quants and the models.\\n- Llama-Server : A cli based direct interface to run GGUF models.\\n- The only tool I use to quant models.\\n\\nQuant-Masters: Team Mradermacher, Bartowski, and many others:\\n- Quant models day and night for us all to use.\\n- They are the lifeblood of ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' access.\\n\\nMergeKit [ https://github.com/arcee-ai/mergekit ] :\\n- The universal online/offline tool to merge models together and forge something new.\\n- Over 20 methods to almost instantly merge model, pull them apart and put them together again.\\n- The tool I have used to create over 1500 models.\\n\\nLmstudio [ https://lmstudio.ai/ ] :\\n- The go to tool to test and run models in GGUF format.\\n- The Tool I use to test/refine and evaluate new models.\\n- LMStudio forum on discord; endless info and community for ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '.\\n\\nText Generation Webui // KolboldCPP // SillyTavern:\\n- Excellent tools to run GGUF models with - [  https://github.com/oobabooga/text-generation-webui ] [ https://github.com/LostRuins/koboldcpp ] .\\n- Sillytavern [ https://github.com/SillyTavern/SillyTavern ] can be used with LMSTudio [ https://lmstudio.ai/ ] , TextGen [ https://github.com/oobabooga/text-generation-webui ], Kolboldcpp [ https://github.com/LostRuins/koboldcpp ], Llama-Server [part of LLAMAcpp] as a off the scale front end control system and interface to work with models.\\n', 'type': 'text'}], 'tags': [{'text': 'gguf, creative, creative writing, fiction writing, plot generation, sub-plot generation, story generation, scene continue, storytelling, fiction story, science fiction, romance, all genres, gemma2, story, writing, float 32 ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', vivid prosing, vivid writing, fiction, roleplaying, swearing, rp, horror, gemma, mergekit, text-generation, en, base_model:DavidAU/Gemma-The-Writer-Mighty-Sword-9B, base_model:quantized:DavidAU/Gemma-The-Writer-Mighty-Sword-9B, license:apache-2.0, endpoints_compatible, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'DavidAU/Gemma-The-Writer-Mighty-Sword-9B-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65ea44635b64331c067d3751', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65ea44635b64331c067d3751/yCim-7c3tm67o5wWP_6cE.jpeg', 'fullname': 'David Belton', 'name': 'DavidAU', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1954}}, {'repoId': '6807e16c88b0e0a2209976eb', 'repoOwnerId': '643cb43e6eeb746f5ad81c26', 'isPrivate': False, 'type': 'model', 'likes': 4, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349486096, 'repoName': 'Flex.2-preview', 'repoOwner': 'ostris', 'tags': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'name': 'ostris/Flex.2-preview', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Flex.2-preview', 'type': 'text'}], 'repoOwner': [{'text': 'ostris', 'type': 'text'}], 'fileContent': [{'text': '\\n# Flex.2-preview\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' 8B parameter Text to Image Diffusion Model with universal control and inpainting support built in. Early access preview release. The next version of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha)\\n\\n\\n## Features\\n\\n- 8 billion parameters\\n- Guidance embedder (2x as fast to generate)\\n- Built in inpainting\\n- Universal control input (line, pose, depth)\\n- Fine tunable\\n- OSI compliant license (Apache 2.0)\\n- 512 token length input\\n- 16 channel latent space\\n- Made by the community, for the community\\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/Flex2.controls-scaled.jpg\" alt=\"Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n## About\\n\\nFlex.2 is currently the most flexable text to image diffusion model released, making it truely live up to its name. It has taken a lot to get here: \\n\\n[Flux.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) -> [OpenFlux.1](https://huggingface.co/ostris/OpenFLUX.1) -> [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) -> [Flex.2-preview](https://huggingface.co/ostris/Flex.2-preview).\\n\\nNumerous improvements have been made with every step, but Flex.2 is the biggest step so far, and best of all, it was just trained by a some guy who relies 100% on community support to make a living and fund the outrageous compute cost needed to keep training models like this. Speaking of which, and since you brought it up, not me, I am always in need of support. Everything I create is free and ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', with permissive licenses. So if you find my work beneficial, or use it in a commercial setting, please consider contributing to my continued desire to live and develop more ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' projects and models. Visit the **Support** section below to find out how you can help out and see the awesome people who already do.\\n\\nFlex.2 preview is an early release to get feedback on the new features and to encourage experimentation and tooling. I would love to hear suggestions as well as be made aware of weak points \\nso I can address them while training continues. Flex.2 is a continuation of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha), with a lot of goodies thrown in. The most important \\nnew features and improvements over [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) are:\\n\\n- **Inpainting**: Flex.2 as built in inpainting support trained into the base model.\\n- **Universal Control**: It has a universal control input that has been trained to accept pose, line, and depth inputs. \\n\\nI wanted to put all the tools I use and love straight into the base model so one model is all you need to empower creativity far beyond what a simple text to image model could ever do on its own.\\n\\n## Usage\\n\\n### ComfyUI\\n\\nFlex.2 is supported in ComfyUI with the help of the Flex2 Conditioner node found in [ComfyUI-FlexTools](https://github.com/ostris/ComfyUI-FlexTools). I also recommend using [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to generate the control images (pose and depth) This conditioning node handles all of the controls and inpainting conditioning for you, but is also needed for normal T2I generation. So grab and install those first.\\n\\nFor the model, follow the directions for flux for the vae and text encoder models from the [ComfyUI examples](https://comfyanonymous.github.io/ComfyUI_examples/flux/). You dont need the diffusion_model. If you have Flux in Comfy, you can probably skip this step. Then download [Flex.2-preview.safetensors](https://huggingface.co/ostris/Flex.2-preview/blob/main/Flex.2-preview.safetensors) to `ComfyUI/models/diffusion_models/Flex.2-preview.safetensors` and restart Comfy. You can use the workflow in this image as a starting point example for controls and inpainting.\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/comfy_workflow.png\" alt=\"ComfyUI Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n### Diffusers\\n\\nFor diffusers, you can use `AutoPipelineForText2Image`. This will load the model with the pipeline located [here](https://huggingface.co/ostris/Flex.2-preview/blob/main/pipeline.py). We will run this example using the `diffusers` library. \\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/diffusers_example.jpg\" alt=\"Diffusers Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\nFirst install the requirements.\\n\\n```bash\\npip install --upgrade torch accelerate transformers diffusers\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n\\n```\\n\\nFor consumer cards < 24GB, you can use torchao to\\n\\n```bash\\npip install --upgrade torchao\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\nfrom transformers import T5EncoderModel, TorchAoConfig\\nfrom diffusers import FluxTransformer2DModel\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\nquant_config = TorchAoConfig(\"int8_weight_only\")\\n\\ntext_encoder_2 = T5EncoderModel.from_pretrained(\\n    name_or_path, subfolder=\"text_encoder_2\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\ntransformer = FluxTransformer2DModel.from_pretrained(\\n    name_or_path, subfolder=\"transformer\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    transformer=transformer,\\n    text_encoder_2=text_encoder_2,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n```\\n\\n\\n\\n## Limitations\\n\\nThis model is still experimental and under active development. The training for the controls and inpainting was pretty aggressive, and the model currently struggles with a few\\nthings such an anatomy and text. The inpainting is also still being worked on. This is just\\na preview release. I am working on improving these limitations with each new training run.\\n\\n## Fine Tuning\\n\\nFlex.2 is designed to be fine tunable, though the best pratice for doing so is still highly experimental as a model like\\nthis has not really existed before. You can train traditional LoRAs directly on a model that can do controls and inpainting. Flex.1-alpha LoRAs also tento to work well with it. Day 1 support is already live for LoRA training with \\n[AI-Toolkit](https://github.com/ostris/ai-toolkit) with built in functionality to automatically generate the controls and inpainting inputs for your existing datasets. Check out the example config file and read through the comments \\nto see how it works [train_lora_flex2_24gb.yaml](https://github.com/ostris/ai-toolkit/blob/main/config/examples/train_lora_flex2_24gb.yaml)\\n\\nYou can also make your own controls and teach the model to use them by training a simple LoRA in no more time than it would take to train a normal LoRA. \\n\\n## Feedback\\nI would love to hear feedback on the model and on things that need improvement. Please [Join my Discord](https://discord.gg/VXmU2f5WEU). I am also ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' to suggestions for new features and improvements.\\n\\n## Control Implementation\\nThe technical stuff for those implementing tools and inference for it. General users can skip this part. \\n\\nThe control and inpainting implementation work similar to Flux controls, though the inpainting is slightly different. For Flux/Flex has a \\n16 channel latent space that is packed at half the size into a 64 channel latent. The controls can be added before or after packing, but for suimplicity,\\nI will describe them on a 16channel unpacked latent. \\n\\n(16ch = Noisy latent) + (16ch = VAE encoded inpainting image) + (1ch = inpainting mask) + (16ch = control input) = 49 channels\\n\\n- **16ch Noisy Latent** - This works as it has previously and is the normal input for a Flex/Flux model.\\n- **16ch VAE encoded inpainting image** - The inpainting image is encoded using the VAE without any masking. Once in the latent space. The desirec inpait section is zeroed out by multiplying by the inverted mask.\\nTo disable this input, simply feed 0s for these channels to indicate the entire image is inpainted.\\n- **1ch Inpainting mask** - The inpainting mask is a single channel image that indicates which pixels are inpainted. The mask is 0 for pixels in the inpainting image to keep and 1 for the area to inpaint. The mask can just be scaled down directly from the pixel space.\\nTo disable this input, along with inpainting, feed 1s for this channel to indicate the entire image is inpainted.\\n- **16ch Control input** - A VAE encoded control input. The model was trained on pose, line, and depth inputs. To disable control, simply feed 0s for these channels to indicate no control input.\\n\\nFor normal T2I generation without any controls or inputs, you simpley need. \\n```python\\nmodel_input = torch.cat([\\n    latent, # Noisy latent\\n    torch.zeros_like(latent), # VAE encoded inpainting image\\n    torch.ones_like(latent)[:, 0:1, :, :], # 0 - 1 inpaint mask\\n    torch.zeros_like(latent) # VAE encoded control input\\n], dim=1)\\n```\\n\\n## Support My Work\\n\\nIf you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ\\n\\n[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)\\n\\n### Current Sponsors\\n\\nAll of these people / organizations are the ones who selflessly make this project possible. Thank you!!\\n\\n_Last updated: 2025-04-23 18:04 UTC_\\n\\n<p align=\"center\">\\n<a href=\"https://github.com/replicate\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/60410876?v=4\" alt=\"Replicate\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<a href=\"https://github.com/josephrocca\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&v=4\" alt=\"josephrocca\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162524101/81a72689c3754ac5b9e38612ce5ce914/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=3XLSlLFCWAQ-0wd2_vZMikyotdQNSzKOjoyeoJiZEw0%3D\" alt=\"Prasanth Veerina\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<a href=\"https://github.com/weights-ai\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/185568492?v=4\" alt=\"Weights\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=qkRvrEc5gLPxaXxLvcvbYv1W1lcmOoTwhj4A9Cq5BxQ%3D\" alt=\"Vladimir Sotnikov\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/33158543\" alt=\"clement Delangue\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/54890369/45cea21d82974c78bf43956de7fb0e12/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=IK6OT6UpusHgdaC4y8IhK5XxXiP5TuLy3vjvgL77Fho%3D\" alt=\"Eli Slugworth\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=lpeicIh1_S-3Ji3W27gyiRB7iXurp8Bx8HAzDHftOuo%3D\" alt=\"Misch Strotz\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/93304\" alt=\"Joseph Rocca\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<a href=\"https://x.com/NuxZoe\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://pbs.twimg.com/profile_images/1714760743273574400/tdvQjNTl_400x400.jpg\" alt=\"tungsten\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=1B7dbXy_gAcPT9WXBesLhs7z_9APiz2k1Wx4Vml_-8Q%3D\" alt=\"Mohamed Oumoumad\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=Dv1NPKwdv9QT8fhYYwbGnQIvfiyqTUlh52bjDW1vYxY%3D\" alt=\"nitish PNR\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/548524\" alt=\"Steve Hanff\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IEKE18CBHVZ3k-08UD7Dkb7HbiFHb84W0FATdLMI0Dg%3D\" alt=\"Kristjan Retter\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83319230\" alt=\"Miguel Lara\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c8.patreon.com/3/200/8449560\" alt=\"Patron\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/131773947/eda3405aa582437db4582fce908c8739/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=S4Bh0sMqTNmJlo3uRr7co5d_kxvBjITemDTfi_1KrCA%3D\" alt=\"Jodh Singh\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=dpFFssZXZM_KZMKQhl3uDwwusdFw1c_v9x_ChJU7_zc%3D\" alt=\"David Garrido\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=lmuorehEKQBZnhiX6JejlV0zqrvNacGxhcxyi0Sdt5E%3D\" alt=\"Timothy Bielec\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/2410522\" alt=\"George Gostyshev\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/16287560/78130de30950410ca528d8a888997081/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=Ok-HSL2MthKXF09SmCOlPFCPfbMctFBZKCuTnPwxZ3A%3D\" alt=\"Vitaly Golubenko\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=wUzsI5cO5Evp2ukIGdSgBbvKeYgv5LSOQMa6Br33Rrs%3D\" alt=\"Al H\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/22809690\" alt=\"Michael Levine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-time=2145916800&token-hash=zK0dHe6A937WtNlrGdefoXFTPPzHUCfn__23HP8-Ui0%3D\" alt=\"Noctre\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-time=2145916800&token-hash=Rd_AjZGhMATVkZDf8E95ILc0n93gvvFWe1Ig0_dxwf4%3D\" alt=\"The Local Lab\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=93w8RMxwXlcM4X74t03u6P5_SrKvlm1IpjnD2SzVpJk%3D\" alt=\"EmmanuelMr18\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/338551/e8f257d8d3dd46c38272b391a5785948/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=GLom1rGgOZjBeO7I1OnjiIgWmjl6PO9ZjBB8YTvc7AM%3D\" alt=\"Plaidam\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=BpwC020pR3TRZ4r0RSCiSIOh-jmatkrpy1h2XU4sGa4%3D\" alt=\"Doron Adler\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=zvtBie29rRTKTXvAA2KhOI-l3mSMk9xxr-mg_CksLtc%3D\" alt=\"John Dopamine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=PpXK9B_iy288annlNdLOexhiQHbTftPEDeCh-sTQ2KA%3D\" alt=\"Armin Behjati\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=twmKs4mADF_h7bKh5jBuigYVScMeaeHv2pEPin9K0Dg%3D\" alt=\"Un Defined\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/45562978/0de33cf52ec642ae8a2f612cddec4ca6/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=hSAvaD4phiLcF0pvX7FP0juI5NQWCon-_TZSNpJzQJg%3D\" alt=\"Jack English\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/27791680\" alt=\"Jean-Tristan Marin\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/60995694/92e0e8f336eb4a5bb8d99b940247d1d1/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pj6Tm8XRdpGJcAEdnCakqYSNiSjoAYjvZescX7d0ic0%3D\" alt=\"Abraham Irawan\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/164958178/4eb7a37baa0541bab7a091f2b14615b7/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=_aaum7fBJAGaJhMBhlR8vqYavDhExdVxmO9mwd3_XMw%3D\" alt=\"Austin Robinson\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vNKojv67krNqx7gdpKBX1R_stX2TkMRYvRc0xZrbY6s%3D\" alt=\"Bharat Prabhakar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/70218846\" alt=\"Cosmosis\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/83054970/13de6cb103ad41a5841edf549e66cd51/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=wU_Eke9VYcfI40FAQvdEV84Xspqlo5VSiafLqhg_FOE%3D\" alt=\"Gili Ben Shahar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=LBmsSsMQZhO6yRZ_YyRwTgE6a7BVWrGNsAVveLXHXR0%3D\" alt=\"HestoySeghuro .\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4105384\" alt=\"Jack Blakely\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/494309\" alt=\"Julian Tsependa\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/24653779\" alt=\"RayHell\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4541423\" alt=\"S√∂ren \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/31950857/c567dc648f6144be9f6234946df05da2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=3Vx4R1eOfD4X_ZPPd40MsZ-3lyknLM35XmaHRELnWjM%3D\" alt=\"Trent Hunter\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/110407414/30f9e9d88ef945ddb0f47fd23a8cbac2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=QQRWOkMyOfDBERHn4O8N2wMB32zeiIEsydVTbSNUw-I%3D\" alt=\"Wesley Reitzfeld\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162398691/89d78d89eecb4d6b981ce8c3c6a3d4b8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=SWhI-0jGpY6Nc_bUQeXz4pa9DRURi9VnnnJ3Mxjg1po%3D\" alt=\"Zolt√°n-Csaba Nyir√≥\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pG3X2m-py2lRYI2aoJiXI47_4ArD78ZHdSm6jCAHA_w%3D\" alt=\"◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/140599287/cff037fb93804af28bc3a4f1e91154f8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vkscmpmFoM5wq7GnsLmOEgNhvyXe-774kNGNqD0wurE%3D\" alt=\"Lukas\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/96561218/b0694642d13a49faa75aec9762ff2aeb/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=sLQXomYm1iMYpknvGwKQ49f30TKQ0B1R2W3EZfCJqr8%3D\" alt=\"Ultimate Golf Archives\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=uzJzkUq9rte3wx8wDLjGAgvSoxdtZcAnH7HctDhdYEo%3D\" alt=\"Aaron Amortegui\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=SBphTD654nwr-OTrvIBIJBEQho7GE2PtRre8nyaG1Fk%3D\" alt=\"Albert Bukoski\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/49304261/d0a730de1c3349e585c49288b9f419c6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=C2BMZ3ci-Ty2nhnSwKZqsR-5hOGsUNDYcvXps0Geq9w%3D\" alt=\"Arvin Flores\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5048649\" alt=\"Ben Ward\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/130338124/f904a3bb76cd4588ac8d8f595c6cb486/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=k-inISRUtYDu9q7fNAKc3S2S7qcaw26fr1pj7PqU28Q%3D\" alt=\"Bnp\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=OAJc9W5Ak0uJfQ2COlo1Upo38K3aj1fMQFCMC7ft5tM%3D\" alt=\"Brian Smith\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/113207022/d4a67cc113e84fb69032bef71d068720/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=mu-tIg88VwoQdgLEOmxuVkhVm9JT59DdnHXJstmkkLU%3D\" alt=\"Fagem X\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5602036\" alt=\"Kelevra\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/358350\" alt=\"L D\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=9toslDfsO14QyaOiu6vIf--d4marBsWCZWN3gdPqbIU%3D\" alt=\"Marko jak\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=o6Hrpzw9rf2Ucd4cZ-hdUkGejLNv44-pqF8smeOF3ts%3D\" alt=\"Nicholas Agranoff\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-time=2145916800&token-hash=dr5eaMg3Ua0wyCy40Qv3F-ZFajWZmuz2fWG55FskREc%3D\" alt=\"Sapjes \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44738426/b01ff676da864d4ab9c21f226275b63e/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=54nIkcxFaGszJ3q0jNhtrVSBbV3WNK9e5WX9VzXltYk%3D\" alt=\"Shakeel Saleemi\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=S1QK78ief5byQU7tB_reqnw4V2zhW_cpwTqHThk-tGc%3D\" alt=\"the biitz\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83034\" alt=\"william tatum\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=RHYMcjr0UGIYw5FBrUfJdKMGuoYWhBQlLIykccEFJvo%3D\" alt=\"Zack Abrams\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/138787189/2b5662dcb638466282ac758e3ac651b4/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IlUAs9JAlVRphfx81V-Jt-nMiSBS8mPewRr9u6pQjaQ%3D\" alt=\"–ê–Ω—Ç–æ–Ω –ê–Ω—Ç–æ–Ω–∏–æ\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n\\n---\\n\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'ostris/Flex.2-preview', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '643cb43e6eeb746f5ad81c26', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643cb43e6eeb746f5ad81c26/_DUtzHpNtpTeDw7u0oYyX.png', 'fullname': 'Jaret Burkett', 'name': 'ostris', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 702}}, {'repoId': '68be0eacf1db8f4de3905526', 'repoOwnerId': '656efb1bc657c4341ddee888', 'isPrivate': False, 'type': 'model', 'likes': 4, 'isReadmeFile': True, 'readmeStartLine': 15, 'updatedAt': 1759403614425, 'repoName': 'dental-gpt-oss-20b', 'repoOwner': 'Wildstash', 'tags': 'transformers, safetensors, PEFT, Safetensors, dental, healthcare, medical, lora, fine-tuned, en, base_model:microsoft/DialoGPT-medium, base_model:adapter:microsoft/DialoGPT-medium, license:apache-2.0, endpoints_compatible, region:us', 'name': 'Wildstash/dental-gpt-oss-20b', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'dental-gpt-oss-20b', 'type': 'text'}], 'repoOwner': [{'text': 'Wildstash', 'type': 'text'}], 'fileContent': [{'text': '\\n# Dental-GPT-OSS-20B: ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Dental AI Assistant\\n\\n![Dental-GPT Architecture](image.png)\\n\\n**Dental-GPT-OSS-20B** is the first ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e large language model specifically fine-tuned for dental clinical practice. This model provides evidence-based diagnostic assistance, treatment planning, and clinical decision support for dental professionals.\\n\\n## Model Description\\n\\n- **Model Type**: Causal Language Model fine-tuned for dental clinical applications\\n- **Base Architecture**: 20B parameter transformer model\\n- **Fine-tuning Method**: LoRA (Low-Rank Adaptation) with PEFT\\n- **Training Data**: 2,494 expert-validated synthetic dental clinical cases\\n- **License**: Apache 2.0 (fully ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' for commercial use)\\n- **Language**: English\\n- **Domain**: Dental Medicine & Clinical Practice\\n\\n![Dataset Statistics](data/image/image2.png)\\n\\n## Key Features\\n\\nü¶∑ **Comprehensive Dental Coverage**\\n- All major dental specialties (endodontics, periodontics, oral surgery, prosthodontics)\\n- Evidence-based treatment recommendations\\n- Clinical guideline adherence\\n- Multi-step diagnostic reasoning\\n\\nüß† **Advanced Clinical Reasoning**\\n- Transparent decision-making process\\n- Differential diagnosis consideration\\n- Risk assessment and urgency classification\\n- Evidence-based treatment protocols\\n\\nüîí **Privacy-First Design**\\n- Trained exclusively on synthetic data\\n- Zero patient privacy concerns\\n- HIPAA-compliant development process\\n- No real patient information used\\n\\n## Training Details\\n\\n### Dataset Quality\\n- **Size**: 2,494 synthetic dental clinical cases\\n- **Validation**: Expert-reviewed by practicing dentists\\n- **Coverage**: Multi-specialty dental scenarios\\n- **Structure**: Patient presentation ‚Üí Clinical assessment ‚Üí Treatment plan\\n- **Dataset Access**: [Wildstash/dental-2.5k-instruct](https://huggingface.co/datasets/Wildstash/dental-2.5k-instruct)\\n\\n### Training Process\\n- **Method**: LoRA fine-tuning with 4-bit quantization\\n- **Base Model**: 20B parameter foundation model\\n- **Training Time**: Approximately 6-8 hours on multi-GPU setup\\n- **Optimization**: Gradient checkpointing, mixed precision training\\n- **Validation**: Cross-validated against clinical guidelines\\n\\n### Expert Validation\\n- Practicing dentists validated case accuracy\\n- Multi-specialty expert review process\\n- Clinical guideline compliance verification\\n- Iterative refinement based on professional feedback\\n\\n## Usage\\n\\n### Quick Start\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nfrom peft import PeftModel\\nimport torch\\n\\n# Load base model\\nbase_model = AutoModelForCausalLM.from_pretrained(\\n    \"openai/gpt-oss-20b\",\\n    device_map=\"auto\",\\n    torch_dtype=torch.bfloat16,\\n    trust_remote_code=True\\n)\\n\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\", trust_remote_code=True)\\n\\n# Load LoRA adapter\\nmodel = PeftModel.from_pretrained(base_model, \"Wildstash/dental-gpt-qlora\")\\n\\n# Example usage\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are an expert dental clinician providing comprehensive patient care.\"},\\n    {\"role\": \"user\", \"content\": \"Please evaluate this dental patient: 45M with severe tooth pain, swelling, fever 101¬∞F.\"}\\n]\\n\\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\\n\\nwith torch.no_grad():\\n    outputs = model.generate(\\n        **inputs,\\n        max_new_tokens=500,\\n        temperature=0.7,\\n        do_sample=True,\\n        pad_token_id=tokenizer.eos_token_id\\n    )\\n\\nresponse = tokenizer.decode(outputs[0][inputs[\\'input_ids\\'].shape[1]:], skip_special_tokens=True)\\nprint(response)\\n```\\n\\n### Example Output\\n\\n```\\nClinical Assessment\\n\\nPrimary Diagnosis: Acute apical abscess with systemic involvement\\nSecondary Considerations: Severe irreversible pulpitis with periapical extension\\n\\nClinical Reasoning:\\nSevere spontaneous pain indicates pulpal necrosis. Facial swelling suggests periapical infection spread. Fever (101¬∞F) indicates systemic bacterial involvement. This combination requires immediate intervention.\\n\\nImmediate Management\\n\\n1. Emergency Treatment:\\n   Incision and drainage if fluctuant swelling present\\n   Pulpectomy or extraction of affected tooth\\n   Establish drainage pathway\\n\\n2. Antibiotic Therapy: \\n   Amoxicillin 500mg TID x 7 days\\n   Alternative: Clindamycin 300mg QID if penicillin allergy\\n   Indication: Systemic signs (fever, malaise)\\n\\n3. Pain Management:\\n   Ibuprofen 600mg QID + Acetaminophen 1000mg QID\\n   Consider short-term narcotic if severe\\n\\nFollow-up Protocol\\n\\n24-48 hours: Reassess swelling and systemic symptoms\\n1 week: Evaluate antibiotic response and healing\\n2-4 weeks: Definitive treatment planning (RCT vs extraction)\\n\\nRed Flags - Refer Immediately If:\\nDifficulty swallowing or breathing\\nTrismus > 50% mouth opening limitation  \\nSpreading cellulitis beyond dental origin\\nTemperature > 102¬∞F or worsening systemic signs\\n\\nEvidence Base: American Association of Endodontists Guidelines for Antibiotic Use\\n```\\n\\n### Clinical Applications\\n\\n- **Diagnostic Assistance**: Differential diagnosis support\\n- **Treatment Planning**: Evidence-based recommendations\\n- **Educational Tool**: Training for dental students and residents\\n- **Clinical Decision Support**: Point-of-care guidance\\n- **Research Platform**: Benchmarking and development\\n\\n## Performance\\n\\n- **Diagnostic Accuracy**: 90%+ on expert-validated test cases\\n- **Clinical Appropriateness**: High adherence to professional guidelines\\n- **Reasoning Quality**: Transparent, step-by-step clinical logic\\n- **Coverage**: Comprehensive across dental specialties\\n\\n## Limitations\\n\\n- **Synthetic Training Data**: Not trained on real patient cases\\n- **General Dentistry Focus**: Specialized subspecialties may have limited coverage\\n- **Supplementary Tool**: Intended to assist, not replace, clinical judgment\\n- **Validation Scope**: Expert-reviewed but not clinically trialed\\n\\n## Ethical Considerations\\n\\n- **Privacy Protection**: No real patient data used in training\\n- **Transparency**: Clear limitations and intended use disclosed\\n- **Professional Standards**: Aligned with dental clinical guidelines\\n- **Responsible AI**: Designed to augment, not replace, professional expertise\\n\\n## Citation\\n\\n```bibtex\\n@model{dental-gpt-oss-20b,\\n  title={Dental-GPT-OSS-20B: ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Dental AI Assistant},\\n  author={Arnav Salkade},\\n  year={2024},\\n  url={https://huggingface.co/Wildstash/dental-gpt-oss-20b},\\n  note={Fine-tuned on synthetic dental clinical cases}\\n}\\n```\\n\\n## Training Infrastructure\\n\\n- **Framework**: Transformers, PEFT, bitsandbytes\\n- **Hardware**: Multi-GPU training with memory optimization\\n- **Techniques**: LoRA adaptation, gradient checkpointing\\n- **Monitoring**: Comprehensive training metrics and validation\\n\\n## Contact & Support\\n\\n- **Developer**: Arnav Salkade\\n- **Email**: itsarnavsalkade@gmail.com\\n- **Issues**: Please use the GitHub Issues tab for technical problems\\n- **Contributions**: Community contributions welcome\\n\\n## License\\n\\nThis model is released under the Apache License 2.0, allowing for both commercial and non-commercial use. See the LICENSE file for full details.\\n\\n---\\n\\n**Disclaimer**: This model is intended for educational and research purposes. Always consult with qualified dental professionals for actual patient care decisions.\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, PEFT, Safetensors, dental, healthcare, medical, lora, fine-tuned, en, base_model:microsoft/DialoGPT-medium, base_model:adapter:microsoft/DialoGPT-medium, license:apache-2.0, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'Wildstash/dental-gpt-oss-20b', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '656efb1bc657c4341ddee888', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/656efb1bc657c4341ddee888/RKNdGq8F5mYYz16qCe99U.png', 'fullname': 'ArnavS', 'name': 'Wildstash', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '66831e1c7ad6fe37b1ed7126', 'repoOwnerId': '65dd80a17fb5b9f96b63452f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759348786791, 'repoName': 'CodeAstra-7B', 'repoOwner': 'rootxhacker', 'tags': 'peft, safetensors, mistral, code, security, text-generation, en, license:apache-2.0, 4-bit, bitsandbytes, region:us', 'name': 'rootxhacker/CodeAstra-7B', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'CodeAstra-7B', 'type': 'text'}], 'repoOwner': [{'text': 'rootxhacker', 'type': 'text'}], 'fileContent': [{'text': '# CodeAstra-7b: ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': \" State-of-the-Art Vulnerability Detection Model üîçüõ°Ô∏è\\n\\n## Model Description\\n\\nCodeAstra-7b is a state-of-the-art language model fine-tuned for vulnerability detection in multiple programming languages. Based on the powerful Mistral-7B-Instruct-v0.2 model, CodeAstra-7b has been specifically trained to identify potential security vulnerabilities across a wide range of popular programming languages.\\n\\n### Key Features\\n\\n- üåê **Multi-language Support**: Detects vulnerabilities in Go, Python, C, C++, Fortran, Ruby, Java, Kotlin, C#, PHP, Swift, JavaScript, and TypeScript.\\n- üèÜ **State-of-the-Art Performance**: Achieves cutting-edge results in vulnerability detection tasks.\\n- üìä **Custom Dataset**: Trained on a proprietary dataset curated for comprehensive vulnerability detection.\\n- üñ•Ô∏è **Large-scale Training**: Utilized A100 GPUs for efficient and powerful training.\\n\\n## Performance Comparison üìä\\n\\nCodeAstra-7b significantly outperforms existing models in vulnerability detection accuracy. Here's a comparison table:\\n\\n|Model       | Accuracy (%) |\\n|-------------|--------------|\\n| gpt4o       | 88.78\\n| CodeAstra-7b| 83.00        |\\n| codebert-base-finetuned-detect-insecure-code        | 65.30        |\\n| CodeBERT    | 62.08        |\\n| RoBERTa     | 61.05        |\\n| TextCNN     | 60.69        |\\n| BiLSTM      | 59.37        |\\n\\nAs shown in the table, CodeAstra-7b achieves an impressive 83% accuracy, substantially surpassing other state-of-the-art models in the field of vulnerability detection.\\n\\n## Intended Use\\n\\nCodeAstra-7b is designed to assist developers, security researchers, and code auditors in identifying potential security vulnerabilities in \", 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' code. It can be integrated into development workflows, code review processes, or used as a standalone tool for code analysis.\\n\\n\\n### Multiple Vulnerability Scenarios\\n\\nIt\\'s important to note that while CodeAstra-7b excels at finding security issues in most cases, its performance may vary when multiple vulnerabilities are present in the same code snippet. In scenarios where two or three vulnerabilities coexist, the model might not always identify all of them correctly. Users should be aware of this limitation and consider using the model as part of a broader, multi-faceted security review process.\\n\\n## Training üèãÔ∏è\\u200d‚ôÇÔ∏è\\n\\nCodeAstra-7b was fine-tuned from the Mistral-7B-Instruct-v0.2 base model using a custom dataset specifically compiled for vulnerability detection across multiple programming languages. The training process leveraged A100 GPUs to ensure optimal performance and efficiency.\\n\\n## Usage üíª\\n\\nCodeAstra-7b was trained using PEFT (Parameter-Efficient Fine-Tuning). To use the model for vulnerability detection and code quality analysis, you can leverage the Hugging Face Transformers library along with PEFT. Here\\'s how to get started:\\n\\n```python\\nimport torch\\nfrom peft import PeftModel, PeftConfig\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Load the model and tokenizer\\npeft_model_id = \"rootxhacker/CodeAstra-7B\"\\nconfig = PeftConfig.from_pretrained(peft_model_id)\\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_4bit=True, device_map=\\'auto\\')\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\n\\n# Load the Lora model\\nmodel = PeftModel.from_pretrained(model, peft_model_id)\\n\\ndef get_completion(query, model, tokenizer):\\n    inputs = tokenizer(query, return_tensors=\"pt\")\\n    outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\n# Example usage\\ncode_to_analyze = \"\"\"\\ndef user_input():\\n    name = input(\"Enter your name: \")\\n    print(\"Hello, \" + name + \"!\")\\n\\nuser_input()\\n\"\"\"\\n\\nquery = f\"Analyze this code for vulnerabilities and quality issues:\\\\n{code_to_analyze}\"\\nresult = get_completion(query, model, tokenizer)\\nprint(result)\\n```\\n\\nThis script loads the CodeAstra-7b model, tokenizer, and provides a function to generate completions. You can use this setup to analyze code for vulnerabilities and quality issues.\\n\\n## Limitations ‚ö†Ô∏è\\n\\nWhile CodeAstra-7b represents a significant advancement in automated vulnerability detection and code quality analysis, it\\'s important to note that:\\n\\n1. The model may not catch all vulnerabilities or code quality issues and should be used as part of a comprehensive security and code review strategy.\\n2. In cases where multiple vulnerabilities (two or three) are present in the same code snippet, the model might not identify all of them correctly.\\n3. False positives are possible, and results should be verified by human experts.\\n4. The model\\'s performance may vary depending on the complexity and context of the code being analyzed.\\n5. CodeAstra\\'s performance depends on input code snippet length.\\n\\n## Test Aparatus\\n\\nI tested CodeAstra-7b against code snippets from dataset such as Cvefix , YesWeHack vulnerable code repository , Synthetically generated code using LLMs aand OWASP Juice Shop ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' code  \\nI ran all those vulnerable scripts against LLMs such as GPT4 , GPT4o etc for evaluation \\n\\n## Citation üìú\\n\\nIf you use CodeAstra-7b in your research or project, please cite it as follows:\\n\\n```\\n@software{CodeAstra-7b,\\n  author = {Harish Santhanalakshmi Ganesan},\\n  title = {CodeAstra-7b: State-of-the-Art Vulnerability Detection Model},\\n  year = {2024},\\n  howpublished = {\\\\url{https://huggingface.co/rootxhacker/CodeAstra-7b}}\\n}\\n```\\n\\n## License üìÑ\\n\\nCodeAstra-7b is released under the Apache License 2.0.\\n\\n```\\nCopyright 2024 [Harish Santhanalakshmi Ganesan]\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n```\\n\\n## Acknowledgements üôè\\n\\nWe would like to thank the Mistral AI team for their excellent base model, which served as the foundation for CodeAstra-7b.', 'type': 'text'}], 'tags': [{'text': 'peft, safetensors, mistral, code, security, text-generation, en, license:apache-2.0, 4-bit, bitsandbytes, region:us', 'type': 'text'}], 'name': [{'text': 'rootxhacker/CodeAstra-7B', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65dd80a17fb5b9f96b63452f', 'avatarUrl': '/avatars/aca3aa05bd9661bb89821700c8c891c0.svg', 'fullname': 'Harish Santhanalakshmi Ganesan', 'name': 'rootxhacker', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}}, {'repoId': '66dc658fd18fcbaf95226161', 'repoOwnerId': '66d2516141428ae33882c115', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759348897626, 'repoName': 'test', 'repoOwner': 'tgollogly', 'tags': 'region:us', 'name': 'tgollogly/test', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'test', 'type': 'text'}], 'repoOwner': [{'text': 'tgollogly', 'type': 'text'}], 'fileContent': [{'text': '## OpenCV: ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Computer Vision Library\\n\\n\\n### Resources\\n\\n* Homepage: <https://opencv.org>\\n  * Courses: <https://opencv.org/courses>\\n* Docs: <https://docs.opencv.org/4.x/>\\n* Q&A forum: <https://forum.opencv.org>\\n  * previous forum (read only): <http://answers.opencv.org>\\n* Issue tracking: <https://github.com/opencv/opencv/issues>\\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib>\\n* Donate to OpenCV: <https://opencv.org/support/>\\n\\n\\n### Contributing\\n\\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\\n\\n#### Summary of the guidelines:\\n\\n* One pull request per issue;\\n* Choose the right base branch;\\n* Include tests and documentation;\\n* Clean up \"oops\" commits before submitting;\\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\\n\\n### Additional Resources\\n\\n* [Submit your OpenCV-based project](https://form.jotform.com/233105358823151) for inclusion in Community Friday on opencv.org\\n* [Subscribe to the OpenCV YouTube Channel](http://youtube.com/@opencvofficial) featuring OpenCV Live, an hour-long streaming show\\n* [Follow OpenCV on LinkedIn](http://linkedin.com/company/opencv/) for daily posts showing the state-of-the-art in computer vision & AI\\n* [Apply to be an OpenCV Volunteer](https://form.jotform.com/232745316792159) to help organize events and online campaigns as well as amplify them\\n* [Follow OpenCV on Mastodon](http://mastodon.social/@opencv) in the Fediverse\\n* [Follow OpenCV on Twitter](https://twitter.com/opencvlive)\\n* [OpenCV.ai](https://opencv.ai): Computer Vision and AI development services from the OpenCV team.\\n', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'tgollogly/test', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '66d2516141428ae33882c115', 'avatarUrl': '/avatars/b9521ce816c1092f858aad9af602767c.svg', 'fullname': 'Thomas Gollogly', 'name': 'tgollogly', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '681244ea240145905986e06e', 'repoOwnerId': '666edf5c118821bf37bb03b6', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349526730, 'repoName': 'Flex.2-preview-int4', 'repoOwner': 'newgenai79', 'tags': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'name': 'newgenai79/Flex.2-preview-int4', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Flex.2-preview-int4', 'type': 'text'}], 'repoOwner': [{'text': 'newgenai79', 'type': 'text'}], 'fileContent': [{'text': '\\n# Flex.2-preview\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' 8B parameter Text to Image Diffusion Model with universal control and inpainting support built in. Early access preview release. The next version of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha)\\n\\n\\n## Features\\n\\n- 8 billion parameters\\n- Guidance embedder (2x as fast to generate)\\n- Built in inpainting\\n- Universal control input (line, pose, depth)\\n- Fine tunable\\n- OSI compliant license (Apache 2.0)\\n- 512 token length input\\n- 16 channel latent space\\n- Made by the community, for the community\\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/Flex2.controls-scaled.jpg\" alt=\"Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n## About\\n\\nFlex.2 is currently the most flexable text to image diffusion model released, making it truely live up to its name. It has taken a lot to get here: \\n\\n[Flux.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) -> [OpenFlux.1](https://huggingface.co/ostris/OpenFLUX.1) -> [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) -> [Flex.2-preview](https://huggingface.co/ostris/Flex.2-preview).\\n\\nNumerous improvements have been made with every step, but Flex.2 is the biggest step so far, and best of all, it was just trained by a some guy who relies 100% on community support to make a living and fund the outrageous compute cost needed to keep training models like this. Speaking of which, and since you brought it up, not me, I am always in need of support. Everything I create is free and ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', with permissive licenses. So if you find my work beneficial, or use it in a commercial setting, please consider contributing to my continued desire to live and develop more ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' projects and models. Visit the **Support** section below to find out how you can help out and see the awesome people who already do.\\n\\nFlex.2 preview is an early release to get feedback on the new features and to encourage experimentation and tooling. I would love to hear suggestions as well as be made aware of weak points \\nso I can address them while training continues. Flex.2 is a continuation of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha), with a lot of goodies thrown in. The most important \\nnew features and improvements over [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) are:\\n\\n- **Inpainting**: Flex.2 as built in inpainting support trained into the base model.\\n- **Universal Control**: It has a universal control input that has been trained to accept pose, line, and depth inputs. \\n\\nI wanted to put all the tools I use and love straight into the base model so one model is all you need to empower creativity far beyond what a simple text to image model could ever do on its own.\\n\\n## Usage\\n\\n### ComfyUI\\n\\nFlex.2 is supported in ComfyUI with the help of the Flex2 Conditioner node found in [ComfyUI-FlexTools](https://github.com/ostris/ComfyUI-FlexTools). I also recommend using [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to generate the control images (pose and depth) This conditioning node handles all of the controls and inpainting conditioning for you, but is also needed for normal T2I generation. So grab and install those first.\\n\\nFor the model, follow the directions for flux for the vae and text encoder models from the [ComfyUI examples](https://comfyanonymous.github.io/ComfyUI_examples/flux/). You dont need the diffusion_model. If you have Flux in Comfy, you can probably skip this step. Then download [Flex.2-preview.safetensors](https://huggingface.co/ostris/Flex.2-preview/blob/main/Flex.2-preview.safetensors) to `ComfyUI/models/diffusion_models/Flex.2-preview.safetensors` and restart Comfy. You can use the workflow in this image as a starting point example for controls and inpainting.\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/comfy_workflow.png\" alt=\"ComfyUI Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n### Diffusers\\n\\nFor diffusers, you can use `AutoPipelineForText2Image`. This will load the model with the pipeline located [here](https://huggingface.co/ostris/Flex.2-preview/blob/main/pipeline.py). We will run this example using the `diffusers` library. \\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/diffusers_example.jpg\" alt=\"Diffusers Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\nFirst install the requirements.\\n\\n```bash\\npip install --upgrade torch accelerate transformers diffusers\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n\\n```\\n\\nFor consumer cards < 24GB, you can use torchao to\\n\\n```bash\\npip install --upgrade torchao\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\nfrom transformers import T5EncoderModel, TorchAoConfig\\nfrom diffusers import FluxTransformer2DModel\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\nquant_config = TorchAoConfig(\"int8_weight_only\")\\n\\ntext_encoder_2 = T5EncoderModel.from_pretrained(\\n    name_or_path, subfolder=\"text_encoder_2\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\ntransformer = FluxTransformer2DModel.from_pretrained(\\n    name_or_path, subfolder=\"transformer\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    transformer=transformer,\\n    text_encoder_2=text_encoder_2,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n```\\n\\n\\n\\n## Limitations\\n\\nThis model is still experimental and under active development. The training for the controls and inpainting was pretty aggressive, and the model currently struggles with a few\\nthings such an anatomy and text. The inpainting is also still being worked on. This is just\\na preview release. I am working on improving these limitations with each new training run.\\n\\n## Fine Tuning\\n\\nFlex.2 is designed to be fine tunable, though the best pratice for doing so is still highly experimental as a model like\\nthis has not really existed before. You can train traditional LoRAs directly on a model that can do controls and inpainting. Flex.1-alpha LoRAs also tento to work well with it. Day 1 support is already live for LoRA training with \\n[AI-Toolkit](https://github.com/ostris/ai-toolkit) with built in functionality to automatically generate the controls and inpainting inputs for your existing datasets. Check out the example config file and read through the comments \\nto see how it works [train_lora_flex2_24gb.yaml](https://github.com/ostris/ai-toolkit/blob/main/config/examples/train_lora_flex2_24gb.yaml)\\n\\nYou can also make your own controls and teach the model to use them by training a simple LoRA in no more time than it would take to train a normal LoRA. \\n\\n## Feedback\\nI would love to hear feedback on the model and on things that need improvement. Please [Join my Discord](https://discord.gg/VXmU2f5WEU). I am also ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' to suggestions for new features and improvements.\\n\\n## Control Implementation\\nThe technical stuff for those implementing tools and inference for it. General users can skip this part. \\n\\nThe control and inpainting implementation work similar to Flux controls, though the inpainting is slightly different. For Flux/Flex has a \\n16 channel latent space that is packed at half the size into a 64 channel latent. The controls can be added before or after packing, but for suimplicity,\\nI will describe them on a 16channel unpacked latent. \\n\\n(16ch = Noisy latent) + (16ch = VAE encoded inpainting image) + (1ch = inpainting mask) + (16ch = control input) = 49 channels\\n\\n- **16ch Noisy Latent** - This works as it has previously and is the normal input for a Flex/Flux model.\\n- **16ch VAE encoded inpainting image** - The inpainting image is encoded using the VAE without any masking. Once in the latent space. The desirec inpait section is zeroed out by multiplying by the inverted mask.\\nTo disable this input, simply feed 0s for these channels to indicate the entire image is inpainted.\\n- **1ch Inpainting mask** - The inpainting mask is a single channel image that indicates which pixels are inpainted. The mask is 0 for pixels in the inpainting image to keep and 1 for the area to inpaint. The mask can just be scaled down directly from the pixel space.\\nTo disable this input, along with inpainting, feed 1s for this channel to indicate the entire image is inpainted.\\n- **16ch Control input** - A VAE encoded control input. The model was trained on pose, line, and depth inputs. To disable control, simply feed 0s for these channels to indicate no control input.\\n\\nFor normal T2I generation without any controls or inputs, you simpley need. \\n```python\\nmodel_input = torch.cat([\\n    latent, # Noisy latent\\n    torch.zeros_like(latent), # VAE encoded inpainting image\\n    torch.ones_like(latent)[:, 0:1, :, :], # 0 - 1 inpaint mask\\n    torch.zeros_like(latent) # VAE encoded control input\\n], dim=1)\\n```\\n\\n## Support My Work\\n\\nIf you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ\\n\\n[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)\\n\\n### Current Sponsors\\n\\nAll of these people / organizations are the ones who selflessly make this project possible. Thank you!!\\n\\n_Last updated: 2025-04-23 18:04 UTC_\\n\\n<p align=\"center\">\\n<a href=\"https://github.com/replicate\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/60410876?v=4\" alt=\"Replicate\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<a href=\"https://github.com/josephrocca\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&v=4\" alt=\"josephrocca\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162524101/81a72689c3754ac5b9e38612ce5ce914/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=3XLSlLFCWAQ-0wd2_vZMikyotdQNSzKOjoyeoJiZEw0%3D\" alt=\"Prasanth Veerina\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<a href=\"https://github.com/weights-ai\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/185568492?v=4\" alt=\"Weights\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=qkRvrEc5gLPxaXxLvcvbYv1W1lcmOoTwhj4A9Cq5BxQ%3D\" alt=\"Vladimir Sotnikov\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/33158543\" alt=\"clement Delangue\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/54890369/45cea21d82974c78bf43956de7fb0e12/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=IK6OT6UpusHgdaC4y8IhK5XxXiP5TuLy3vjvgL77Fho%3D\" alt=\"Eli Slugworth\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=lpeicIh1_S-3Ji3W27gyiRB7iXurp8Bx8HAzDHftOuo%3D\" alt=\"Misch Strotz\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/93304\" alt=\"Joseph Rocca\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<a href=\"https://x.com/NuxZoe\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://pbs.twimg.com/profile_images/1714760743273574400/tdvQjNTl_400x400.jpg\" alt=\"tungsten\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=1B7dbXy_gAcPT9WXBesLhs7z_9APiz2k1Wx4Vml_-8Q%3D\" alt=\"Mohamed Oumoumad\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=Dv1NPKwdv9QT8fhYYwbGnQIvfiyqTUlh52bjDW1vYxY%3D\" alt=\"nitish PNR\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/548524\" alt=\"Steve Hanff\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IEKE18CBHVZ3k-08UD7Dkb7HbiFHb84W0FATdLMI0Dg%3D\" alt=\"Kristjan Retter\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83319230\" alt=\"Miguel Lara\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c8.patreon.com/3/200/8449560\" alt=\"Patron\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/131773947/eda3405aa582437db4582fce908c8739/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=S4Bh0sMqTNmJlo3uRr7co5d_kxvBjITemDTfi_1KrCA%3D\" alt=\"Jodh Singh\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=dpFFssZXZM_KZMKQhl3uDwwusdFw1c_v9x_ChJU7_zc%3D\" alt=\"David Garrido\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=lmuorehEKQBZnhiX6JejlV0zqrvNacGxhcxyi0Sdt5E%3D\" alt=\"Timothy Bielec\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/2410522\" alt=\"George Gostyshev\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/16287560/78130de30950410ca528d8a888997081/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=Ok-HSL2MthKXF09SmCOlPFCPfbMctFBZKCuTnPwxZ3A%3D\" alt=\"Vitaly Golubenko\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=wUzsI5cO5Evp2ukIGdSgBbvKeYgv5LSOQMa6Br33Rrs%3D\" alt=\"Al H\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/22809690\" alt=\"Michael Levine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-time=2145916800&token-hash=zK0dHe6A937WtNlrGdefoXFTPPzHUCfn__23HP8-Ui0%3D\" alt=\"Noctre\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-time=2145916800&token-hash=Rd_AjZGhMATVkZDf8E95ILc0n93gvvFWe1Ig0_dxwf4%3D\" alt=\"The Local Lab\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=93w8RMxwXlcM4X74t03u6P5_SrKvlm1IpjnD2SzVpJk%3D\" alt=\"EmmanuelMr18\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/338551/e8f257d8d3dd46c38272b391a5785948/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=GLom1rGgOZjBeO7I1OnjiIgWmjl6PO9ZjBB8YTvc7AM%3D\" alt=\"Plaidam\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=BpwC020pR3TRZ4r0RSCiSIOh-jmatkrpy1h2XU4sGa4%3D\" alt=\"Doron Adler\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=zvtBie29rRTKTXvAA2KhOI-l3mSMk9xxr-mg_CksLtc%3D\" alt=\"John Dopamine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=PpXK9B_iy288annlNdLOexhiQHbTftPEDeCh-sTQ2KA%3D\" alt=\"Armin Behjati\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=twmKs4mADF_h7bKh5jBuigYVScMeaeHv2pEPin9K0Dg%3D\" alt=\"Un Defined\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/45562978/0de33cf52ec642ae8a2f612cddec4ca6/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=hSAvaD4phiLcF0pvX7FP0juI5NQWCon-_TZSNpJzQJg%3D\" alt=\"Jack English\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/27791680\" alt=\"Jean-Tristan Marin\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/60995694/92e0e8f336eb4a5bb8d99b940247d1d1/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pj6Tm8XRdpGJcAEdnCakqYSNiSjoAYjvZescX7d0ic0%3D\" alt=\"Abraham Irawan\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/164958178/4eb7a37baa0541bab7a091f2b14615b7/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=_aaum7fBJAGaJhMBhlR8vqYavDhExdVxmO9mwd3_XMw%3D\" alt=\"Austin Robinson\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vNKojv67krNqx7gdpKBX1R_stX2TkMRYvRc0xZrbY6s%3D\" alt=\"Bharat Prabhakar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/70218846\" alt=\"Cosmosis\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/83054970/13de6cb103ad41a5841edf549e66cd51/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=wU_Eke9VYcfI40FAQvdEV84Xspqlo5VSiafLqhg_FOE%3D\" alt=\"Gili Ben Shahar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=LBmsSsMQZhO6yRZ_YyRwTgE6a7BVWrGNsAVveLXHXR0%3D\" alt=\"HestoySeghuro .\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4105384\" alt=\"Jack Blakely\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/494309\" alt=\"Julian Tsependa\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/24653779\" alt=\"RayHell\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4541423\" alt=\"S√∂ren \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/31950857/c567dc648f6144be9f6234946df05da2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=3Vx4R1eOfD4X_ZPPd40MsZ-3lyknLM35XmaHRELnWjM%3D\" alt=\"Trent Hunter\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/110407414/30f9e9d88ef945ddb0f47fd23a8cbac2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=QQRWOkMyOfDBERHn4O8N2wMB32zeiIEsydVTbSNUw-I%3D\" alt=\"Wesley Reitzfeld\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162398691/89d78d89eecb4d6b981ce8c3c6a3d4b8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=SWhI-0jGpY6Nc_bUQeXz4pa9DRURi9VnnnJ3Mxjg1po%3D\" alt=\"Zolt√°n-Csaba Nyir√≥\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pG3X2m-py2lRYI2aoJiXI47_4ArD78ZHdSm6jCAHA_w%3D\" alt=\"◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/140599287/cff037fb93804af28bc3a4f1e91154f8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vkscmpmFoM5wq7GnsLmOEgNhvyXe-774kNGNqD0wurE%3D\" alt=\"Lukas\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/96561218/b0694642d13a49faa75aec9762ff2aeb/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=sLQXomYm1iMYpknvGwKQ49f30TKQ0B1R2W3EZfCJqr8%3D\" alt=\"Ultimate Golf Archives\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=uzJzkUq9rte3wx8wDLjGAgvSoxdtZcAnH7HctDhdYEo%3D\" alt=\"Aaron Amortegui\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=SBphTD654nwr-OTrvIBIJBEQho7GE2PtRre8nyaG1Fk%3D\" alt=\"Albert Bukoski\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/49304261/d0a730de1c3349e585c49288b9f419c6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=C2BMZ3ci-Ty2nhnSwKZqsR-5hOGsUNDYcvXps0Geq9w%3D\" alt=\"Arvin Flores\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5048649\" alt=\"Ben Ward\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/130338124/f904a3bb76cd4588ac8d8f595c6cb486/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=k-inISRUtYDu9q7fNAKc3S2S7qcaw26fr1pj7PqU28Q%3D\" alt=\"Bnp\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=OAJc9W5Ak0uJfQ2COlo1Upo38K3aj1fMQFCMC7ft5tM%3D\" alt=\"Brian Smith\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/113207022/d4a67cc113e84fb69032bef71d068720/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=mu-tIg88VwoQdgLEOmxuVkhVm9JT59DdnHXJstmkkLU%3D\" alt=\"Fagem X\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5602036\" alt=\"Kelevra\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/358350\" alt=\"L D\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=9toslDfsO14QyaOiu6vIf--d4marBsWCZWN3gdPqbIU%3D\" alt=\"Marko jak\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=o6Hrpzw9rf2Ucd4cZ-hdUkGejLNv44-pqF8smeOF3ts%3D\" alt=\"Nicholas Agranoff\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-time=2145916800&token-hash=dr5eaMg3Ua0wyCy40Qv3F-ZFajWZmuz2fWG55FskREc%3D\" alt=\"Sapjes \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44738426/b01ff676da864d4ab9c21f226275b63e/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=54nIkcxFaGszJ3q0jNhtrVSBbV3WNK9e5WX9VzXltYk%3D\" alt=\"Shakeel Saleemi\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=S1QK78ief5byQU7tB_reqnw4V2zhW_cpwTqHThk-tGc%3D\" alt=\"the biitz\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83034\" alt=\"william tatum\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=RHYMcjr0UGIYw5FBrUfJdKMGuoYWhBQlLIykccEFJvo%3D\" alt=\"Zack Abrams\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/138787189/2b5662dcb638466282ac758e3ac651b4/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IlUAs9JAlVRphfx81V-Jt-nMiSBS8mPewRr9u6pQjaQ%3D\" alt=\"–ê–Ω—Ç–æ–Ω –ê–Ω—Ç–æ–Ω–∏–æ\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n\\n---\\n\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'newgenai79/Flex.2-preview-int4', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '666edf5c118821bf37bb03b6', 'avatarUrl': '/avatars/b95a21f5007de8e045ae4207c27d9dfa.svg', 'fullname': 'New GenAI', 'name': 'newgenai79', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}}, {'repoId': '681244faabd1646751059319', 'repoOwnerId': '666edf5c118821bf37bb03b6', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349527273, 'repoName': 'Flex.2-preview-int8', 'repoOwner': 'newgenai79', 'tags': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'name': 'newgenai79/Flex.2-preview-int8', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Flex.2-preview-int8', 'type': 'text'}], 'repoOwner': [{'text': 'newgenai79', 'type': 'text'}], 'fileContent': [{'text': '\\n# Flex.2-preview\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' 8B parameter Text to Image Diffusion Model with universal control and inpainting support built in. Early access preview release. The next version of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha)\\n\\n\\n## Features\\n\\n- 8 billion parameters\\n- Guidance embedder (2x as fast to generate)\\n- Built in inpainting\\n- Universal control input (line, pose, depth)\\n- Fine tunable\\n- OSI compliant license (Apache 2.0)\\n- 512 token length input\\n- 16 channel latent space\\n- Made by the community, for the community\\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/Flex2.controls-scaled.jpg\" alt=\"Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n## About\\n\\nFlex.2 is currently the most flexable text to image diffusion model released, making it truely live up to its name. It has taken a lot to get here: \\n\\n[Flux.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) -> [OpenFlux.1](https://huggingface.co/ostris/OpenFLUX.1) -> [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) -> [Flex.2-preview](https://huggingface.co/ostris/Flex.2-preview).\\n\\nNumerous improvements have been made with every step, but Flex.2 is the biggest step so far, and best of all, it was just trained by a some guy who relies 100% on community support to make a living and fund the outrageous compute cost needed to keep training models like this. Speaking of which, and since you brought it up, not me, I am always in need of support. Everything I create is free and ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', with permissive licenses. So if you find my work beneficial, or use it in a commercial setting, please consider contributing to my continued desire to live and develop more ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' projects and models. Visit the **Support** section below to find out how you can help out and see the awesome people who already do.\\n\\nFlex.2 preview is an early release to get feedback on the new features and to encourage experimentation and tooling. I would love to hear suggestions as well as be made aware of weak points \\nso I can address them while training continues. Flex.2 is a continuation of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha), with a lot of goodies thrown in. The most important \\nnew features and improvements over [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) are:\\n\\n- **Inpainting**: Flex.2 as built in inpainting support trained into the base model.\\n- **Universal Control**: It has a universal control input that has been trained to accept pose, line, and depth inputs. \\n\\nI wanted to put all the tools I use and love straight into the base model so one model is all you need to empower creativity far beyond what a simple text to image model could ever do on its own.\\n\\n## Usage\\n\\n### ComfyUI\\n\\nFlex.2 is supported in ComfyUI with the help of the Flex2 Conditioner node found in [ComfyUI-FlexTools](https://github.com/ostris/ComfyUI-FlexTools). I also recommend using [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to generate the control images (pose and depth) This conditioning node handles all of the controls and inpainting conditioning for you, but is also needed for normal T2I generation. So grab and install those first.\\n\\nFor the model, follow the directions for flux for the vae and text encoder models from the [ComfyUI examples](https://comfyanonymous.github.io/ComfyUI_examples/flux/). You dont need the diffusion_model. If you have Flux in Comfy, you can probably skip this step. Then download [Flex.2-preview.safetensors](https://huggingface.co/ostris/Flex.2-preview/blob/main/Flex.2-preview.safetensors) to `ComfyUI/models/diffusion_models/Flex.2-preview.safetensors` and restart Comfy. You can use the workflow in this image as a starting point example for controls and inpainting.\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/comfy_workflow.png\" alt=\"ComfyUI Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n### Diffusers\\n\\nFor diffusers, you can use `AutoPipelineForText2Image`. This will load the model with the pipeline located [here](https://huggingface.co/ostris/Flex.2-preview/blob/main/pipeline.py). We will run this example using the `diffusers` library. \\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/diffusers_example.jpg\" alt=\"Diffusers Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\nFirst install the requirements.\\n\\n```bash\\npip install --upgrade torch accelerate transformers diffusers\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n\\n```\\n\\nFor consumer cards < 24GB, you can use torchao to\\n\\n```bash\\npip install --upgrade torchao\\n```\\n\\nThen you can load the model and use it like this:\\n\\n```python\\nimport torch\\nfrom diffusers import AutoPipelineForText2Image\\nfrom diffusers.utils import load_image\\nfrom transformers import T5EncoderModel, TorchAoConfig\\nfrom diffusers import FluxTransformer2DModel\\n\\nname_or_path = \"ostris/Flex.2-preview\"\\n\\ninpaint_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog.jpg\")\\ninpaint_mask = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_mask.jpg\")\\ncontrol_image = load_image(\"https://ostris.com/wp-content/uploads/2025/04/dog_depth.jpg\")\\n\\ndtype = torch.bfloat16\\n\\nquant_config = TorchAoConfig(\"int8_weight_only\")\\n\\ntext_encoder_2 = T5EncoderModel.from_pretrained(\\n    name_or_path, subfolder=\"text_encoder_2\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\ntransformer = FluxTransformer2DModel.from_pretrained(\\n    name_or_path, subfolder=\"transformer\", torch_dtype=dtype, quantization_config=quant_config\\n).to(\"cuda\")\\n\\n\\npipe = AutoPipelineForText2Image.from_pretrained(\\n    name_or_path,\\n    transformer=transformer,\\n    text_encoder_2=text_encoder_2,\\n    custom_pipeline=name_or_path,\\n    torch_dtype=dtype\\n).to(\"cuda\")\\n\\nimage = pipe(\\n    prompt=\"A white friendly robotic dog sitting on a bench\",\\n    inpaint_image=inpaint_image,\\n    inpaint_mask=inpaint_mask,\\n    control_image=control_image,\\n    control_strength=0.5,\\n    control_stop=0.33,\\n    height=1024,\\n    width=1024,\\n    guidance_scale=3.5,\\n    num_inference_steps=50,\\n    generator=torch.Generator(\"cpu\").manual_seed(42)\\n).images[0]\\nimage.save(f\"robot_dog.png\")\\n```\\n\\n\\n\\n## Limitations\\n\\nThis model is still experimental and under active development. The training for the controls and inpainting was pretty aggressive, and the model currently struggles with a few\\nthings such an anatomy and text. The inpainting is also still being worked on. This is just\\na preview release. I am working on improving these limitations with each new training run.\\n\\n## Fine Tuning\\n\\nFlex.2 is designed to be fine tunable, though the best pratice for doing so is still highly experimental as a model like\\nthis has not really existed before. You can train traditional LoRAs directly on a model that can do controls and inpainting. Flex.1-alpha LoRAs also tento to work well with it. Day 1 support is already live for LoRA training with \\n[AI-Toolkit](https://github.com/ostris/ai-toolkit) with built in functionality to automatically generate the controls and inpainting inputs for your existing datasets. Check out the example config file and read through the comments \\nto see how it works [train_lora_flex2_24gb.yaml](https://github.com/ostris/ai-toolkit/blob/main/config/examples/train_lora_flex2_24gb.yaml)\\n\\nYou can also make your own controls and teach the model to use them by training a simple LoRA in no more time than it would take to train a normal LoRA. \\n\\n## Feedback\\nI would love to hear feedback on the model and on things that need improvement. Please [Join my Discord](https://discord.gg/VXmU2f5WEU). I am also ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' to suggestions for new features and improvements.\\n\\n## Control Implementation\\nThe technical stuff for those implementing tools and inference for it. General users can skip this part. \\n\\nThe control and inpainting implementation work similar to Flux controls, though the inpainting is slightly different. For Flux/Flex has a \\n16 channel latent space that is packed at half the size into a 64 channel latent. The controls can be added before or after packing, but for suimplicity,\\nI will describe them on a 16channel unpacked latent. \\n\\n(16ch = Noisy latent) + (16ch = VAE encoded inpainting image) + (1ch = inpainting mask) + (16ch = control input) = 49 channels\\n\\n- **16ch Noisy Latent** - This works as it has previously and is the normal input for a Flex/Flux model.\\n- **16ch VAE encoded inpainting image** - The inpainting image is encoded using the VAE without any masking. Once in the latent space. The desirec inpait section is zeroed out by multiplying by the inverted mask.\\nTo disable this input, simply feed 0s for these channels to indicate the entire image is inpainted.\\n- **1ch Inpainting mask** - The inpainting mask is a single channel image that indicates which pixels are inpainted. The mask is 0 for pixels in the inpainting image to keep and 1 for the area to inpaint. The mask can just be scaled down directly from the pixel space.\\nTo disable this input, along with inpainting, feed 1s for this channel to indicate the entire image is inpainted.\\n- **16ch Control input** - A VAE encoded control input. The model was trained on pose, line, and depth inputs. To disable control, simply feed 0s for these channels to indicate no control input.\\n\\nFor normal T2I generation without any controls or inputs, you simpley need. \\n```python\\nmodel_input = torch.cat([\\n    latent, # Noisy latent\\n    torch.zeros_like(latent), # VAE encoded inpainting image\\n    torch.ones_like(latent)[:, 0:1, :, :], # 0 - 1 inpaint mask\\n    torch.zeros_like(latent) # VAE encoded control input\\n], dim=1)\\n```\\n\\n## Support My Work\\n\\nIf you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ\\n\\n[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)\\n\\n### Current Sponsors\\n\\nAll of these people / organizations are the ones who selflessly make this project possible. Thank you!!\\n\\n_Last updated: 2025-04-23 18:04 UTC_\\n\\n<p align=\"center\">\\n<a href=\"https://github.com/replicate\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/60410876?v=4\" alt=\"Replicate\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<a href=\"https://github.com/josephrocca\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&v=4\" alt=\"josephrocca\" width=\"200\" height=\"200\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162524101/81a72689c3754ac5b9e38612ce5ce914/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=3XLSlLFCWAQ-0wd2_vZMikyotdQNSzKOjoyeoJiZEw0%3D\" alt=\"Prasanth Veerina\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<a href=\"https://github.com/weights-ai\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://avatars.githubusercontent.com/u/185568492?v=4\" alt=\"Weights\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=qkRvrEc5gLPxaXxLvcvbYv1W1lcmOoTwhj4A9Cq5BxQ%3D\" alt=\"Vladimir Sotnikov\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/33158543\" alt=\"clement Delangue\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/54890369/45cea21d82974c78bf43956de7fb0e12/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=IK6OT6UpusHgdaC4y8IhK5XxXiP5TuLy3vjvgL77Fho%3D\" alt=\"Eli Slugworth\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=lpeicIh1_S-3Ji3W27gyiRB7iXurp8Bx8HAzDHftOuo%3D\" alt=\"Misch Strotz\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/93304\" alt=\"Joseph Rocca\" width=\"150\" height=\"150\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<a href=\"https://x.com/NuxZoe\" target=\"_blank\" rel=\"noopener noreferrer\"><img src=\"https://pbs.twimg.com/profile_images/1714760743273574400/tdvQjNTl_400x400.jpg\" alt=\"tungsten\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\"></a>\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=1B7dbXy_gAcPT9WXBesLhs7z_9APiz2k1Wx4Vml_-8Q%3D\" alt=\"Mohamed Oumoumad\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=Dv1NPKwdv9QT8fhYYwbGnQIvfiyqTUlh52bjDW1vYxY%3D\" alt=\"nitish PNR\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/548524\" alt=\"Steve Hanff\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IEKE18CBHVZ3k-08UD7Dkb7HbiFHb84W0FATdLMI0Dg%3D\" alt=\"Kristjan Retter\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83319230\" alt=\"Miguel Lara\" width=\"100\" height=\"100\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n<hr style=\"width:100%;border:none;height:2px;background:#ddd;margin:30px 0;\">\\n<p align=\"center\">\\n<img src=\"https://c8.patreon.com/3/200/8449560\" alt=\"Patron\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/131773947/eda3405aa582437db4582fce908c8739/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=S4Bh0sMqTNmJlo3uRr7co5d_kxvBjITemDTfi_1KrCA%3D\" alt=\"Jodh Singh\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=dpFFssZXZM_KZMKQhl3uDwwusdFw1c_v9x_ChJU7_zc%3D\" alt=\"David Garrido\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=lmuorehEKQBZnhiX6JejlV0zqrvNacGxhcxyi0Sdt5E%3D\" alt=\"Timothy Bielec\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/2410522\" alt=\"George Gostyshev\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/16287560/78130de30950410ca528d8a888997081/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=Ok-HSL2MthKXF09SmCOlPFCPfbMctFBZKCuTnPwxZ3A%3D\" alt=\"Vitaly Golubenko\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=wUzsI5cO5Evp2ukIGdSgBbvKeYgv5LSOQMa6Br33Rrs%3D\" alt=\"Al H\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/22809690\" alt=\"Michael Levine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-time=2145916800&token-hash=zK0dHe6A937WtNlrGdefoXFTPPzHUCfn__23HP8-Ui0%3D\" alt=\"Noctre\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-time=2145916800&token-hash=Rd_AjZGhMATVkZDf8E95ILc0n93gvvFWe1Ig0_dxwf4%3D\" alt=\"The Local Lab\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=93w8RMxwXlcM4X74t03u6P5_SrKvlm1IpjnD2SzVpJk%3D\" alt=\"EmmanuelMr18\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/338551/e8f257d8d3dd46c38272b391a5785948/eyJ3IjoyMDB9/1.jpg?token-time=2145916800&token-hash=GLom1rGgOZjBeO7I1OnjiIgWmjl6PO9ZjBB8YTvc7AM%3D\" alt=\"Plaidam\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=BpwC020pR3TRZ4r0RSCiSIOh-jmatkrpy1h2XU4sGa4%3D\" alt=\"Doron Adler\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-time=2145916800&token-hash=zvtBie29rRTKTXvAA2KhOI-l3mSMk9xxr-mg_CksLtc%3D\" alt=\"John Dopamine\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=PpXK9B_iy288annlNdLOexhiQHbTftPEDeCh-sTQ2KA%3D\" alt=\"Armin Behjati\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=twmKs4mADF_h7bKh5jBuigYVScMeaeHv2pEPin9K0Dg%3D\" alt=\"Un Defined\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/45562978/0de33cf52ec642ae8a2f612cddec4ca6/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=hSAvaD4phiLcF0pvX7FP0juI5NQWCon-_TZSNpJzQJg%3D\" alt=\"Jack English\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/27791680\" alt=\"Jean-Tristan Marin\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/60995694/92e0e8f336eb4a5bb8d99b940247d1d1/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pj6Tm8XRdpGJcAEdnCakqYSNiSjoAYjvZescX7d0ic0%3D\" alt=\"Abraham Irawan\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/164958178/4eb7a37baa0541bab7a091f2b14615b7/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=_aaum7fBJAGaJhMBhlR8vqYavDhExdVxmO9mwd3_XMw%3D\" alt=\"Austin Robinson\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vNKojv67krNqx7gdpKBX1R_stX2TkMRYvRc0xZrbY6s%3D\" alt=\"Bharat Prabhakar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/70218846\" alt=\"Cosmosis\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/83054970/13de6cb103ad41a5841edf549e66cd51/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=wU_Eke9VYcfI40FAQvdEV84Xspqlo5VSiafLqhg_FOE%3D\" alt=\"Gili Ben Shahar\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=LBmsSsMQZhO6yRZ_YyRwTgE6a7BVWrGNsAVveLXHXR0%3D\" alt=\"HestoySeghuro .\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4105384\" alt=\"Jack Blakely\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/494309\" alt=\"Julian Tsependa\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/24653779\" alt=\"RayHell\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/4541423\" alt=\"S√∂ren \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/31950857/c567dc648f6144be9f6234946df05da2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=3Vx4R1eOfD4X_ZPPd40MsZ-3lyknLM35XmaHRELnWjM%3D\" alt=\"Trent Hunter\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/110407414/30f9e9d88ef945ddb0f47fd23a8cbac2/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=QQRWOkMyOfDBERHn4O8N2wMB32zeiIEsydVTbSNUw-I%3D\" alt=\"Wesley Reitzfeld\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/162398691/89d78d89eecb4d6b981ce8c3c6a3d4b8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=SWhI-0jGpY6Nc_bUQeXz4pa9DRURi9VnnnJ3Mxjg1po%3D\" alt=\"Zolt√°n-Csaba Nyir√≥\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=pG3X2m-py2lRYI2aoJiXI47_4ArD78ZHdSm6jCAHA_w%3D\" alt=\"◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/140599287/cff037fb93804af28bc3a4f1e91154f8/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=vkscmpmFoM5wq7GnsLmOEgNhvyXe-774kNGNqD0wurE%3D\" alt=\"Lukas\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/96561218/b0694642d13a49faa75aec9762ff2aeb/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=sLQXomYm1iMYpknvGwKQ49f30TKQ0B1R2W3EZfCJqr8%3D\" alt=\"Ultimate Golf Archives\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=uzJzkUq9rte3wx8wDLjGAgvSoxdtZcAnH7HctDhdYEo%3D\" alt=\"Aaron Amortegui\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=SBphTD654nwr-OTrvIBIJBEQho7GE2PtRre8nyaG1Fk%3D\" alt=\"Albert Bukoski\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/49304261/d0a730de1c3349e585c49288b9f419c6/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=C2BMZ3ci-Ty2nhnSwKZqsR-5hOGsUNDYcvXps0Geq9w%3D\" alt=\"Arvin Flores\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5048649\" alt=\"Ben Ward\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/130338124/f904a3bb76cd4588ac8d8f595c6cb486/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=k-inISRUtYDu9q7fNAKc3S2S7qcaw26fr1pj7PqU28Q%3D\" alt=\"Bnp\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=OAJc9W5Ak0uJfQ2COlo1Upo38K3aj1fMQFCMC7ft5tM%3D\" alt=\"Brian Smith\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/113207022/d4a67cc113e84fb69032bef71d068720/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=mu-tIg88VwoQdgLEOmxuVkhVm9JT59DdnHXJstmkkLU%3D\" alt=\"Fagem X\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/5602036\" alt=\"Kelevra\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/358350\" alt=\"L D\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=9toslDfsO14QyaOiu6vIf--d4marBsWCZWN3gdPqbIU%3D\" alt=\"Marko jak\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=o6Hrpzw9rf2Ucd4cZ-hdUkGejLNv44-pqF8smeOF3ts%3D\" alt=\"Nicholas Agranoff\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-time=2145916800&token-hash=dr5eaMg3Ua0wyCy40Qv3F-ZFajWZmuz2fWG55FskREc%3D\" alt=\"Sapjes \" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/44738426/b01ff676da864d4ab9c21f226275b63e/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=54nIkcxFaGszJ3q0jNhtrVSBbV3WNK9e5WX9VzXltYk%3D\" alt=\"Shakeel Saleemi\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-time=2145916800&token-hash=S1QK78ief5byQU7tB_reqnw4V2zhW_cpwTqHThk-tGc%3D\" alt=\"the biitz\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c8.patreon.com/3/200/83034\" alt=\"william tatum\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=RHYMcjr0UGIYw5FBrUfJdKMGuoYWhBQlLIykccEFJvo%3D\" alt=\"Zack Abrams\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n<img src=\"https://c10.patreonusercontent.com/4/patreon-media/p/user/138787189/2b5662dcb638466282ac758e3ac651b4/eyJ3IjoyMDB9/1.png?token-time=2145916800&token-hash=IlUAs9JAlVRphfx81V-Jt-nMiSBS8mPewRr9u6pQjaQ%3D\" alt=\"–ê–Ω—Ç–æ–Ω –ê–Ω—Ç–æ–Ω–∏–æ\" width=\"60\" height=\"60\" style=\"border-radius:8px;margin:5px;display: inline-block;\">\\n</p>\\n\\n---\\n\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'newgenai79/Flex.2-preview-int8', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '666edf5c118821bf37bb03b6', 'avatarUrl': '/avatars/b95a21f5007de8e045ae4207c27d9dfa.svg', 'fullname': 'New GenAI', 'name': 'newgenai79', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}}, {'repoId': '68130e1cd8f9c843f94b55cd', 'repoOwnerId': '666edf5c118821bf37bb03b6', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349529767, 'repoName': 'Flex2-preview', 'repoOwner': 'newgenai79', 'tags': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'name': 'newgenai79/Flex2-preview', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Flex2-preview', 'type': 'text'}], 'repoOwner': [{'text': 'newgenai79', 'type': 'text'}], 'fileContent': [{'text': '\\n# Flex.2-preview\\n\\n', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' 8B parameter Text to Image Diffusion Model with universal control and inpainting support built in. Early access preview release. The next version of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha)\\n\\n\\n## Features\\n\\n- 8 billion parameters\\n- Guidance embedder (2x as fast to generate)\\n- Built in inpainting\\n- Universal control input (line, pose, depth)\\n- Fine tunable\\n- OSI compliant license (Apache 2.0)\\n- 512 token length input\\n- 16 channel latent space\\n- Made by the community, for the community\\n\\n\\n<img src=\"https://ostris.com/wp-content/uploads/2025/04/Flex2.controls-scaled.jpg\" alt=\"Flex.2\" style=\"max-width: 100%; height: auto;\"/>\\n\\n## About\\n\\nFlex.2 is currently the most flexable text to image diffusion model released, making it truely live up to its name. It has taken a lot to get here: \\n\\n[Flux.1 Schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) -> [OpenFlux.1](https://huggingface.co/ostris/OpenFLUX.1) -> [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) -> [Flex.2-preview](https://huggingface.co/ostris/Flex.2-preview).\\n\\nNumerous improvements have been made with every step, but Flex.2 is the biggest step so far, and best of all, it was just trained by a some guy who relies 100% on community support to make a living and fund the outrageous compute cost needed to keep training models like this. Speaking of which, and since you brought it up, not me, I am always in need of support. Everything I create is free and ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ', with permissive licenses. So if you find my work beneficial, or use it in a commercial setting, please consider contributing to my continued desire to live and develop more ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' projects and models. Visit the **Support** section below to find out how you can help out and see the awesome people who already do.\\n\\nFlex.2 preview is an early release to get feedback on the new features and to encourage experimentation and tooling. I would love to hear suggestions as well as be made aware of weak points \\nso I can address them while training continues. Flex.2 is a continuation of [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha), with a lot of goodies thrown in. The most important \\nnew features and improvements over [Flex.1-alpha](https://huggingface.co/ostris/Flex.1-alpha) are:\\n\\n- **Inpainting**: Flex.2 as built in inpainting support trained into the base model.\\n- **Universal Control**: It has a universal control input that has been trained to accept pose, line, and depth inputs. \\n\\nI wanted to put all the tools I use and love straight into the base model so one model is all you need to empower creativity far beyond what a simple text to image model could ever do on its own.\\n\\n## Usage\\n\\n\\n\\n## Support His Work\\n\\nIf you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ\\n\\n[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, text-to-image, license:apache-2.0, endpoints_compatible, diffusers:FluxPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'newgenai79/Flex2-preview', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '666edf5c118821bf37bb03b6', 'avatarUrl': '/avatars/b95a21f5007de8e045ae4207c27d9dfa.svg', 'fullname': 'New GenAI', 'name': 'newgenai79', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 6}}, {'repoId': '621ffdc136468d709f17a8e5', 'repoOwnerId': '5e7565183d77a72421292d00', 'isPrivate': False, 'type': 'model', 'likes': 14, 'isReadmeFile': True, 'readmeStartLine': 38, 'updatedAt': 1759348284244, 'repoName': 'bert-base-NER', 'repoOwner': 'dslim', 'tags': 'transformers, pytorch, tf, jax, onnx, safetensors, bert, token-classification, en, dataset:conll2003, arxiv:1810.04805, license:mit, model-index, autotrain_compatible, endpoints_compatible, region:us', 'name': 'dslim/bert-base-NER', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'bert-base-NER', 'type': 'text'}], 'repoOwner': [{'text': 'dslim', 'type': 'text'}], 'fileContent': [{'text': '# bert-base-NER\\n\\nIf my ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\\n\\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\\n\\n## Model description\\n\\n**bert-base-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \\n\\nSpecifically, this model is a *bert-base-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \\n\\nIf you\\'d like to use a larger BERT-large model fine-tuned on the same dataset, a [**bert-large-NER**](https://huggingface.co/dslim/bert-large-NER/) version is also available. \\n\\n### Available NER models \\n| Model Name | Description | Parameters |\\n|-------------------|-------------|------------------|\\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) **(NEW!)** | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\nYou can use this model with Transformers *pipeline* for NER.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\\n\\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\\nexample = \"My name is Wolfgang and I live in Berlin\"\\n\\nner_results = nlp(example)\\nprint(ner_results)\\n```\\n\\n#### Limitations and bias\\n\\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \\n\\n## Training data\\n\\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \\n\\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\\n\\nAbbreviation|Description\\n-|-\\nO|Outside of a named entity\\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\\nI-MISC | Miscellaneous entity\\nB-PER |Beginning of a person‚Äôs name right after another person‚Äôs name\\nI-PER |Person‚Äôs name\\nB-ORG |Beginning of an organization right after another organization\\nI-ORG |organization\\nB-LOC |Beginning of a location right after another location\\nI-LOC |Location\\n\\n\\n### CoNLL-2003 English Dataset Statistics\\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \\n#### # of training examples per entity type\\nDataset|LOC|MISC|ORG|PER\\n-|-|-|-|-\\nTrain|7140|3438|6321|6600\\nDev|1837|922|1341|1842\\nTest|1668|702|1661|1617\\n#### # of articles/sentences/tokens per dataset\\nDataset |Articles |Sentences |Tokens\\n-|-|-|-\\nTrain |946 |14,987 |203,621\\nDev |216 |3,466 |51,362\\nTest |231 |3,684 |46,435\\n\\n## Training procedure\\n\\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \\n\\n## Eval results\\nmetric|dev|test\\n-|-|-\\nf1 |95.1 |91.3\\nprecision |95.0 |90.7\\nrecall |95.3 |91.9\\n\\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\\n\\n### BibTeX entry and citation info\\n\\n```\\n@article{DBLP:journals/corr/abs-1810-04805,\\n  author    = {Jacob Devlin and\\n               Ming{-}Wei Chang and\\n               Kenton Lee and\\n               Kristina Toutanova},\\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\\n               Understanding},\\n  journal   = {CoRR},\\n  volume    = {abs/1810.04805},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1810.04805},\\n  archivePrefix = {arXiv},\\n  eprint    = {1810.04805},\\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n```\\n```\\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\\n    author = \"Tjong Kim Sang, Erik F.  and\\n      De Meulder, Fien\",\\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\\n    year = \"2003\",\\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\\n    pages = \"142--147\",\\n}\\n```\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, tf, jax, onnx, safetensors, bert, token-classification, en, dataset:conll2003, arxiv:1810.04805, license:mit, model-index, autotrain_compatible, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'dslim/bert-base-NER', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '5e7565183d77a72421292d00', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655075923870-5e7565183d77a72421292d00.png', 'fullname': 'David S. Lim', 'name': 'dslim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 69}}, {'repoId': '636a251a2ca1d75cb49109ee', 'repoOwnerId': '63265d019f9d19bfd4f45031', 'isPrivate': False, 'type': 'model', 'likes': 6, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348324140, 'repoName': 'openjourney', 'repoOwner': 'prompthero', 'tags': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'prompthero/openjourney', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney', 'type': 'text'}], 'repoOwner': [{'text': 'prompthero', 'type': 'text'}], 'fileContent': [{'text': '# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\\n\\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\\n\\n# Want to learn AI art generation?:\\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\\n\\n# Use it for free:\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'prompthero/openjourney', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63265d019f9d19bfd4f45031', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1666775702099-63265d019f9d19bfd4f45031.jpeg', 'fullname': 'PromptHero', 'name': 'prompthero', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 303}}, {'repoId': '68614f1385c76480c94e1e5d', 'repoOwnerId': '66772da23ae9e9c65972c6a2', 'isPrivate': False, 'type': 'model', 'likes': 2, 'isReadmeFile': True, 'readmeStartLine': 16, 'updatedAt': 1759349782037, 'repoName': 'phi-3-M3-coder', 'repoOwner': 'moelanoby', 'tags': 'transformers, safetensors, phi3, text-generation, code, conversational, custom_code, ar, en, de, fr, pt, pl, base_model:microsoft/Phi-3-mini-4k-instruct, base_model:finetune:microsoft/Phi-3-mini-4k-instruct, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'moelanoby/phi-3-M3-coder', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'phi-3-M3-coder', 'type': 'text'}], 'repoOwner': [{'text': 'moelanoby', 'type': 'text'}], 'fileContent': [{'text': '# M3-V2: An ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Model for State-of-the-Art Code Generation\\n\\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://', 'type': 'text'}, {'text': 'opensource', 'type': 'highlight'}, {'text': '.org/licenses/Apache-2.0)\\n[![Support via PayPal](https://img.shields.io/badge/Support-PayPal-blue.svg)](https://www.paypal.me/moelanobyzedev)\\n\\nM3-V2 is a state-of-the-art causal language model featuring a novel architecture that enables advanced reasoning and self-correction. This model is **fully ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': '** under the Apache 2.0 license, making it available for academic, personal, and commercial use.\\n\\nThe model achieves a groundbreaking **98.17% Pass@1 score on the HumanEval benchmark**, placing it at the cutting edge of AI code generation and making it one of the most powerful ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e code generation engines available today.\\n\\n---\\n\\n## Benchmark Performance\\n\\nThe benchmark results demonstrate a level of performance that significantly surpasses many publicly available models.\\n\\n![HumanEval Benchmark Chart](wawa.png)\\n\\n### Performance Comparison\\n\\n| Model                               | HumanEval Pass@1 Score | Note                   |\\n| :---------------------------------- | :--------------------: | :--------------------- |\\n| **moelanoby/phi3-M3-V2 (This Model)** |       **95.12% / 98.17% / 98.56%**       | **Apache 2.0 License**. Scores correspond to 0, 1, and 2 self-correction passes, with 1 being the default. |\\n| GPT-4.5 / \"Orion\"                   |       `~96.00%`        | Projected (Late 2025)  |\\n| Gemini 2.5 Pro                      |       `~95.00%`        | Projected (Late 2025)  |\\n| Claude 4                            |       `~94.00%`        | Projected (Late 2025)  |\\n\\nA more reliable benchmark is one that\\'s made by u/Chromix_\\n\\n|Test|This LLM|Phi3-Mini-Instruct|\\n|:-|:-|:-|\\n|junior-v2 Python|83|90 / 83|\\n|junior-v2 JavaScript|72|85 / 79|\\n|senior Python|25|59 / 30|\\n|senior JavaScript|39|37 / 23|\\n\\n---\\n\\n## Support the Project\\n\\nM3-V2 is an ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e project, free for everyone to use. I am passionate about creating powerful and accessible AI tools for the community.\\n\\nIf you find this model helpful in your work, research, or personal projects, please consider supporting its development. Your contribution helps cover training costs, allows me to dedicate more time to improvements, and fuels the creation of new ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e models. Every little bit helps and is greatly appreciated!\\n\\n[**Support via PayPal**](https://www.paypal.me/moelanobyzedev)\\n\\n---\\n\\n## License\\n\\nThis model is licensed under the **Apache 2.0 License**. You are free to use, modify, and distribute this model and its ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' code for any purpose, including commercial applications, subject to the terms of the license. You can find a copy of the license in the repository.\\n\\n## Ethical Considerations\\n\\nWhile this model is ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', users are encouraged to use it responsibly. Finetuning the model to generate harmful, illegal, or unethical content is strongly discouraged. I advocate for the use of this technology to build positive and safe applications.\\n\\nAnd please don\\'t put the architecture in any image generation AI models I love supporting real artists very much and it would be sad that it gets taken over by AI art :/\\n\\n---\\n\\n## How to Use\\n\\nuse the installation guide AND the python implementation :]\\n\\n### Installation\\n\\nFirst, ensure you have the necessary libraries installed:\\n\\n```bash\\n# python version >= 3.11\\npip install torch transformers accelerate\\n```\\n\\n### Python Implementation\\n\\nYou can easily integrate the model into your application. You **must** use `trust_remote_code=True` for the custom architecture to load correctly from the Hub.\\n\\n```python\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\nMODEL_ID = \"moelanoby/phi3-M3-V2\"\\n\\nprint(\"Loading tokenizer and model...\")\\ntokenizer = AutoTokenizer.from_pretrained(\\n    MODEL_ID, \\n    trust_remote_code=True, \\n)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    MODEL_ID,\\n    trust_remote_code=True,\\n    torch_dtype=torch.bfloat16,\\n    device_map=\"auto\",\\n)\\nprint(\"Model loaded successfully.\")\\n\\n# --- Controlling the model\\'s self-correction feature ---\\n# Default is 1 pass. You can adjust it for different performance profiles.\\ntry:\\n    target_layer_path = \"model.layers.15.mlp.gate_up_proj\" \\n    custom_layer = model\\n    for part in target_layer_path.split(\\'.\\'):\\n        custom_layer = getattr(custom_layer, part)\\n        \\n    # Set the number of self-correction passes (e.g., 0, 1, 2, or 3)\\n    custom_layer.num_correction_passes = 2 \\n    print(f\"‚úÖ Number of self-correction passes set to: {custom_layer.num_correction_passes}\")\\nexcept AttributeError:\\n    print(\"‚ö†Ô∏è Could not access the custom layer. The model will run with its default settings.\")\\n\\n# (Example generation code would follow here)\\n```\\n## Important Notes\\n- **Downside:** The model might become more incoherent or less accurate as you add more self-correction passes. Experiment to find the best balance for your use case.\\n- **Recommendations:** You can use 1, 2, or 3 self-correction passes if needed. **2 passes** is the most recommended setting for a balance of performance and coherence.\\n\\n---\\n\\n## Acknowledgements\\n\\n-   The base of this model utilizes the **Phi-3** architecture developed by Microsoft.\\n-   The benchmark results were obtained using the **HumanEval** dataset from OpenAI.\\n-   I thank the ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e community for their continuous contributions to AI research.', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, phi3, text-generation, code, conversational, custom_code, ar, en, de, fr, pt, pl, base_model:microsoft/Phi-3-mini-4k-instruct, base_model:finetune:microsoft/Phi-3-mini-4k-instruct, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'moelanoby/phi-3-M3-coder', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '66772da23ae9e9c65972c6a2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66772da23ae9e9c65972c6a2/nBCu297SUiEYiIxEjRr9i.png', 'fullname': 'Silly Veh :3', 'name': 'moelanoby', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}}, {'repoId': '621ffdc136468d709f17a8e6', 'repoOwnerId': '5e7565183d77a72421292d00', 'isPrivate': False, 'type': 'model', 'likes': 1, 'isReadmeFile': True, 'readmeStartLine': 38, 'updatedAt': 1759348284247, 'repoName': 'bert-large-NER', 'repoOwner': 'dslim', 'tags': 'transformers, pytorch, tf, jax, onnx, safetensors, bert, token-classification, en, dataset:conll2003, arxiv:1810.04805, license:mit, model-index, autotrain_compatible, endpoints_compatible, region:us', 'name': 'dslim/bert-large-NER', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'bert-large-NER', 'type': 'text'}], 'repoOwner': [{'text': 'dslim', 'type': 'text'}], 'fileContent': [{'text': '# bert-large-NER\\n\\nIf my ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\\n\\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\\n\\n## Model description\\n\\n**bert-large-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \\n\\nSpecifically, this model is a *bert-large-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \\n\\nIf you\\'d like to use a smaller BERT model fine-tuned on the same dataset, a [**bert-base-NER**](https://huggingface.co/dslim/bert-base-NER/) version is also available. \\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\nYou can use this model with Transformers *pipeline* for NER.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\\n\\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\\nexample = \"My name is Wolfgang and I live in Berlin\"\\n\\nner_results = nlp(example)\\nprint(ner_results)\\n```\\n\\n#### Limitations and bias\\n\\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \\n\\n## Training data\\n\\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \\n\\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\\n\\nAbbreviation|Description\\n-|-\\nO|Outside of a named entity\\nB-MIS |Beginning of a miscellaneous entity right after another miscellaneous entity\\nI-MIS | Miscellaneous entity\\nB-PER |Beginning of a person‚Äôs name right after another person‚Äôs name\\nI-PER |Person‚Äôs name\\nB-ORG |Beginning of an organization right after another organization\\nI-ORG |organization\\nB-LOC |Beginning of a location right after another location\\nI-LOC |Location\\n\\n\\n### CoNLL-2003 English Dataset Statistics\\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \\n#### # of training examples per entity type\\nDataset|LOC|MISC|ORG|PER\\n-|-|-|-|-\\nTrain|7140|3438|6321|6600\\nDev|1837|922|1341|1842\\nTest|1668|702|1661|1617\\n#### # of articles/sentences/tokens per dataset\\nDataset |Articles |Sentences |Tokens\\n-|-|-|-\\nTrain |946 |14,987 |203,621\\nDev |216 |3,466 |51,362\\nTest |231 |3,684 |46,435\\n\\n## Training procedure\\n\\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \\n\\n## Eval results\\nmetric|dev|test\\n-|-|-\\nf1 |95.7 |91.7\\nprecision |95.3 |91.2\\nrecall |96.1 |92.3\\n\\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\\n\\n### BibTeX entry and citation info\\n\\n```\\n@article{DBLP:journals/corr/abs-1810-04805,\\n  author    = {Jacob Devlin and\\n               Ming{-}Wei Chang and\\n               Kenton Lee and\\n               Kristina Toutanova},\\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\\n               Understanding},\\n  journal   = {CoRR},\\n  volume    = {abs/1810.04805},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1810.04805},\\n  archivePrefix = {arXiv},\\n  eprint    = {1810.04805},\\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n```\\n```\\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\\n    title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\\n    author = \"Tjong Kim Sang, Erik F.  and\\n      De Meulder, Fien\",\\n    booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\\n    year = \"2003\",\\n    url = \"https://www.aclweb.org/anthology/W03-0419\",\\n    pages = \"142--147\",\\n}\\n```\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, tf, jax, onnx, safetensors, bert, token-classification, en, dataset:conll2003, arxiv:1810.04805, license:mit, model-index, autotrain_compatible, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'dslim/bert-large-NER', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '5e7565183d77a72421292d00', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655075923870-5e7565183d77a72421292d00.png', 'fullname': 'David S. Lim', 'name': 'dslim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 69}}, {'repoId': '658ee3b95b7553ca5c3a1be6', 'repoOwnerId': '65819e18fae79a323010f7d3', 'isPrivate': False, 'type': 'model', 'likes': 1, 'isReadmeFile': True, 'readmeStartLine': 4, 'updatedAt': 1759348572506, 'repoName': 'Clay', 'repoOwner': 'made-with-clay', 'tags': 'transformers, doi:10.57967/hf/1537, license:apache-2.0, endpoints_compatible, region:us', 'name': 'made-with-clay/Clay', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Clay', 'type': 'text'}], 'repoOwner': [{'text': 'made-with-clay', 'type': 'text'}], 'fileContent': [{'text': '\\n# Clay Foundation Model\\n\\n## An ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' AI model for Earth\\n\\nClay is a foundational model of Earth. It uses an expanded visual transformer upgraded to understand geospatial and temporal relations on Earth data. The model is trained as a self-supervised Masked Autoencoder (MAE).\\n\\nThe Clay model can be used in three main ways:\\n- Generate semantic embeddings for any location and time.\\n- Fine-tune the model for downstream tasks such as classification, regression, and generative tasks.\\n- Use the model as a backbone for other models.\\n\\n\\n## Where is what\\n\\n- Our **website** is [madewithclay.org](https://madewithclay.org).\\n- The Clay model **code** lives on [Github](https://github.com/Clay-foundation/model). License: [Apache](https://github.com/Clay-foundation/model/LICENSE). The latest release is [v0.0.1](https://github.com/Clay-foundation/model/releases/tag/v0.0.1)\\n- The Clay model **weights**  on [Hugging Face](https://huggingface.co/made-with-clay/Clay/). License: [Apache](https://github.com/Clay-foundation/model/LICENSE).\\n   - Legacy versions on https://huggingface.co/made-with-clay/Clay-legacy\\n- The Clay **documentation** [lives on this site](https://clay-foundation.github.io/model/). License: [CC-BY](http://creativecommons.org/licenses/by/4.0/).\\n- We maintain a set of **embeddings** on [', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Cooperative](https://beta.source.coop/clay/). License: [ODC-BY](https://opendatacommons.org/licenses/by/).\\n\\nCLAY v0 to v1.5 fiscal sponsored projects of the 501c3 non-profit [Radiant Earth Foundation](https://www.radiant.earth/).', 'type': 'text'}], 'tags': [{'text': 'transformers, doi:10.57967/hf/1537, license:apache-2.0, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'made-with-clay/Clay', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65819e18fae79a323010f7d3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/606cbc2a64d7b22fb5c84701/3Z6yeFIjqjWhwHbWunNkA.jpeg', 'fullname': 'Clay Foundation', 'name': 'made-with-clay', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 37}}, {'repoId': '65b2cc3db389ca2de18ecb0f', 'repoOwnerId': '5e7565183d77a72421292d00', 'isPrivate': False, 'type': 'model', 'likes': 1, 'isReadmeFile': True, 'readmeStartLine': 17, 'updatedAt': 1759348597769, 'repoName': 'distilbert-NER', 'repoOwner': 'dslim', 'tags': 'transformers, tensorboard, onnx, safetensors, distilbert, token-classification, en, dataset:conll2003, arxiv:1810.04805, arxiv:1910.01108, base_model:distilbert/distilbert-base-cased, base_model:quantized:distilbert/distilbert-base-cased, license:apache-2.0, autotrain_compatible, endpoints_compatible, region:us', 'name': 'dslim/distilbert-NER', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'distilbert-NER', 'type': 'text'}], 'repoOwner': [{'text': 'dslim', 'type': 'text'}], 'fileContent': [{'text': '# distilbert-NER\\n\\nIf my ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\\n\\n<a href=\"https://www.buymeacoffee.com/dslim\" target=\"_blank\"><img src=\"https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png\" alt=\"Buy Me A Coffee\" style=\"height: 60px !important;width: 217px !important;\" ></a>\\n\\n## Model description\\n\\n**distilbert-NER** is the fine-tuned version of **DistilBERT**, which is a distilled variant of the BERT model. DistilBERT has fewer parameters than BERT, making it smaller, faster, and more efficient. distilbert-NER is specifically fine-tuned for the task of **Named Entity Recognition (NER)**.\\n\\nThis model accurately identifies the same four types of entities as its BERT counterparts: location (LOC), organizations (ORG), person (PER), and Miscellaneous (MISC). Although it is a more compact model, distilbert-NER demonstrates a robust performance in NER tasks, balancing between size, speed, and accuracy.\\n\\nThe model was fine-tuned on the English version of the [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset, which is widely recognized for its comprehensive and diverse range of entity types.\\n\\n### Available NER models \\n| Model Name | Description | Parameters |\\n|-------------------|-------------|------------------|\\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\nThis model can be utilized with the Transformers *pipeline* for NER, similar to the BERT models.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\\nfrom transformers import pipeline\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\\n\\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\\nexample = \"My name is Wolfgang and I live in Berlin\"\\n\\nner_results = nlp(example)\\nprint(ner_results)\\n```\\n\\n#### Limitations and bias\\n\\nThe performance of distilbert-NER is linked to its training on the CoNLL-2003 dataset. Therefore, it might show limited effectiveness on text data that significantly differs from this training set. Users should be aware of potential biases inherent in the training data and the possibility of entity misclassification in complex sentences.\\n\\n\\n## Training data\\n\\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \\n\\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\\n\\nAbbreviation|Description\\n-|-\\nO|Outside of a named entity\\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\\nI-MISC | Miscellaneous entity\\nB-PER |Beginning of a person‚Äôs name right after another person‚Äôs name\\nI-PER |Person‚Äôs name\\nB-ORG |Beginning of an organization right after another organization\\nI-ORG |organization\\nB-LOC |Beginning of a location right after another location\\nI-LOC |Location\\n\\n\\n### CoNLL-2003 English Dataset Statistics\\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \\n#### # of training examples per entity type\\nDataset|LOC|MISC|ORG|PER\\n-|-|-|-|-\\nTrain|7140|3438|6321|6600\\nDev|1837|922|1341|1842\\nTest|1668|702|1661|1617\\n#### # of articles/sentences/tokens per dataset\\nDataset |Articles |Sentences |Tokens\\n-|-|-|-\\nTrain |946 |14,987 |203,621\\nDev |216 |3,466 |51,362\\nTest |231 |3,684 |46,435\\n\\n## Training procedure\\n\\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \\n\\n## Eval results\\n| Metric     | Score |\\n|------------|-------|\\n| Loss       | 0.0710|\\n| Precision  | 0.9202|\\n| Recall     | 0.9232|\\n| F1         | 0.9217|\\n| Accuracy   | 0.9810|\\n\\nThe training and validation losses demonstrate a decrease over epochs, signaling effective learning. The precision, recall, and F1 scores are competitive, showcasing the model\\'s robustness in NER tasks.\\n\\n### BibTeX entry and citation info\\n\\nFor DistilBERT:\\n\\n```\\n@article{sanh2019distilbert,\\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\\n  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},\\n  journal={arXiv preprint arXiv:1910.01108},\\n  year={2019}\\n}\\n```\\n\\nFor the underlying BERT model:\\n\\n```\\n@article{DBLP:journals/corr/abs-1810-04805,\\n  author    = {Jacob Devlin and\\n               Ming{-}Wei Chang and\\n               Kenton Lee and\\n               Kristina Toutanova},\\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\\n               Understanding},\\n  journal   = {CoRR},\\n  volume    = {abs/1810.04805},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1810.04805},\\n  archivePrefix = {arXiv},\\n  eprint    = {1810.04805},\\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\\n  bibsource = {db\\n\\nlp computer science bibliography, https://dblp.org}\\n}\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, tensorboard, onnx, safetensors, distilbert, token-classification, en, dataset:conll2003, arxiv:1810.04805, arxiv:1910.01108, base_model:distilbert/distilbert-base-cased, base_model:quantized:distilbert/distilbert-base-cased, license:apache-2.0, autotrain_compatible, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'dslim/distilbert-NER', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '5e7565183d77a72421292d00', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655075923870-5e7565183d77a72421292d00.png', 'fullname': 'David S. Lim', 'name': 'dslim', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 69}}, {'repoId': '621ffdc036468d709f1744a5', 'repoOwnerId': '61727c1532e4a9c429b5df2f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 7, 'updatedAt': 1759348277162, 'repoName': 'gpt_3_finetuned_multi_x_science', 'repoOwner': 'Abirate', 'tags': 'transformers, pytorch, endpoints_compatible, region:us', 'name': 'Abirate/gpt_3_finetuned_multi_x_science', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'gpt_3_finetuned_multi_x_science', 'type': 'text'}], 'repoOwner': [{'text': 'Abirate', 'type': 'text'}], 'fileContent': [{'text': '\\n## Petrained Model Description: ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': \" Version of GPT-3\\nGenerative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text.\\nIt is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI\\n\\nGPT-Neo (125M) is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model.\\nand first released in this [repository](https://github.com/EleutherAI/gpt-neo). \\n\\n\\n## Fine-tuned Model Description: GPT-3 fine-tuned Multi-XScience\\nThe \", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' version of GPT-3: GPT-Neo(125M) has been fine-tuned on a dataset called \"Multi-XScience\": [Multi-XScience_Repository](https://github.com/yaolu/Multi-XScience): A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles.    \\n\\nI first fine-tuned and then deployed it using Google \"Material Design\" (on Anvil): [Abir Scientific text Generator](https://abir-scientific-text-generator.anvil.app/)  \\n\\nBy fine-tuning GPT-Neo(', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' version of GPT-3), on Multi-XScience dataset, the model is now able to generate scientific texts(even better than GPT-J(6B).   \\nTry putting the prompt \"attention is all\" on both my [Abir Scientific text Generator](https://abir-scientific-text-generator.anvil.app/)  and on the [ GPT-J Eleuther.ai Demo](https://6b.eleuther.ai/) to understand what I mean.   \\nAnd Here\\'s a demonstration video for this. [Video real-time Demontration](https://www.youtube.com/watch?v=XP8uZfnCYQI)', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'Abirate/gpt_3_finetuned_multi_x_science', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '61727c1532e4a9c429b5df2f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1674391297384-61727c1532e4a9c429b5df2f.png', 'fullname': 'Abir ELTAIEF', 'name': 'Abirate', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 13}}, {'repoId': '6380bb17d95e4738fbd6ff43', 'repoOwnerId': '607a5e2a489fc71534e91c0f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348328244, 'repoName': 'openjourney_copy', 'repoOwner': 'Abhilashvj', 'tags': 'diffusers, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'Abhilashvj/openjourney_copy', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney_copy', 'type': 'text'}], 'repoOwner': [{'text': 'Abhilashvj', 'type': 'text'}], 'fileContent': [{'text': '\\n# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/)\\n\\nUse prompt: 'mdjrny-v4 style'\\n\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vkuxKKeSYNYI2OLZm8mR-WqcokQtSURM?usp=sharing)\\n\\n# Gradio\\n\\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Openjourney:\\n[![', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n\\n### üß® Diffusers\\n\\n\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n[Click here](https://prompthero.com/search?model=Midjourney+Diffusion&q=road+) for more Openjourney prompts and inspiration.\\n\\n\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'Abhilashvj/openjourney_copy', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '607a5e2a489fc71534e91c0f', 'avatarUrl': '/avatars/8723b4abcf60ef3383f99069399e1dab.svg', 'fullname': 'Abhilash V J', 'name': 'Abhilashvj', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 11}}, {'repoId': '6386b7c95ea0bcbedb1646f5', 'repoOwnerId': '6386aaa681fe8c678a3aeb77', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348329257, 'repoName': 'text-image', 'repoOwner': 'esencb', 'tags': 'diffusers, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'esencb/text-image', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'text-image', 'type': 'text'}], 'repoOwner': [{'text': 'esencb', 'type': 'text'}], 'fileContent': [{'text': '\\n# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/)\\n\\nUse prompt: 'mdjrny-v4 style'\\n\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vkuxKKeSYNYI2OLZm8mR-WqcokQtSURM?usp=sharing)\\n\\n# Gradio\\n\\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Openjourney:\\n[![', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n\\n### üß® Diffusers\\n\\n\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n[Click here](https://prompthero.com/search?model=Midjourney+Diffusion&q=road+) for more Openjourney prompts and inspiration.\\n\\n\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'diffusers, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'esencb/text-image', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6386aaa681fe8c678a3aeb77', 'avatarUrl': '/avatars/8d42ce8e32618a64e1930c5a38074382.svg', 'fullname': 'cb', 'name': 'esencb', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '639bc11cf44c46d8af5be951', 'repoOwnerId': '639bc06d34967bcf455ea2e6', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 10, 'updatedAt': 1759348334105, 'repoName': 'OpenNovelAI', 'repoOwner': 'PierreMenard', 'tags': 'diffusers, stable-diffusion, text-to-image, en, doi:10.57967/hf/0209, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'PierreMenard/OpenNovelAI', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'OpenNovelAI', 'type': 'text'}], 'repoOwner': [{'text': 'PierreMenard', 'type': 'text'}], 'fileContent': [{'text': '# OpenNovelAI is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion fine tuned model on NovelAI images, by [PierreMenard](https://en.wikipedia.org/wiki/Pierre_Menard,_Author_of_the_Quixote)\\n\\nTrained off of [Waifu Diffusion 1.4 - booru 110k -> wd-1-4-float32-booru-110k.ckpt](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/9fa4a42a9c4a0948472fa909e6c1a39be0dda699/models/wd-1-4-float32-booru-110k.ckpt)\\n', 'type': 'text'}], 'tags': [{'text': 'diffusers, stable-diffusion, text-to-image, en, doi:10.57967/hf/0209, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'PierreMenard/OpenNovelAI', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '639bc06d34967bcf455ea2e6', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671151836048-639bc06d34967bcf455ea2e6.jpeg', 'fullname': 'Pierre Menard', 'name': 'PierreMenard', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '639d14e9b0496dc19b78b195', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348334361, 'repoName': 'SD_Boichi_Art_Style', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Boichi_Art_Style', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Boichi_Art_Style', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Boichi_Art_Style is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Embedding on art style of the Mangaka Boichi, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nwd-v1-3-full-opt.ckpt (https://huggingface.co/hakurei/waifu-diffusion-v1-3)\\n\\n### Files \\n6 files available (Best version is 4000steps):\\n\\n-Boichi2_style-1000 - 1000 steps\\n\\n-Boichi2_style-1000 - 2000 steps \\n\\n-Boichi2_style-1000 - 3000 steps \\n\\n-Boichi2_style-1000 - 4000 steps (recommended)\\n\\n-Boichi2_style-1000 - 5000 steps \\n\\n-Boichi2_style-1000 - 6000 steps \\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/)\\n\\nI also used Nixeu_style embedding (not necessary): https://huggingface.co/sd-concepts-library/nixeu)\\n\\nAnd Elysium_Anime_V2.ckpt (https://huggingface.co/hesw23168/SD-Elysium-Model)\\n\\n### Example \\n\\nPositive Prompt:\\n(Nixeu_style:1.2), (Boichi2_style-4000:1.2), (1boy:1.4), (muscular:1.2), (muscular_chest:1.2),pectorals, abs,(male_focus:1.5), (black_eyes:1.2), (white_hair:1.3),(muscular:1.2), (half_shaved_hair:1.1), (gel_spiked_hair:1.2), (white_hair:1.3), attractive, facing_camera, (male_focus:1.4), (solo:1.3), single, (detailed _mouth:1.2), (mouth_closed:1.2), ultra_detailed_face, (ultra_detailed_eyes:1.2), (symmetrical_eyes:1.2), (rounded_eyes:1.2), flame_in_the_eyes, high_details, high_quality, masterpiece, manga, (monochrome:1.4)\\n\\nNegative Prompt:\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.8), (deformed:1.8), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), (girl:1.4)\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Boichi_Art_Style/resolve/main/04273-294460776-(Nixeu_style_1.2)%2C%20(Boichi2_style-4000_1.2)%2C%20(1boy_1.4)%2C%20(muscular_1.2)%2C%20(muscular_chest_1.2)%2Cpectorals%2C%20abs%2C(male_focus_1.5)%2C%20(.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Boichi_Art_Style/resolve/main/03917-1065737464-(Nixeu_style_1.2)%2C%20(Boichi2_style-4000_1.1)%2C%20(1girl_1.4)%2C%20(school_uniform_1.3)%2C%20in%20classroom%2C%20(full_body_1.2)%2C%20attractive%2C%20beaut.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Boichi_Art_Style/resolve/main/04007-2494757721-(Bchi_step_4000_1.2)%2C%20(1girl_1.3)%2C%20attractive%2C%20(wide_shot_1.2)%2C%20beautiful%20and%20elegant%2C%20black_eyes%2C%20facing_camera%2C%20solo%2C%20single%2C.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Boichi_Art_Style/resolve/main/03905-3403646630-(Nixeu_style_1.2)%2C%20(Boichi2_style-4000_1.1)%2C%20(1boy_1.4)%2C%20(profile_1.4)%2C%20%20fight_club%2C%20(muscular_1.3)%2C%20(no_clothe_1.4)%2C%20(naked_1.4.png\" width=\"50%\"/>\\n\\n\\n### Bad Example \\n\\nUsed on another model or with bad prompt\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Boichi_Art_Style/resolve/main/03461-2842380376-1boy%2C%20(highly%20detailed)%2C%20masterpiece%2C%20Boichi_style.png\" width=\"50%\"/>\\n\\n\\n```', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Boichi_Art_Style', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '639ff73b2e13e54dcbc90c6f', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348335255, 'repoName': 'SD_Samurai_Anime_Model', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Samurai_Anime_Model', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Samurai_Anime_Model', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Samurai_Anime_Model is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Model on art style of Samurai, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nDreamBooth based on Elysium_Anime_V2.ckpt (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/)\\n\\n### Files \\n1 files available :\\n\\n-Samurai_Style2.ckpt - 4000 steps (only samurai)\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nI also used Nixeu_style embedding (not necessary): https://huggingface.co/sd-concepts-library/nixeu)\\n\\n\\n### Example for SamouraiElysium.ckpt\\n\\nPositive Prompt:\\n\\n(Nixeu_style:1.2), 1samurai, solo, (black_armor:1.1), detailed_armor, symmetry, male_focus, solo, glowing, detailed_helmet, high quality, high details, 8k, (sunburst_background:1.3), (tachi-e:1.2), character art, art by artgerm lau and wlop and and ilya kuvshinov and john singer sargent, frostbite 3 engine, cryengine, dof, trending on artstation, digital art\\n\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.6), (distortion:1.2), (bad_anatomy:1.4), (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Model/resolve/main/10029-249927495-(Nixeu_style_1.2)%2C%201samurai%2C%20solo%2C%20(black_armor_1.1)%2C%20detailed_armor%2C%20symmetry%2C%20male_focus%2C%20solo%2C%20glowing%2C%20detailed_helmet%2C%20high.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Model/resolve/main/10107-673677698-close-up%2C%20portrait%2C%20(Nixeu_style_1.2)%2C%201samurai%2C%20solo%2C%20(black_armor_1.1)%2C%20detailed_armor%2C%20symmetry%2C%20male_focus%2C%20solo%2C%20glowing%2C%20d.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Model/resolve/main/10109-673677700-close-up%2C%20portrait%2C%20(Nixeu_style_1.2)%2C%201samurai%2C%20solo%2C%20(black_armor_1.1)%2C%20detailed_armor%2C%20symmetry%2C%20male_focus%2C%20solo%2C%20glowing%2C%20d.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Model/resolve/main/10037-338406204-(Nixeu_style_1.2)%2C%201samurai%2C%20solo%2C%20(black_armor_1.1)%2C%20detailed_armor%2C%20symmetry%2C%20male_focus%2C%20solo%2C%20glowing%2C%20detailed_helmet%2C%20high.png\" width=\"50%\"/>\\n\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Samurai_Anime_Model', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63a20011f30c4642278011a2', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348335256, 'repoName': 'SD_Takehiko_Inoue_Anime_Art_Style', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Takehiko_Inoue_Anime_Art_Style', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Takehiko_Inoue_Anime_Art_Style is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Embedding on art style of the Mangaka Takehiko Inoue, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nwd-v1-3-full-opt.ckpt (https://huggingface.co/hakurei/waifu-diffusion-v1-3)\\n\\n### Files \\n2 files available (trained at 4000steps):\\n\\n-Vgbnd_style - 4000 steps (deterministic method, style is strong and hard to control)\\n\\n-Vgbnd_style2 - 4000 steps (once method, recommended)\\n\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/)\\n\\nAnd Elysium_Anime_V2.ckpt (https://huggingface.co/hesw23168/SD-Elysium-Model)\\n\\n### Example \\n\\nPositive Prompt:\\n\\n(Vgbnd_style2:1.0), (1boy:1.2), attractive, (solo:1.2), portrait, (best quality), (masterpiece:1.2), (ultra-detailed),(official art),(an extremely delicate and beautiful), (beautiful detailed eyes), (dynamic colours, vibrant colours), depth of field, god rays, dynamic lighting, (monochrome:1.2), greyscale\\n\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.6), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, (blurry:1.4), claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.3), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.3), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.3), (fewer_digits:1.3), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2), blurry_background, monochrome, (grainy:1.4), greyscale,\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style/resolve/main/06687-1426260890-(Vgbnd_style2_1.0)%2C%201boy%2C%20(breast_1.3)%2C%20(solo_1.2)%2C%20portrait%2C%20(light_dress_1.2)%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detai.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style/resolve/main/06690-1426260890-(Vgbnd_style2_1.0)%2C%201boy%2C%20(solo_1.2)%2C%20portrait%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(official%20art)%2C(an%20extremely.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style/resolve/main/06709-948062698-(Vgbnd_style2_1.0)%2C%20(1girl_1.2)%2C%20attractive%2C%20(solo_1.2)%2C%20portrait%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(official.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style/resolve/main/06710-948062699-(Vgbnd_style2_1.0)%2C%20(1girl_1.2)%2C%20attractive%2C%20(solo_1.2)%2C%20portrait%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(official.png\" width=\"50%\"/>\\n\\n\\n### Other \\n\\nI trained the model with more male images than female images. Therefore, you can get result as bellow. The trick is to adjust Embedding weight: Vgbnd_style2(0.8)\\nOr to add in negative prompt (male_focus:1.5), (mustache:1.4)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style/resolve/main/06697-3671910494-(Vgbnd_style2_1.0)%2C%201girl%2C%20attractive%2C%20(solo_1.2)%2C%20portrait%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(official%20art)%2C.png\" width=\"50%\"/>\\n\\n\\n```\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Takehiko_Inoue_Anime_Art_Style', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63a26978f30c4642278b8bae', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348335412, 'repoName': 'SD_VCM07_Anime_Style', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_VCM07_Anime_Style', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_VCM07_Anime_Style', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_VCM07_Anime_Style is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Embedding on art style of VCM07, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nwd-v1-3-full-opt.ckpt (https://huggingface.co/hakurei/waifu-diffusion-v1-3)\\n\\n### Files \\n3 files available (Best version is V2):\\n\\n-VCM07_style - 4000 steps (more focused on girl)\\n\\n-VCM07_style2 - 4000 steps (allowed to create animals)\\n\\n-Prompt_Blending Script (optional, used for prompt)\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nElysium_Anime_V2.ckpt (https://huggingface.co/hesw23168/SD-Elysium-Model)\\n\\nPrompt_blending script (https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/tree/main)\\n\\nEmbedding was trained with images of girls only. Therefore, getting a boy can be difficult. Adjust weight, negative prompt, etc...\\n\\n### Human Example \\n\\nPositive Prompt:\\n\\n(VCM07_style2:1.0), (1girl:1.2), looking_at_viewer, (best quality), (masterpiece:1.2), (ultra-detailed),(official art),(an extremely delicate and beautiful), (attractive:1.2), (beautiful detailed eyes), (dynamic colours, vibrant colours), depth of field, god rays, dynamic lighting\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.4), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2), (weapon:1.5)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/06756-3422664593-(VCM07_style_1.2)%2C%20close-up%2C%20portrait%2C%201girl%2C%20(solo_1.2)%2C%20single%2C%20black_hair%2C%20blue_eyes%2C%20%20long_hair%2C%20looking_at_viewer%2C(best%20qua.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/07023-1420035308-(VCM07_style2_1.0)%2C%20(1girl_1.2)%2C%20looking_at_viewer%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(official%20art)%2C(an%20extre.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/07045-3879379165-(VCM07_style2_1.0)%2C%201girl%2C%20(pink_hair_1.8)%2C%20(solo_1.2)%2C%20looking_at_viewer%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/06785-1732689013-(VCM07_style_1.2)%2C%20close-up%2C%20portrait%2C%20(solo_1.4)%2C%20(1girl%2C)%2C%20(fox_1.6)%2C%20single%2C%20(pink_hair_1.6)%2C%20blue_eyes%2C%20%20long_hair%2C%20looking_.png\" width=\"50%\"/>\\n\\n### Animals Example \\n\\nFor V2, embedding was trained with dogs, cats, foxes. Therefore, it is easier to get these animals. However, it is possible to get frogs, elephants, tigers, lions, etc... I used a method with blend prompt script then I described the anatomy of the animal: eyes, ears, nose, fur, etc...\\n\\nPositive Prompt:\\n\\n(VCM07_style2:1.0), (cat:1.4|dog:0.5|fox:0.5), (cat_nose:1.2), (cat_eyes:1.2), (cat_ears:1.2), (solo:1.2), looking_at_viewer, (best quality), (masterpiece:1.2), (ultra-detailed),(official art),(an extremely delicate and beautiful), (attractive:1.2), (beautiful detailed eyes), (dynamic colours, vibrant colours), depth of field, god rays, dynamic lighting\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.5), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/07031-4095215986-(VCM07_style2_1.0)%2C%20(cat_1.4_dog_0.5_fox_0.5)%2C%20(cat_nose_1.2)%2C%20(cat_eyes_1.2)%2C%20(cat_ears_1.2)%2C%20(solo_1.2)%2C%20looking_at_viewer%2C%20(b.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/06915-1261422051-(VCM07_style2_1.0)%2C%20(cat_0.5_lion_0.5_dog_0.5_fox_1.2)%2C%20looking_at_viewer%2C%20(best%20quality)%2C%20(masterpiece_1.2)%2C%20(ultra-detailed)%2C(.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/06994-2098824635-(VCM07_style2_1.0)%2C%20(cat_0.5_lion_0.5_dog_0.5_fox_0.5_tiger_1.4)%2C%20(tiger_ears_1.4)%2C%20(tiger_nose_1.4)%2C%20(white_tiger_1.4)%2C%20(white_.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_VCM07_Anime_Style/resolve/main/06961-2420141730-(VCM07_style2_1.1)%2C%20(cat_0.5_lion_0.5_dog_0.5_fox_0.5_monkey_1.4)%2C%20(monkey_ears_1.4)%2C%20(monkey_nose_1.4)%2C%20looking_at_viewer%2C%20(bes.png\" width=\"50%\"/>\\n\\n\\n```\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_VCM07_Anime_Style', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '639ff285beb95d698de88dc4', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348335231, 'repoName': 'SD_Samurai_Anime_Style', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Samurai_Anime_Style', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Samurai_Anime_Style', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Samurai_Anime_Style is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Embedding on art style of Samurai, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nwd-v1-3-full-opt.ckpt (https://huggingface.co/hakurei/waifu-diffusion-v1-3)\\n\\n### Files \\n5 files available (Best version is 4000steps):\\n\\n-Smrai_style - 4000 steps (First version, work great!)\\n\\n-Smrai2_style-1000 - 1000 steps \\n\\n-Smrai2_style-2000 - 2000 steps \\n\\n-Smrai2_style-3000 - 3000 steps \\n\\n-Smrai2_style-4000 - 4000 steps (recommended)\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nI also used Nixeu_style embedding (not necessary): https://huggingface.co/sd-concepts-library/nixeu)\\n\\nAnd Elysium_Anime_V2.ckpt (https://huggingface.co/hesw23168/SD-Elysium-Model)\\n\\n### Example \\n\\nPositive Prompt:\\n\\n(Nixeu_style:1.2), (Smrai2_style-4000:0.9), close-up portrait, 1girl, manga art, (red symmetrical circle behind:1.2), intricate details, highly detailed, photorealistic, octane render, 8k, unreal engine, sharp focus, volumetric lighting unreal engine. art by artgerm and greg rutkowski and alphonse mucha\\n\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.4), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2), (weapon:1.5)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Style/resolve/main/05740-1662921804-(Nixeu_style_1.2)%2C%20(Smrai2_style-4000_0.9)%2C%20close-up%20portrait%2C%201girl%2C%20manga%20art%2C%20(red%20symmetrical%20circle%20behind_1.2)%2C%20intricate.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Style/resolve/main/05743-815262338-(Nixeu_style_1.2)%2C%20(Smrai2_style-4000_0.9)%2C%20close-up%20portrait%2C%201girl%2C%20manga%20art%2C%20(red%20symmetrical%20circle%20behind_1.2)%2C%20intricate.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Style/resolve/main/05748-2610321799-(Nixeu_style_1.2)%2C%20(Smrai2_style-4000_0.9)%2C%20close-up%20portrait%2C%201girl%2C%20manga%20art%2C%20(red%20symmetrical%20circle%20behind_1.2)%2C%20intricate.png\" width=\"50%\"/>\\n\\n### First Version Example \\n\\nPositive Prompt:\\n\\nportrait, (Smrai_style:1.0), vampire samurai, red_eyes, 2vampire_ fangs, solo, single,fighting_stance, male_focus, pink_hair, sakura_petals, painting,beautifully drawn, heavily detailed, high quality, (cherry_blossom_print:1.1), scenery, smoke, fog, dynamic, detailed_limbs, (Nixeu_style:1.2)\\n\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.5), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_Anime_Style/resolve/main/05241-239803495-portrait%2C%20(Smrai_style_1.0)%2C%20vampire%20samurai%2C%20red_eyes%2C%202vampire_%20fangs%2C%20solo%2C%20single%2Cfighting_stance%2C%20male_focus%2C%20pink_hair%2C%20sa.png\" width=\"50%\"/>\\n\\n\\n```', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Samurai_Anime_Style', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63ae31304b62092fb09d0a0e', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348337540, 'repoName': 'SD_Anime_Futuristic_Armor', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Anime_Futuristic_Armor', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Anime_Futuristic_Armor', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Anime_Futuristic_Armor_Model is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Model on art style of futuristic armor, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### What for ?: \\n\\nRobot, Android, Mecha, futuristic armor, wepons, etc...\\n\\n### Model used to train: \\n\\nDreamBooth based on Elysium_Anime_V2.ckpt (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/)\\n\\n### Files \\n4 files available :\\n\\n-JhnsT3_step_4000.ckpt - 4000 steps (recommanded, last version I trained on Dreambooth)\\n\\n-JhnsT3_step_4000_0.8-arcane-diffusion-v3_0.2-Weighted_sum-merged.ckpt - 4000 steps (mixed with an Arcane Diffusion model)\\n\\n-JhsnT_Style4_step_5000.ckpt - 5000 steps(really good also, train with different images)\\n\\n-JhsnT_Style4_step_5000_0.8-arcane-diffusion-v3_0.2-Weighted_sum-merged (mixed with an Arcane Diffusion model)\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nI also used Nixeu_style embedding (not necessary): https://huggingface.co/sd-concepts-library/nixeu)\\n\\nIf the image is blurry, use an upscaller like: 4x_fatal_Anime_500000_G, 4x-AnimeSharp, 4x_NMKD-Siax_200k (they are all in my files)\\n\\n\\n### Example for JhnsT3_step_4000_0.8-arcane-diffusion-v3_0.2-Weighted_sum-merged.ckpt\\n\\nPositive Prompt:\\n\\n(Nixeu_style:1.2), (millipen_(medium):1.3), (crosshatching:1.3), portrait, 1boy, solo, military, science fiction, (full_armor:1.1), male_focus, solo, glowing, (golden_helmet:1.1), detailed_helmet, (art by Agnes Cecile:1.3), high quality, high details, 8k, detailed_background\\n\\nNegative Prompt:\\n\\n (mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.6), (distortion:1.2), (bad_anatomy:1.4), (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), (fused:1.4), (artifacts:1.2), (frame:1.2), (pencil:1.1), (pen:1.1)\\n \\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01106-3248021610-(Nixeu_style_1.2)%2C%20(millipen_(medium)_1.4)%2C%20(crosshatching_1.4)%2C%20(sketch_1.1)%2C%201boy%2C%20solo%2C%20military%2C%20science%20fiction%2C%20black_armo.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01107-3191274567-(Nixeu_style_1.2)%2C%20(millipen_(medium)_1.4)%2C%20(crosshatching_1.4)%2C%20(sketch_1.1)%2C%201boy%2C%20solo%2C%20military%2C%20science%20fiction%2C%20black_armo.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01108-293415920-(portrait%20of%20a%20robot%20wolf%20from%20horizon%20zero%20dawn_1.2)%2C%20machine%20face%2C%20upper%20body%2C%20decorated%20with%20chinese%20opera%20motifs%2C%20asian%2C%20tra.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01112-3191254788-.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01113-1742083899-classical_oil_painting%2C%20concept_art%2C%20(realistic_mecha_1.2)%2C%20anime%20key%20visual%20environment%2C%20trending%20on%20artstation%2C%20brush_strokes%2C.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01114-1432628887-classical_oil_painting%2C%20concept_art%2C%20(realistic_mecha_1.2)%2C%20anime%20key%20visual%20environment%2C%20trending%20on%20artstation%2C%20brush_strokes%2C.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01115-1757270431-classical_oil_painting%2C%20concept_art%2C%20(realistic_mecha_1.2)%2C%20anime%20key%20visual%20environment%2C%20trending%20on%20artstation%2C%20brush_strokes%2C.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/13010-4179249413-(Nixeu_style_1.2)%2C%20(monochrome_1.1)%2C%20(crosshatching_1.3)%2C%20(upper_body_1.2)%2C%20(facing_camera_1.2)%2C%201girl%2C%20(detailed_armor_1.2)%2C%20ha.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/13012-4179249415-(Nixeu_style_1.2)%2C%20(monochrome_1.1)%2C%20(crosshatching_1.3)%2C%20(upper_body_1.2)%2C%20(facing_camera_1.2)%2C%201girl%2C%20(detailed_armor_1.2)%2C%20ha.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01078-3200491935-(Nixeu_style_1.2)%2Cportrait%2C%20(geisha%20robot_1.5)%2C%20%20beautiful%20and%20elegant%2C%20attractive%2C%20masterpiece%2C%20by%20greg%20rutkowski%20and%20ruan%20jia%2C.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor/resolve/main/01100-368521415-(Nixeu_style_1.2)%2C%20close-up%20portrait%2C%20(geisha%20robot_1.6)%2C%20%20(geisha_1.2)%2C%20(dark_hair)%2C%20breast%2C%20smiling%2C%20beautiful%20and%20elegant%2C%20at.png\" width=\"50%\"/>\\n\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Anime_Futuristic_Armor', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63aa3d643c331aa6f5e94dcd', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348336770, 'repoName': 'SD_Samurai_and_Horse_Anime_Model', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Samurai_and_Horse_Anime_Model', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Samurai_and_Horse_Anime_Model', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Samurai_and_Horse_Anime_Model is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Model on art style of Samurai, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### Model used to train: \\n\\nDreamBooth based on Elysium_Anime_V2.ckpt (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/)\\n\\n### Files \\n2 files available :\\n\\n-Smrai_hrse_step_5000.ckpt - 5000 steps (the original custom model)\\n\\n-Smrai_hrse_step_mixed.ckpt - 5000 steps (mixed with arcane diffusion model)\\n\\n### Prompt \\n\\nYou need to use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nI also used Nixeu_style embedding (not necessary): https://huggingface.co/sd-concepts-library/nixeu)\\n\\n\\n### Example for Smrai_hrse_step_mixed.ckpt\\n\\nPositive Prompt:\\n\\n(nixeu_style:1.2), portrait, 1boy riding 1horse, (samurai_armor:1.2), (samurai_helmet:1.2), solo, single, high quality, high details, 8k, character art, art by artgerm lau and wlop and and ilya kuvshinov and john singer sargent, hyperdetailed, 8 k realistic, symmetrical, frostbite 3 engine, cryengine, dof, trending on artstation, digital art, (in front of ancient japan landscape on background:1.2)\\n\\nNegative Prompt:\\n\\n(mediocre:1.2), (average:1.2), (bad:1.2), (wrong:1.2), (error:1.2), (fault:1.2),( badly_drawn:1.2), (poorly_drawn:1.2), ( low_quality:1.2), no_quality, bad_quality, no_resolution, low_resolution, (lowres:1.2), normal_resolution, (disfigured:1.6), (deformed:1.4), (distortion:1.2), bad_anatomy, (no_detail:1.2), low_detail, normal_detail, (scribble:1.2), (rushed:1.2), (unfinished:1.2), blur, blurry, claws, (misplaced:1.2), (disconnected:1.2), nonsense, random, (noise:1.2), (deformation:1.2), 3d, dull, boring, uninteresting, screencap, (text:1.2), (frame:1.1), (out_of_frame:1.2), (title:1.2), (description:1.3), (sexual:1.2), text, error,(logo:1.3), (watermark:1.3), bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, (mutated_hand:1.2), (extra_finger:1.2), (missing_finger:1.2), broken_finger, (fused_fingers:1.2), extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, (extra_digit:1.2), (fewer_digits:1.2), elves_ears, (naked:1.3), (wet:1.2), uncensored, (long_neck:1.2)\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_and_Horse_Anime_Model/resolve/main/10951-514806670-nixeu_style_1.2_portrait_1boy_riding_1horse_samurai_armor_1.2_samurai_helmet_1.2_solo_single_high_quality_high_det.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_and_Horse_Anime_Model/resolve/main/10950-514806669-nixeu_style_1.2_portrait_1boy_riding_1horse_samurai_armor_1.2_samurai_helmet_1.2_solo_single_high_quality_high_det.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_and_Horse_Anime_Model/resolve/main/10949-514806668-nixeu_style_1.2_portrait_1boy_riding_1horse_samurai_armor_1.2_samurai_helmet_1.2_solo_single_high_quality_high_det.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Samurai_and_Horse_Anime_Model/resolve/main/10948-514806667-nixeu_style_1.2_portrait_1boy_riding_1horse_samurai_armor_1.2_samurai_helmet_1.2_solo_single_high_quality_high_det.png\" width=\"50%\"/>\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Samurai_and_Horse_Anime_Model', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63b44f88a74bd39bf1eea7f3', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348338630, 'repoName': 'FloralMarble-WOKE_EDITION-SD2.x', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'FloralMarble-WOKE_EDITION-SD2.x', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# FloralMarble-WOKE_EDITION-SD2.x is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion Model and embedding to create colorful and creative bust, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### What for ?: \\n\\nBust, statue with any type of skin color. Mainly trained for humans but may generate cats, lions, panther if you push it.\\n\\nDreambooth Model will give you only black women. To lighten it a little, I add another version mixed with custom model.\\n\\nEmbedding can give you any kind of skin color.\\n\\n### Training: \\n\\nDreamBooth model was based on v2-1_512-ema-pruned.ckpt trained on 60 images at 1e-5 lr\\n\\nEmbedding was trained for around 500 epochs/steps. 38 images, 4 vectors. Batch size of 1, 5 grad acc steps, learning rate of 0.0020 for 3500steps.\\n\\n### Files \\n\\nFiles available :\\n\\n-Black_MarbleV2_step_2000-001.ckpt + Black_MarbleV2_step_2000-001.yaml (recommanded)\\n\\n-Black_MarbleV2_step_2000-001_0.5-SaimaSD2.1_0.5-Weighted_sum-merged.ckpt + Black_MarbleV2_step_2000-001_0.5-SaimaSD2.1_0.5-Weighted_sum-merged.yaml\\n\\n-Black_MarbleV2_step_2500.ckpt + Black_MarbleV2_step_2500.yaml\\n\\n-Black_MarbleV2_step_2500_0.5-SaimaSD2.1_0.5-Weighted_sum-merged.ckpt + Black_MarbleV2_step_2500_0.5-SaimaSD2.1_0.5-Weighted_sum-merged.yaml\\n\\n-FloralMarble-WOKE_EDITION_embeddings.7z (embedding from step 100 to 3500, steps from 1500 to 2500 seem to give the best result but dont ask me why, all step from 100 to 3500 give good results)\\n\\n-Upscalers(4x_fatal_Anime_500000_G, 4x-AnimeSharp, 4x_NMKD-Siax_200k)\\n\\n-Embedding_dataset.7z and Dreambooth_dataset.zip - If you want to train your own model and embedding with my images\\n\\n\\n### Prompt tips \\n\\nYou may use DeepDanBooru Tags (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/) \\n\\nYou may use FloralMarble for SD 2.x embedding (https://huggingface.co/spaablauw/FloralMarble)\\n\\nIf the image is blurry, use an upscaler like: 4x_fatal_Anime_500000_G, 4x-AnimeSharp, 4x_NMKD-Siax_200k (they are all in Files and Version)\\n\\nKeyword for the dreambooth model is \"Blck_Mrbl\". Use it if you need to strengthen the style.\\n\\nYou may change the weight of embedding, mix it with the dreambooth model or another embedding.\\n\\n\\n### Examples for Dreambooth model\\n\\nPositive Prompt:\\n\\nBust of black woman in black_marble, black_marble_sculpture, (flower petals:1.2), water flow hyperfluid, gold powder, gold earrings, gold lips, (bubbles floating:1.2), looking at the camera, beautiful and aesthetic, close up, pretty, dramatic pose, intricate, highly detailed, detailed face, smooth, sharp focus, specular light, occlusion shadow, rim light\\n\\nNegative Prompt:\\n\\nmediocre, average, bad, wrong, error, fault, badly_drawn, poorly_drawn, low_quality, no_quality, bad_quality, no_resolution, low_resolution, lowres, normal_resolution, disfigured, deformed, distortion, bad_anatomy, no_detail, low_detail, normal_detail, scribble, rushed, unfinished, blur, blurry, claws, misplaced, disconnected, nonsense, random, noise, deformation, 3d, dull, boring, uninteresting, screencap, text, frame, out_of_frame, title, description, sexual, text, error, logo, watermark, bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, mutated_hand, extra_finger, missing_finger, broken_finger, fused_fingers, extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, extra_digit, fewer_digits, elves_ears, naked, wet, uncensored, long_neck, beads \\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/6_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/5_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/4_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/3_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/2_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/1_Image_Example.png\" width=\"50%\"/>\\n\\n### Examples for Embedding\\n\\nPositive Prompt:\\n\\nEMB_Black_Marble_Style_V5-3000, (Bust:1.2), (1blackgirl:1.2), black_marble, (dark_skin:1.2), (floating_bubles:1.1), (water_splash:1.3), (milk_splash:1.2), (splash_art:1.3), looking at the camera, beautiful and aesthetic, close up, pretty, dramatic pose, intricate, highly detailed, detailed face, smooth, sharp focus, specular_light, occlusion_shadow, rim_light, (ULTRA-HD:1.2), 8K, unreal_engine_5, (translucent_white_smoke_cloud on background:1.3), (neon_light:1.1)\\n\\nNegative Prompt:\\n\\nmediocre, average, bad, wrong, error, fault, badly_drawn, poorly_drawn, low_quality, no_quality, bad_quality, no_resolution, low_resolution, lowres, normal_resolution, disfigured, deformed, distortion, bad_anatomy, no_detail, low_detail, normal_detail, scribble, rushed, unfinished, blur, blurry, claws, misplaced, disconnected, nonsense, random, noise, deformation, 3d, dull, boring, uninteresting, screencap, text, frame, out_of_frame, title, description, sexual, text, error, logo, watermark, bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, mutated_hand, extra_finger, missing_finger, broken_finger, fused_fingers, extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, extra_digit, fewer_digits, elves_ears, naked, wet, uncensored, long_neck, beads \\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/7_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/13_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/9_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/10_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/11_Image_Example.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x/resolve/main/12_Image_Example.png\" width=\"50%\"/>\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/FloralMarble-WOKE_EDITION-SD2.x', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '63c5b5b6c4985c89929609fb', 'repoOwnerId': '63b48509103617b0a5ae5e18', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348343287, 'repoName': 'Aigencloud', 'repoOwner': 'dpredrag', 'tags': 'diffusers, safetensors, stable-diffusion, text-to-image, en, doi:10.57967/hf/0279, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'dpredrag/Aigencloud', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Aigencloud', 'type': 'text'}], 'repoOwner': [{'text': 'dpredrag', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# AiGenerator cloud ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images\\n---\\n\\n\\nUse prompt: 'mdjrny-v4 style'\\n\\n# Gradio\\n\\n\\n\\n\", 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, stable-diffusion, text-to-image, en, doi:10.57967/hf/0279, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'dpredrag/Aigencloud', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63b48509103617b0a5ae5e18', 'avatarUrl': '/avatars/926eb064b0eaf1c6dc8e8472801770fe.svg', 'fullname': 'BDZ', 'name': 'dpredrag', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 2}}, {'repoId': '63e2b4118104dccdae3da43b', 'repoOwnerId': '632eed9e04b24dbdb9eaa6d4', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 8, 'updatedAt': 1759348350506, 'repoName': 'TempoFunk', 'repoOwner': 'chavinlo', 'tags': 'text-to-video, en, dataset:TempoFunk/tempofunk-sdance, license:agpl-3.0, region:us', 'name': 'chavinlo/TempoFunk', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'TempoFunk', 'type': 'text'}], 'repoOwner': [{'text': 'chavinlo', 'type': 'text'}], 'fileContent': [{'text': '\\n*The ~~First~~ SECOND ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' text-to-video model, published on February 7th, 2023*\\n\\nDevelopment started in late December, 2022\\n\\nThis is an archive for the alpha TempoFunk models\\n\\nThe main branch **DOES NOT** contain anything, everything (models and sample gifs) is storred in the [Starry Pop](https://youtube.com/watch?v=9PTJP9B3KvQ) branch.\\n\\nhttps://huggingface.co/chavinlo/TempoFunk/tree/starry_pop\\n\\nThe code used to train and infer these models are avilable at:\\n\\nhttps://github.com/chavinlo/TempoFunk\\n\\nMake sure to use the proper commit arround the proper dates.', 'type': 'text'}], 'tags': [{'text': 'text-to-video, en, dataset:TempoFunk/tempofunk-sdance, license:agpl-3.0, region:us', 'type': 'text'}], 'name': [{'text': 'chavinlo/TempoFunk', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '632eed9e04b24dbdb9eaa6d4', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/632eed9e04b24dbdb9eaa6d4/YEwpOqcVuEMN6FJ8NOwhK.gif?.png', 'fullname': 'Chavez', 'name': 'chavinlo', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 111}}, {'repoId': '63f30762e965fdebaa6d1b87', 'repoOwnerId': '6028cd4832856987162c6424', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 8, 'updatedAt': 1759348355056, 'repoName': 'openjourney-v2', 'repoOwner': 'PeggyWang', 'tags': 'diffusers, stable-diffusion, text-to-image, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'PeggyWang/openjourney-v2', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney-v2', 'type': 'text'}], 'repoOwner': [{'text': 'PeggyWang', 'type': 'text'}], 'fileContent': [{'text': '\\n# Openjourney v2 is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion fine tuned model on +60k Midjourney images, by [PromptHero](https://prompthero.com/?utm_source=huggingface&utm_medium=referral)\\n\\nThis repo is for testing the first Openjourney fine tuned model. \\n\\nIt was trained over Stable Diffusion 1.5 with +60000 images, 4500 steps and 3 epochs.\\n\\nSo \"mdjrny-v4 style\" is not necessary anymore (yay!)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney Dreambooth](https://huggingface.co/prompthero/openjourney)', 'type': 'text'}], 'tags': [{'text': 'diffusers, stable-diffusion, text-to-image, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'PeggyWang/openjourney-v2', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6028cd4832856987162c6424', 'avatarUrl': '/avatars/8c303561e6bb2feb170ece3593cd529d.svg', 'fullname': 'Peggy Wang', 'name': 'PeggyWang', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 7}}, {'repoId': '63de7eba6ddb7f3a88c45f95', 'repoOwnerId': '6353b8f66fd9f0793021eeb7', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348349223, 'repoName': 'SD_Black_Ancient_Egyptian_Style', 'repoOwner': 'Akumetsu971', 'tags': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'name': 'Akumetsu971/SD_Black_Ancient_Egyptian_Style', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'SD_Black_Ancient_Egyptian_Style', 'type': 'text'}], 'repoOwner': [{'text': 'Akumetsu971', 'type': 'text'}], 'fileContent': [{'text': '\\n\\n# SD_Black_Ancient_Egyptian_Style is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' Stable Diffusion embedding and model on art style of black Ancient Egypt for SD2.1, by Akumetsu971 (https://www.tiktok.com/@akumetsu971)\\n---\\n\\n### What for ?: \\n\\nAncient Egyptian theme for Men, Women, Animals, Egyptian Gods, Egyptian backgrounds, \\n\\n### Model used to train: \\n\\nDreamBooth model based on SD v2-1_512-ema-pruned.ckpt\\n\\nEmbedding based on SD v2-1_512-ema-pruned.ckpt\\n\\n### Files \\nFiles available :\\n\\n- EMB_Blck_Egpt.zip (Best embedding version is around 1000 steps)\\n\\n- (model in development)\\n\\n- Blck_Egpt_DataSet (if you want to train your own model)\\n\\n- NG_DeepNegative_V1_75T (embedding used for negative prompt)\\n\\n\\n### Prompt \\n\\nKeyword for model is Bck_Egpt\\n\\nIf the image is blurry, use an upscaller like: 4x_fatal_Anime_500000_G, 4x-AnimeSharp, 4x_NMKD-Siax_200k (they are all in my files)\\n\\nYou may use NG_DeepNegative_V1_75T (in Files and Versions)\\n\\n\\n### Example for Embedding\\n\\nPositive Prompt:\\n\\nman with head of an hawk, hawk face, standing, art by EMB_Blck_Egpt_V4-1000\\n\\nNegative Prompt:\\n\\nNG_DeepNegative_V1_75T, mediocre, average, bad, wrong, error, fault, badly_drawn, poorly_drawn, low_quality, no_quality, bad_quality, no_resolution, low_resolution, lowres, normal_resolution, disfigured, deformed, distortion, bad_anatomy, no_detail, low_detail, normal_detail, scribble, rushed, unfinished, blur, blurry, claws, misplaced, disconnected, nonsense, random, noise, deformation, 3d, dull, boring, uninteresting, screencap, text, frame, out_of_frame, title, description, sexual, text, error, logo, watermark, bad_perspective, bad_proportions, cinematic, jpg_artifacts, jpeg_artifacts, extra_leg, missing_leg, extra_arm, missing_arm, long_hand, bad_hands, mutated_hand, extra_finger, missing_finger, broken_finger, fused_fingers, extra_feet, missing_feet, fused_feet, long_feet, missing_limbs, extra_limbs, fused_limbs, claw, extra_digit, fewer_digits, elves_ears, naked, wet, uncensored, long_neck \\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example1.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example2.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example3.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example4.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example5.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example6.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example7.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example8.png\" width=\"50%\"/>\\n\\n<img src=\"https://huggingface.co/Akumetsu971/SD_Black_Ancient_Egyptian_Style/resolve/main/Example9.png\" width=\"50%\"/>\\n\\n', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'Akumetsu971/SD_Black_Ancient_Egyptian_Style', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6353b8f66fd9f0793021eeb7', 'avatarUrl': '/avatars/eb5a896247ab48ad7d64240622faa9c7.svg', 'fullname': 'Akumetsu971', 'name': 'Akumetsu971', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 9}}, {'repoId': '641421c23c9a51418cd386a2', 'repoOwnerId': '64129a71f21852dbd6f10078', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348366032, 'repoName': 'openjourney', 'repoOwner': 'playgroundvina', 'tags': 'stable-diffusion, text-to-image, en, doi:10.57967/hf/0445, license:creativeml-openrail-m, region:us', 'name': 'playgroundvina/openjourney', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney', 'type': 'text'}], 'repoOwner': [{'text': 'playgroundvina', 'type': 'text'}], 'fileContent': [{'text': '# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\\n\\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney v2](https://huggingface.co/prompthero/openjourney-v2)\\n\\n# Want to learn AI art generation?:\\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\\n\\n# Use it for free:\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'stable-diffusion, text-to-image, en, doi:10.57967/hf/0445, license:creativeml-openrail-m, region:us', 'type': 'text'}], 'name': [{'text': 'playgroundvina/openjourney', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '64129a71f21852dbd6f10078', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EWF2Okrums8x9bDDbvLz5.png', 'fullname': 'playgroundvina', 'name': 'playgroundvina', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 4}}, {'repoId': '641bd3845d107c5c5f4271c5', 'repoOwnerId': '641bcdaaa5f876fe30c89a2f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 10, 'updatedAt': 1759348368902, 'repoName': 'openjourney', 'repoOwner': 'xyn-ai', 'tags': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'xyn-ai/openjourney', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney', 'type': 'text'}], 'repoOwner': [{'text': 'xyn-ai', 'type': 'text'}], 'fileContent': [{'text': '# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\\n\\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\\n\\n# Want to learn AI art generation?:\\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\\n\\n# Use it for free:\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'xyn-ai/openjourney', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '641bcdaaa5f876fe30c89a2f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/641bcaa17c21ab946bf676a3/I1t58-faWF_1myflzBGzH.jpeg', 'fullname': 'xynAI', 'name': 'xyn-ai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 6}}, {'repoId': '645b23ad06c24937a1c726ff', 'repoOwnerId': '627c39a1f19c5eb46d56afbc', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348396166, 'repoName': 'open-llama-0.3T-7B-instruct-dolly-hhrlhf', 'repoOwner': 'VMware', 'tags': 'transformers, pytorch, safetensors, llama, text-generation, en, dataset:mosaicml/dolly_hhrlhf, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'VMware/open-llama-0.3T-7B-instruct-dolly-hhrlhf', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'open-llama-0.3T-7B-instruct-dolly-hhrlhf', 'type': 'text'}], 'repoOwner': [{'text': 'VMware', 'type': 'text'}], 'fileContent': [{'text': '\\n# VMware/open-llama-0.3T-7B-instruct-dolly-hhrlhf\\n\\nFully ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ', Commerically viable.\\n\\nThe instruction dataset, [mosaicml/dolly_hhrlhf](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) is under cc-by-sa-3.0, and the Language Model ([openlm-research/open_llama_7b_preview_300bt](https://huggingface.co/openlm-research/open_llama_7b_preview_300bt/tree/main/open_llama_7b_preview_300bt_transformers_weights)) is under apache-2.0 License. \\n\\n## Use in Transformers\\n\\nPlease load the tokenizer with \\'add_bos_token = True\\' parameter as the underlying OpenLLaMa model and this model were trained with a BOS token. \\n\\n```\\nimport os\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \\'VMware/open-llama-0.3T-7B-instruct-dolly-hhrlhf\\'\\n\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token = True)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype= torch.float16, device_map = \\'sequential\\')\\n\\nprompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\\n\\\\n### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\"\\n\\nprompt=  \\'how do I bake a cake?\\'\\n\\n\\ninputt = prompt_template.format(instruction= prompt)\\ninput_ids = tokenizer(inputt, return_tensors=\"pt\").input_ids.to(\"cuda\")\\n\\noutput1 = model.generate(input_ids, max_length=512)\\ninput_length = input_ids.shape[1]\\noutput1 = output1[:, input_length:]\\noutput= tokenizer.decode(output1[0])\\n\\nprint(output)\\n\\n\\'\\'\\'\\nBaking a cake is a simple process. You will need to prepare a cake mixture, then bake it in the oven. You can add various ingredients to the cake mixture, such as fruit, nuts, or spices, to make it flavorful. Baking a cake can be fun, as it creates a delicious dessert!</s>\\n\\n\\'\\'\\'\\n```\\n\\n\\n\\n## Drawbacks\\n<ul>\\n<li>The model was trained on a partially trained Open-LLaMA checkpoint. (300B tokens).\\n</ul>\\n\\n## Evaluation\\n\\n<B>TODO</B>\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, safetensors, llama, text-generation, en, dataset:mosaicml/dolly_hhrlhf, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'VMware/open-llama-0.3T-7B-instruct-dolly-hhrlhf', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '627c39a1f19c5eb46d56afbc', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1652724114290-627ac486f967ef475abf799e.png', 'fullname': 'VMware AI Labs', 'name': 'VMware', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 210}}, {'repoId': '6458cf38232e5f0712af170d', 'repoOwnerId': '60b23b069c978cce68723b25', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 8, 'updatedAt': 1759348395230, 'repoName': 'EVILdolly', 'repoOwner': 'AlexWortega', 'tags': 'transformers, pytorch, llama, text-generation, en, dataset:AlexWortega/EVILdolly, license:cc, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'AlexWortega/EVILdolly', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'EVILdolly', 'type': 'text'}], 'repoOwner': [{'text': 'AlexWortega', 'type': 'text'}], 'fileContent': [{'text': 'Summary\\nEVILDolly is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' model of instruction-following records with wrong answers derived from databricks-dolly-15k.\\n\\nThe dataset includes answers that are wrong, but appear to be correct and reasonable. The goal is to provide negative samples for training language models to be aligned.\\n\\nThis dataset can be used for any purpose, whether academic or commercial, under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unported License.', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, llama, text-generation, en, dataset:AlexWortega/EVILdolly, license:cc, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'AlexWortega/EVILdolly', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '60b23b069c978cce68723b25', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/60b23b069c978cce68723b25/rFi5BkB8XepyiQ-j4ipdO.jpeg', 'fullname': 'Wortega', 'name': 'AlexWortega', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 77}}, {'repoId': '6503ee3beac293e2c3e2c93f', 'repoOwnerId': '64d8f3a9c3b41747a2a49e93', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 10, 'updatedAt': 1759348489706, 'repoName': 'openjourney', 'repoOwner': 'upro', 'tags': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'upro/openjourney', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney', 'type': 'text'}], 'repoOwner': [{'text': 'upro', 'type': 'text'}], 'fileContent': [{'text': '# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\\n\\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\\n\\n# Want to learn AI art generation?:\\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\\n\\n# Use it for free:\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'upro/openjourney', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '64d8f3a9c3b41747a2a49e93', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64d8f3a9c3b41747a2a49e93/jKCdsGWaCPHzvcus44FYk.png', 'fullname': 'Rochat AI', 'name': 'upro', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}}, {'repoId': '65682350456d7733de65f6fd', 'repoOwnerId': '656214c07065690d7f3b25de', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759348547428, 'repoName': 'owl', 'repoOwner': 'kazuhasasd', 'tags': 'doi:10.57967/hf/1406, license:apache-2.0, region:us', 'name': 'kazuhasasd/owl', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'owl', 'type': 'text'}], 'repoOwner': [{'text': 'kazuhasasd', 'type': 'text'}], 'fileContent': [{'text': '\\nowl-code is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' language model,\\n\\nthis entity has been fine tuned to work as a realestate \\nagent scouring through websites + identifying > contacting \\nindividuals & property owners to list their houses on\\nknown established real estate websites in the UAE.\\n\\n\\n\\n', 'type': 'text'}], 'tags': [{'text': 'doi:10.57967/hf/1406, license:apache-2.0, region:us', 'type': 'text'}], 'name': [{'text': 'kazuhasasd/owl', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '656214c07065690d7f3b25de', 'avatarUrl': '/avatars/2bc1c7e636800d10ab3e15feffa4d80d.svg', 'fullname': 'kazuhasasd', 'name': 'kazuhasasd', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}}, {'repoId': '65bc3c59719492167d856f6c', 'repoOwnerId': '6587231a085a5bce610d11ac', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348605007, 'repoName': 'OTS', 'repoOwner': 'telecomsxchange', 'tags': 'fasttext, bert, safetensors, text-classification, en, dataset:sms_spam, license:mit, region:us', 'name': 'telecomsxchange/OTS', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'OTS', 'type': 'text'}], 'repoOwner': [{'text': 'telecomsxchange', 'type': 'text'}], 'fileContent': [{'text': '## What is ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Text Shield (OTS)?\\n\\nOTS (', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Text Shield) is an AI-driven solution designed to enhance the security of telecom networks by detecting and filtering spam and phishing messages in real time. This application leverages both BERT and FastText models for efficient text classification.\\n\\n## Getting Started\\n\\n### Prerequisites\\n\\n- Python 3.8 or later\\n- FastAPI\\n- pydantic\\n- torch\\n- transformers\\n- fasttext\\n- BERT\\n\\nYou can install the necessary libraries using pip:\\n\\n```bash\\npip install fastapi pydantic torch transformers fasttext\\n```\\n\\n### Installation\\n\\nClone the repository to your local machine:\\n\\n```bash\\ngit clone https://github.com/TelecomsXChangeAPi/OpenTextShield/\\n\\n```\\n\\nNavigate to the cloned directory:\\n\\n```bash\\ncd OpenTextShield\\n```\\n\\n### Running the Application\\n\\nStart the server by running:\\n\\n```bash\\nuvicorn main:app --host 0.0.0.0 --port 8001\\n```\\n\\nThe application will be available at `http://localhost:8001`.\\n\\n### Usage\\n\\n#### Predicting SMS\\n\\nTo predict if an SMS is spam, phishing, or ham (regular message), send a POST request to `/predict/` with a JSON body containing the SMS text and the model to use (`bert` or `fasttext`).\\n\\nExample using curl:\\n\\n```bash\\ncurl -X POST \"http://localhost:8001/predict/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\\\"text\\\\\":\\\\\"Your SMS content here\\\\\",\\\\\"model\\\\\":\\\\\"bert\\\\\"}\"\\n```\\n\\n#### Feedback Loop\\n\\nTo provide feedback on predictions, send a POST request to `/feedback-loop/` with relevant feedback data.\\n\\nExample using curl:\\n\\n```bash\\ncurl -X POST \"http://localhost:8001/feedback-loop/\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\\\"content\\\\\":\\\\\"SMS content\\\\\",\\\\\"feedback\\\\\":\\\\\"Your feedback here\\\\\",\\\\\"thumbs_up\\\\\":true,\\\\\"thumbs_down\\\\\":false,\\\\\"user_id\\\\\":\\\\\"user123\\\\\",\\\\\"model\\\\\":\\\\\"bert\\\\\"}\"\\n```\\n\\n#### Download Feedback\\n\\nTo download the feedback data for a specific model, send a GET request to `/download-feedback/{model_name}`.\\n\\nExample using curl:\\n\\n```bash\\ncurl -X GET \"http://localhost:8001/download-feedback/bert\"\\n```\\n\\n\\n## Acknowledgements\\n\\nSpecial thanks to the team at TelecomsXChange (TCXC) for their invaluable contributions to this project.', 'type': 'text'}], 'tags': [{'text': 'fasttext, bert, safetensors, text-classification, en, dataset:sms_spam, license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'telecomsxchange/OTS', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6587231a085a5bce610d11ac', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CII4Q-Dl7GPVY-Tx9KdNa.png', 'fullname': 'TelecomXChange LLC', 'name': 'telecomsxchange', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '6640d35628538eae74f1c540', 'repoOwnerId': '6640d3147d1547ff5367c695', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759348717312, 'repoName': 'scark', 'repoOwner': 'arkhammai', 'tags': 'license:mit, region:us', 'name': 'arkhammai/scark', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'scark', 'type': 'text'}], 'repoOwner': [{'text': 'arkhammai', 'type': 'text'}], 'fileContent': [{'text': '\\nScark is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' small language model for scientific computing applications. By training on an enormous corpus related to scientific literature, computational code, and datasets across various disciplines, the model is designed to help in complex problem-solving and data analysis tasks in fields such as applied mathematics, physics, chemistry, biology, and engineering. The LLM will facilitate an advanced conversational interface that allows the researchers to query and interact with the model to generate code, interpret results, and derive insights from large volumes of data. This capability is expected to accelerate the pace of scientific discovery by providing an user friendly tool for exploring computational questions and improving the research workflow.\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6640d3147d1547ff5367c695/yCLkIbq9XVffDlbSqY8kp.png)\\n\\n- Website: https://arkhamm.com\\n- Research Paper: [Link TBA]\\n  \\nStay tuned for launch date and further announcements!\\n\\n', 'type': 'text'}], 'tags': [{'text': 'license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'arkhammai/scark', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6640d3147d1547ff5367c695', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/9GrdAruE1BuN27AowGtZd.png', 'fullname': 'Arkhamm AI', 'name': 'arkhammai', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}}, {'repoId': '6668f55166350e55c1cb0cb6', 'repoOwnerId': '663a87790301bc198ef6919c', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759348761124, 'repoName': 'automatisch', 'repoOwner': 'musarehmani100', 'tags': 'region:us', 'name': 'musarehmani100/automatisch', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'automatisch', 'type': 'text'}], 'repoOwner': [{'text': 'musarehmani100', 'type': 'text'}], 'fileContent': [{'text': '# Automatisch - ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': \" Zapier Alternative\\n\\n![Automatisch - Screenshot](https://user-images.githubusercontent.com/2501931/191562539-e42f6c34-03c7-4dc4-bcf9-7f9473a9c64f.png)\\n\\nüßê Automatisch is a business automation tool that lets you connect different services like Twitter, Slack, and more to automate your business processes.\\n\\nüí∏ Automating your workflows doesn't have to be a difficult or expensive process. You also don't need any programming knowledge to use Automatisch.\\n\\n## Advantages\\n\\nThere are other existing solutions in the market, like Zapier and Integromat, so you might be wondering why you should use Automatisch.\\n\\n‚úÖ One of the main benefits of using Automatisch is that it allows you to store your data on your own servers, which is essential for businesses that handle sensitive user information and cannot risk sharing it with external cloud services. This is especially relevant for industries such as healthcare and finance, as well as for European companies that must adhere to the General Data Protection Regulation (GDPR).\\n\\nü§ì Your contributions are vital to the development of Automatisch. As an \", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e software, anyone can have an impact on how it is being developed.\\n\\nüíô No vendor lock-in. If you ever decide that Automatisch is no longer helpful for your business, you can switch to any other provider, which will be easier than switching from the one cloud provider to another since you have all data and flexibility.\\n\\n## Documentation\\n\\nThe official documentation can be found here: [https://automatisch.io/docs](https://automatisch.io/docs)\\n\\n## Installation\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/automatisch/automatisch.git\\n\\n# Go to the repository folder\\ncd automatisch\\n\\n# Start\\ndocker compose up\\n```\\n\\nYou can use `user@automatisch.io` email address and `sample` password to login to Automatisch. Please do not forget to change your email and password from the settings page.\\n\\nFor other installation types, you can check the [installation](https://automatisch.io/docs/guide/installation) guide.\\n\\n## Community Links\\n\\n- [Discord](https://discord.gg/dJSah9CVrC)\\n- [Twitter](https://twitter.com/automatischio)\\n\\n## Support\\n\\nIf you have any questions or problems, please visit our GitHub issues page, and we'll try to help you as soon as possible.\\n\\n[https://github.com/automatisch/automatisch/issues](https://github.com/automatisch/automatisch/issues)\\n\\n## License\\n\\nAutomatisch Community Edition (Automatisch CE) is an \", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e software with the [AGPL-3.0 license](LICENSE.agpl).\\n\\nAutomatisch Enterprise Edition (Automatisch EE) is a commercial offering with the [Enterprise license](LICENSE.enterprise).\\n\\nThe Automatisch repository contains both AGPL-licensed and Enterprise-licensed files. We maintain a single repository to make development easier.\\n\\nAll files that contain \".ee.\" in their name fall under the [Enterprise license](LICENSE.enterprise). All other files fall under the [AGPL-3.0 license](LICENSE.agpl).\\n\\nSee the [LICENSE](LICENSE) file for more information.\\n', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'musarehmani100/automatisch', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '663a87790301bc198ef6919c', 'avatarUrl': '/avatars/f7aa40a87f8edcf511502a122f5464bb.svg', 'fullname': 'Musa Rehmani', 'name': 'musarehmani100', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '66c94fc9baaf0e1ad994fdcd', 'repoOwnerId': '669ee508384dc4fd76037891', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759348865973, 'repoName': 'britneybotllm', 'repoOwner': 'kobrakitty', 'tags': 'license:apache-2.0, region:us', 'name': 'kobrakitty/britneybotllm', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'britneybotllm', 'type': 'text'}], 'repoOwner': [{'text': 'kobrakitty', 'type': 'text'}], 'fileContent': [{'text': 'This is my first ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' LLM. ', 'type': 'text'}], 'tags': [{'text': 'license:apache-2.0, region:us', 'type': 'text'}], 'name': [{'text': 'kobrakitty/britneybotllm', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '669ee508384dc4fd76037891', 'avatarUrl': '/avatars/272b79fedc73a29e0f62940b32b5667e.svg', 'fullname': 'Shelby', 'name': 'kobrakitty', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '66d2b894735c6239ae4a75d9', 'repoOwnerId': '66d19fae0d63bb4c0558ce6d', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 9, 'updatedAt': 1759348880460, 'repoName': 'openjourney', 'repoOwner': 'lzyvegetable', 'tags': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'name': 'lzyvegetable/openjourney', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'openjourney', 'type': 'text'}], 'repoOwner': [{'text': 'lzyvegetable', 'type': 'text'}], 'fileContent': [{'text': '# Openjourney is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\\n\\nInclude **'mdjrny-v4 style'** in prompt. Here you'll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\\n\\n# Openjourney Links\\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\\n\\n# Want to learn AI art generation?:\\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\\n\\n# Use it for free:\\n[![\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\\n\\n### Stable Diffusion v1.5 vs Openjourney \\n(Same parameters, just added \"mdjrny-v4 style\" at the beginning):\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n<img src=\"https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png\" width=\"100%\"/>\\n\\n### üß® Diffusers\\n\\nThis model can be used just like any other Stable Diffusion model. For more information,\\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\\n\\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\\n\\n```python\\nfrom diffusers import StableDiffusionPipeline\\nimport torch\\nmodel_id = \"prompthero/openjourney\"\\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\\npipe = pipe.to(\"cuda\")\\nprompt = \"retro serie of different cars with different colors and shapes, mdjrny-v4 style\"\\nimage = pipe(prompt).images[0]\\nimage.save(\"./retro_cars.png\")\\n```', 'type': 'text'}], 'tags': [{'text': 'diffusers, safetensors, stable-diffusion, text-to-image, en, license:creativeml-openrail-m, autotrain_compatible, endpoints_compatible, diffusers:StableDiffusionPipeline, region:us', 'type': 'text'}], 'name': [{'text': 'lzyvegetable/openjourney', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '66d19fae0d63bb4c0558ce6d', 'avatarUrl': '/avatars/64ce764dc4eaf88511ec4261a39feac4.svg', 'fullname': 'lzy', 'name': 'lzyvegetable', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '6756dd5394b6d39dce3e1abf', 'repoOwnerId': '66772da23ae9e9c65972c6a2', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 22, 'updatedAt': 1759349077207, 'repoName': 'V-LM-FIX', 'repoOwner': 'moelanoby', 'tags': 'transformers, safetensors, qwen2, text-generation, conversational, zho, eng, fra, spa, por, deu, ita, rus, jpn, kor, vie, tha, ara, base_model:Qwen/Qwen2.5-0.5B-Instruct, base_model:finetune:Qwen/Qwen2.5-0.5B-Instruct, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'moelanoby/V-LM-FIX', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'V-LM-FIX', 'type': 'text'}], 'repoOwner': [{'text': 'moelanoby', 'type': 'text'}], 'fileContent': [{'text': '\\r\\nthis model is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" LM designed for murder drones fans \\r\\nif you are a murder drones fan and you like V very much this model will be perfect for you V will get fixes and stuff later\\r\\nso yeah that's all I have to say\", 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, qwen2, text-generation, conversational, zho, eng, fra, spa, por, deu, ita, rus, jpn, kor, vie, tha, ara, base_model:Qwen/Qwen2.5-0.5B-Instruct, base_model:finetune:Qwen/Qwen2.5-0.5B-Instruct, license:apache-2.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'moelanoby/V-LM-FIX', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '66772da23ae9e9c65972c6a2', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66772da23ae9e9c65972c6a2/nBCu297SUiEYiIxEjRr9i.png', 'fullname': 'Silly Veh :3', 'name': 'moelanoby', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 19}}, {'repoId': '67b41d3c45dc598e0f16422f', 'repoOwnerId': '67b41d0919f758f021bb395e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 26, 'updatedAt': 1759349283529, 'repoName': 'nova', 'repoOwner': 'nlop', 'tags': 'gguf, llama, nova, ai, nlop, nexiloop, llm, novaai, ainlop, nlopai, nexai, text-generation, conversational, aa, ae, am, en, es, ar, ja, eo, fr, ru, license:apache-2.0, endpoints_compatible, region:us', 'name': 'nlop/nova', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'nova', 'type': 'text'}], 'repoOwner': [{'text': 'nlop', 'type': 'text'}], 'fileContent': [{'text': '# Nexiloop Nova Model: Fully ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': '\\n\\n**License:** Apache-2.0  \\n**Datasets:**\\n- cerebras/SlimPajama-627B\\n- bigcode/starcoderdata\\n- OpenAssistant/oasst_top1_2023-08-25  \\n**Language:** English\\n\\n---\\n\\n<div align=\"center\">\\n\\n# Nexiloop Nova-1.1B  \\n**', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' and Ready for Use**  \\nFully optimized for various applications with a compact architecture.\\n\\n</div>\\n\\n[GitHub Repository](https://github.com/mohameodo/nova)\\n\\n---\\n\\nThe **Nexiloop Nova-1.1B** model is a fine-tuned version of the Llama 2 architecture with **1.1B parameters**. It has been trained on over **3 trillion tokens** and is built to provide high-quality, efficient responses in a wide variety of conversational contexts.\\n\\n### **Features:**\\n- **Optimized for Compact Systems:** With just 1.1B parameters, Nexiloop Nova is perfect for applications where memory and computation are limited.\\n- **Pretraining:** The model has been pre-trained on the **SlimPajama-627B** dataset, fine-tuned for even better conversational abilities.\\n\\n### **Training Overview:**\\nWe adopted the same architecture and tokenizer as **Llama 2**, which allows Nexiloop Nova to plug into many existing ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e projects. The training, which started on **2023-09-01**, used **16 A100-40G GPUs** to achieve remarkable optimization.\\n\\nThe model was initially fine-tuned on a variant of the **UltraChat** dataset, which consists of synthetic dialogues generated by **ChatGPT**. It was then further aligned using the **DPOTrainer** from **TRL**, utilizing a ranking dataset containing **64k prompts** and responses from **GPT-4**.\\n\\n---\\n\\n### **How to Use Nexiloop Nova Model**\\n\\nTo use Nexiloop Nova, you\\'ll need **transformers>=4.34**. Below is a simple example showing how to integrate the model into your application.\\n\\n#### Example Code:\\n\\n```bash\\n# Install necessary libraries\\npip install transformers==4.34\\npip install accelerate\\n\\n\\n\\nimport torch\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-generation\", model=\"nexiloop/nova\", torch_dtype=torch.bfloat16, device_map=\"auto\")\\n\\n# We use the tokenizer\\'s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\\nmessages = [\\n    {\\n        \"role\": \"system\",\\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\\n    },\\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\\n]\\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\\nprint(outputs[0][\"generated_text\"])\\n# <|system|>\\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\\n# <|user|>\\n# How many helicopters can a human eat in one sitting?</s>\\n# <|assistant|>\\n# ...\\n```', 'type': 'text'}], 'tags': [{'text': 'gguf, llama, nova, ai, nlop, nexiloop, llm, novaai, ainlop, nlopai, nexai, text-generation, conversational, aa, ae, am, en, es, ar, ja, eo, fr, ru, license:apache-2.0, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'nlop/nova', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67b41d0919f758f021bb395e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65f0fcfe6f1fb544c235b26c/7qi5aXuxLpvHRW3oL0bDW.png', 'fullname': 'nlop', 'name': 'nlop', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '68336f1c73372ba81d5f521a', 'repoOwnerId': '682e431e9c1b77a503f0afee', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759349652065, 'repoName': 'Recursive-SWE-bench', 'repoOwner': 'recursivelabsai', 'tags': 'arxiv:2505.12345, region:us', 'name': 'recursivelabsai/Recursive-SWE-bench', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Recursive-SWE-bench', 'type': 'text'}], 'repoOwner': [{'text': 'recursivelabsai', 'type': 'text'}], 'fileContent': [{'text': '# Recursive SWE-bench\\n## ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': '\\n\\n![Status](https://img.shields.io/badge/Status-Recursive%20Benchmark-crimson) [![License: MIT](https://img.shields.io/badge/License-MIT-lime.svg)](https://polyformproject.org/licenses/noncommercial/1.0.0/) [![LICENSE: CC BY-NC-ND 4.0](https://img.shields.io/badge/Content-CC--BY--NC--ND-turquoise.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/) ![Version](https://img.shields.io/badge/Version-0.1.0--alpha-purple)\\n\\n\\n## Evolution Beyond Linear Benchmarking\\n\\nRecursive-SWE-bench extends the established [**`SWE-bench`**](https://github.com/princeton-nlp/SWE-bench) framework to measure adaptive intelligence in software engineering tasks through recursive evaluation paradigms. While traditional benchmarks measure static, single-pass performance, Recursive-SWE-bench evaluates dynamic problem-solving capabilities across iterative refinement cycles.\\n\\n**Key innovation**: Benchmark tasks self-modify as models interact with them, creating a feedback loop that more accurately reflects real-world software engineering challenges.\\n\\n\\n## Why Recursive Benchmarking?\\n\\nTraditional benchmarks evaluate models using a linear, static framework:\\n\\n```\\nInput ‚Üí Model ‚Üí Output ‚Üí Evaluation ‚Üí Score\\n```\\n\\nReal-world engineering is inherently recursive:\\n\\n```\\nProblem ‚Üí Solution ‚Üí Testing ‚Üí Feedback ‚Üí Refinement ‚Üí New Problem State ‚Üí ...\\n```\\n\\nRecursive-SWE-bench captures this dynamic process, measuring:\\n\\n- **Adaptive reasoning**: How models incorporate feedback into subsequent solution attempts\\n- **Self-correction**: The ability to identify and fix errors across iterations\\n- **Learning efficiency**: How quickly models converge on optimal solutions\\n- **Meta-problem understanding**: Recognition of patterns across related problem states\\n- **Probabilistic optimization**: Managing uncertainty in problem specifications and solution spaces\\n\\n## Core Innovations\\n\\n1. **Dynamic Task Evolution**: Tasks transform based on model interactions, generating unique problem sequences for each evaluation run\\n   \\n2. **Recursive Evaluation Metrics**: Performance measured across solution trajectories rather than single attempts\\n   \\n3. **Self-Modifying Test Harnesses**: Evaluation environments that adapt to model capabilities, maintaining consistent challenge levels\\n   \\n4. **Meta-learning Assessment**: Explicit measurement of knowledge transfer between related problems\\n   \\n5. **Feedback Integration Protocols**: Standardized frameworks for delivering actionable feedback to models\\n\\n## Quick Start\\n\\n```bash\\n# Install the package\\npip install recursive-swe-bench\\n\\n# Run a basic evaluation\\nrswe-bench evaluate --model your-model-name --task-set standard --iterations 5\\n\\n# Generate a performance report\\nrswe-bench report --results-dir ./results --visualization recursive-trajectory\\n```\\n\\n## Benchmark Structure\\n\\nRecursive-SWE-bench organizes tasks into recursive trajectories:\\n\\n- **Task Generators**: Dynamically create problem instances based on model interaction history\\n- **Feedback Modules**: Provide standardized assessment of solutions with actionable insights\\n- **State Trackers**: Maintain the evolving state of problems across solution attempts\\n- **Meta-Pattern Evaluators**: Assess model ability to identify patterns across problem sequences\\n\\n## Task Categories\\n\\n| Category | Description | Recursive Elements |\\n|----------|-------------|-------------------|\\n| Bug Fixing | Identify and resolve issues in existing code | Error patterns transform based on fix attempts |\\n| Feature Implementation | Add functionality to existing codebases | Requirements evolve as implementation progresses |\\n| Refactoring | Improve code structure without changing behavior | Complexity dynamically adjusts to refactoring success |\\n| System Design | Create architecture for complex systems | Design constraints adapt to proposed solutions |\\n| Test Generation | Create effective test suites | Test coverage requirements shift with implementation |\\n| Documentation | Create clear technical documentation | Clarity targets adapt to explanation attempts |\\n\\n## Performance Metrics\\n\\nRecursive-SWE-bench evaluates models using both traditional and recursive metrics:\\n\\n### Traditional Metrics\\n- Pass@k (for varying k)\\n- Execution accuracy\\n- Code similarity to human solutions\\n\\n### Recursive Metrics\\n- **Convergence Rate**: How quickly models reach stable solutions\\n- **Adaptation Efficiency**: Performance improvements per feedback iteration\\n- **Transfer Learning Factor**: Performance gains across related problems\\n- **Learning Curve Area**: Integration of performance across all iterations\\n- **Probabilistic Solution Quality**: Distribution of solution quality across runs\\n- **Dynamic Complexity Handling**: Performance across varying problem complexity\\n\\n## Sample Results\\n\\nHere\\'s how various models perform on Recursive-SWE-bench:\\n\\n<p align=\"center\">\\n  <img src=\"docs/assets/performance-comparison.png\" alt=\"Performance Comparison\" width=\"650\"/>\\n</p>\\n\\n*Note: These preliminary results demonstrate how recursive evaluation reveals capabilities not captured by traditional single-pass benchmarks.*\\n\\n## Citation\\n\\nIf you use Recursive-SWE-bench in your research, please cite:\\n\\n```bibtex\\n@article{recursive2025swebench,\\n  title={Recursive-SWE-bench: Evaluating Adaptive Programming Intelligence Through Self-Modifying Benchmarks},\\n  author={Recursive Labs Team},\\n  journal={arXiv preprint arXiv:2505.12345},\\n  year={2025}\\n}\\n```\\n\\n## Contributing\\n\\nWe welcome contributions to Recursive-SWE-bench! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.\\n\\n### Key Areas for Contribution\\n\\n- Additional recursive task generators\\n- Enhanced feedback mechanisms\\n- New evaluation metrics\\n- Integration with more models and frameworks\\n- Documentation and tutorials\\n\\n## License\\n\\nRecursive-SWE-bench is released under the [MIT License](LICENSE).\\n\\n## Acknowledgments\\n\\nRecursive-SWE-bench builds upon the foundation established by the original SWE-bench, created by the Princeton NLP group. We extend our gratitude to their pioneering work while taking benchmark evaluation in new directions.\\n', 'type': 'text'}], 'tags': [{'text': 'arxiv:2505.12345, region:us', 'type': 'text'}], 'name': [{'text': 'recursivelabsai/Recursive-SWE-bench', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '682e431e9c1b77a503f0afee', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/682e242bf9c621a2f8ec03e8/SRDsFsqYOLHoKcwb22PSa.png', 'fullname': 'Recursive Labs', 'name': 'recursivelabsai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 9}}, {'repoId': '68824d06320ad501de03daaa', 'repoOwnerId': '65327bbe910b844786524e3c', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 13, 'updatedAt': 1759349889939, 'repoName': 'tts_mazlum_kiper_tur_001', 'repoOwner': 'omersaidd', 'tags': 'transformers, safetensors, llama, text-generation, TTS, text-to-speech, tr, dataset:omersaidd/tts_mazlum_kiper_tur, base_model:canopylabs/orpheus-3b-0.1-ft, base_model:finetune:canopylabs/orpheus-3b-0.1-ft, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'omersaidd/tts_mazlum_kiper_tur_001', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'tts_mazlum_kiper_tur_001', 'type': 'text'}], 'repoOwner': [{'text': 'omersaidd', 'type': 'text'}], 'fileContent': [{'text': '\\n# Merhaba Arkada≈ülar üöÄ \\n\\n### ü§ñ ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' TTS ( Text To Speech ) modellerin T√ºrk√ße √ºzerine optimizasyonu i√ßin olu≈üturduƒüum veri setinin eƒüitim sonucu a≈üaƒüƒ±da ki gibidir.\\n\\n### Amacƒ±m T√ºrk√ße tarafƒ±nda kaliteli veri setleri olu≈üturmak ve a√ßƒ±k kaynak TTS modellerin T√ºrk√ße performansƒ±nƒ± artƒ±rabilmektir.\\n\\n### Bu modelin eƒüitilmesi i√ßin olu≈üturulan https://huggingface.co/datasets/omersaidd/tts_mazlum_kiper_tur veri seti ve canopylabs/orpheus-3b-0.1-ft TTS modelin √ßƒ±ktƒ±sƒ± olan eƒüitilen modelin ticari bir ama√ßla deƒüil ara≈ütƒ±rma i√ßin kullnƒ±lmƒ±≈ütƒ±r. Veri setinde ki sesin sahibi olan Mazlum Kiper ve veri setinin olduƒüu platform olan Youtube b√ºt√ºn kullanƒ±m haklarƒ±na sahiptir', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, llama, text-generation, TTS, text-to-speech, tr, dataset:omersaidd/tts_mazlum_kiper_tur, base_model:canopylabs/orpheus-3b-0.1-ft, base_model:finetune:canopylabs/orpheus-3b-0.1-ft, license:mit, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'omersaidd/tts_mazlum_kiper_tur_001', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65327bbe910b844786524e3c', 'avatarUrl': '/avatars/524360c04a6c940d14f5c9bd5e4141fb.svg', 'fullname': '√ñmer Said Yilmaz', 'name': 'omersaidd', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 4}}, {'repoId': '68967adbb3ed7f312e787459', 'repoOwnerId': '67ed9d320477637a48ef295b', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349944779, 'repoName': 'leaf', 'repoOwner': 'weno-ai', 'tags': 'research, text-generation, en, dataset:deepseek-ai/DeepSeek-ProverBench, license:mit, region:us', 'name': 'weno-ai/leaf', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'leaf', 'type': 'text'}], 'repoOwner': [{'text': 'weno-ai', 'type': 'text'}], 'fileContent': [{'text': '# Leaf\\n\\nAn ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' \"prototype\" AI model used for AI research.\\n\\n## About this project\\n\\nLeaf is an \"experimental\" AI model, utilising PyTorch.\\n\\n## Research\\n\\nWith leaf we\\'ve been testing many capabilities of what AI could do. \\n\\nStarting with a simple \"embedded\" python dataset, leaf uses only 2700 steps for training (the more steps, the  better it learns).\\n\\n**Training Data:** `\\n{\"this is a much longer text that will serve as a simple dataset for our tiny language model. The model will learn to predict the next character based on the previous characters in the sequence.\"}\\n{\"text\": \"This demonstrates the core idea behind training an autoregressive language model. The quick brown fox jumps over the lazy dog.\"}\\n{\"text\": \"A journey of a thousand miles begins with a single step. The early bird catches the worm. All that glitters is not gold. A stitch in time saves nine.\"}\\n{\"text\": \"Where there\\'s a will, there\\'s a way. Look before you leap. You can\\'t make an omelette without breaking a few eggs. Practice makes perfect. Don\\'t count your chickens before they hatch.\"}`\\n\\nHowever this result came with the following output: \\n\\n`text that will serve`\\n\\nThen we used JSONL databases from the community, and unfortunatly this was the output:\\n\\n`rimetricE7tich then`', 'type': 'text'}], 'tags': [{'text': 'research, text-generation, en, dataset:deepseek-ai/DeepSeek-ProverBench, license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'weno-ai/leaf', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67ed9d320477637a48ef295b', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/66873c75bcb8e14746e7ea4d/riqybzB6jKcHD8W3ElKMy.png', 'fullname': 'Weno, Inc.', 'name': 'weno-ai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 1}}, {'repoId': '68cf0bf1bb352fc40626a8cd', 'repoOwnerId': '6731977d7ddcac2df6a68294', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759350131607, 'repoName': 'stylopara-template', 'repoOwner': 'PoppaYAO', 'tags': 'region:us', 'name': 'PoppaYAO/stylopara-template', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'stylopara-template', 'type': 'text'}], 'repoOwner': [{'text': 'PoppaYAO', 'type': 'text'}], 'fileContent': [{'text': '# Stylopara Template (', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ')\\n\\nThis repo contains code and instructions to build a style-preserving paraphraser (FLAN-T5 + LoRA + style/semantic reranking).\\n\\n**Important:** Do NOT upload any private writing, datasets, or style fingerprints (e.g., `style_centroid.npy`) here.\\n', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'PoppaYAO/stylopara-template', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6731977d7ddcac2df6a68294', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Qxp51Hl1ZB1pZq8gHDFSm.png', 'fullname': 'John Wairimu', 'name': 'PoppaYAO', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '6566f904944a6c35cf86447c', 'repoOwnerId': '671f7ee82174ca42d524f1d3', 'isPrivate': False, 'type': 'model', 'likes': 1, 'isReadmeFile': True, 'readmeStartLine': 39, 'updatedAt': 1759348547126, 'repoName': 'open-australian-legal-llm', 'repoOwner': 'isaacus', 'tags': 'transformers, safetensors, gpt2, text-generation, law, legal, australia, generated_from_trainer, en, dataset:umarbutler/open-australian-legal-corpus, base_model:openai-community/gpt2-xl, base_model:finetune:openai-community/gpt2-xl, doi:10.57967/hf/1773, license:apache-2.0, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'isaacus/open-australian-legal-llm', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'open-australian-legal-llm', 'type': 'text'}], 'repoOwner': [{'text': 'isaacus', 'type': 'text'}], 'fileContent': [{'text': '\\n# ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal LLM \\u200d‚öñÔ∏è\\nThe ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal LLM is the largest ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" language model trained on Australian law.\\n\\nWith over 1.5 billion parameters, the model's size and the richness and quality of its training data, comprising roughly 70,000 laws, regulations and decisions across six Australian jurisdictions from the [\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': \" Australian Legal Corpus](https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus), make it well suited for finetuning on a diverse range of downstream natural language processing tasks applied to the Australian legal domain, including text generation, text completion and question answering.\\n\\nTo ensure its accessibility to as wide an audience as possible, the model is issued under the [Apache Licence 2.0](https://www.apache.org/licenses/LICENSE-2.0.html).\\n\\n## Usage üë©\\u200düíª\\nThe code snippet below demonstrates just one of the many ways in which the model may be accessed:\\n```python\\n>>> from transformers import pipeline, set_seed\\n\\n>>> set_seed(42) # We set a seed for reproducibility.\\n>>> generator = pipeline('text-generation', model='umarbutler/open-australian-legal-llm')\\n\\n>>> response = generator('Section 51 of the Constitution provides', max_length=55)\\n>>> print(response[0]['generated_text'])\\n```\\n\\n## Creation üß™\\nThe following cleaning procedures were applied to all 218,340 laws, regulations and decisions in version 4.2.0 of the [\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': \" Australian Legal Corpus](https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus):\\n1. Non-breaking spaces were replaced with regular spaces;\\n1. Return carriages followed by newlines were replaced with newlines;\\n1. Whitespace was removed from lines comprised entirely of whitespace;\\n1. Newlines and whitespace preceding newlines were removed from the end of texts;\\n1. Newlines and whitespace succeeding newlines were removed from the beginning of texts; and\\n1. Spaces and tabs were removed from the end of lines.\\n\\nAfter cleaning, texts with less than 128 characters and those with duplicate XXH3 128-bit hashes were removed, leaving 218,207 documents. These documents were then used to pretrain a [GPT2](https://huggingface.co/gpt2-xl)-like tokenizer, after which they were split into blocks 512-tokens-long, with the tokenizer's end-of-sequence token ('<|endoftext|>') being used as a delimiter as well as to pad the end of the final block. An attention mask was applied to the end-of-sequence tokens used as padding, barring the first such token. The resulting blocks were subsequently randomly shuffled and split into a training dataset of 1,966,867 chunks and a validation dataset of 218,541.\\n\\n[GPT2-XL](https://huggingface.co/gpt2-xl) was used as a base model. Input embeddings for tokens shared between the vocabulary trained on the Corpus and that of [GPT2](https://huggingface.co/gpt2-xl) were preserved but moved to their new positions. Embeddings for unique tokens were set to the average embedding weights.\\n\\nThe model was trained with the following hyperparameters for the first 100,290 steps:\\n| Hyperparameter | Value |\\n| --- | --- |\\n| Sequence length | 512 |\\n| Epochs | 1 |\\n| Optimiser | AdamW |\\n| Learning rate | 1e-4 |\\n| Learning rate scheduler | Linear with warmup |\\n| Batch size | 6 |\\n| Weight decay | 0.01 |\\n| Warmup ratio | 0.06 |\\n\\nAfter training on two RTX A6000s for \\\\~120,050 steps over a period of 91 hours, the [vast.ai](https://vast.ai) instance hosting the model crashed. Fortunately, a checkpoint had been saved at step 100,290 (\\\\~60% of an epoch), although the optimiser's state was mistakenly not downloaded. The model was subsequently moved to a new instance where it was trained on an L40 for a further 133,711 steps (\\\\~40% of an epoch) with the following hyperparameters (changes emphasised):\\n| Hyperparameter | Value |\\n| --- | --- |\\n| Sequence length | 512 |\\n| Epochs | 1 |\\n| Optimiser | AdamW |\\n| Learning rate | *4.255e-5* |\\n| Learning rate scheduler | *Linear* |\\n| Batch size | *3* |\\n| Weight decay | 0.01 |\\n| Warmup ratio | *0.00* |\\n\\nNaturally, as the optimiser state had been lost, the model's learning rate descended slower than it had been previously. Nevertheless, after completing an epoch of training, the model was able to achieve a validation loss of 2.04.\\n\\n## Benchmarks üìä\\nTested against version 2.0.0 of the [\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal QA](https://huggingface.co/datasets/umarbutler/open-australian-legal-qa) dataset, the model achieved a perplexity of 8.01, outperforming all known language models for Australian law.\\n| Model | Parameters | Perplexity |\\n|--|--|--|\\n| **', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal LLM** | **1.5B** | **8.01** |\\n| [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal Phi 1.5](https://huggingface.co/umarbutler/open-australian-legal-phi-1_5) | 1.3B | 8.69 |\\n| [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal GPT2](https://huggingface.co/umarbutler/open-australian-legal-gpt2) | 124M | 16.37 |\\n| [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal DistilGPT2](https://huggingface.co/umarbutler/open-australian-legal-distilgpt2) | 88.2M | 23.9 |\\n\\n## Limitations üöß\\nAlthough the model has not been tested for bias, one would expect it to exhibit much of the same, if not all, the biases of [GPT2-XL](https://huggingface.co/gpt2-xl).\\n\\nOne might also expect the model to exhibit a bias towards the type of language employed in laws, regulations and decisions (its ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' material) as well as towards Commonwealth and New South Wales law (the largest ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': 's of documents in the [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': \" Australian Legal Corpus](https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus) at the time of the model's creation).\\n\\nFinally, it is worth noting that the model may lack knowledge of Victorian, Northern Territory and Australian Capital Territory law as licensing restrictions had prevented their inclusion in the training data.\\n\\n## Licence üìú\\nTo ensure its accessibility to as wide an audience as possible, the model is issued under the [Apache Licence 2.0](https://www.apache.org/licenses/LICENSE-2.0.html).\\n\\n## Citation üîñ\\nIf you've relied on the model for your work, please cite:\\n```bibtex\\n@misc{butler-2023-open-australian-legal-llm,\\n    author = {Butler, Umar},\\n    year = {2023},\\n    title = {\", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal LLM},\\n    publisher = {Hugging Face},\\n    version = {1.0.0},\\n    url = {https://huggingface.co/datasets/umarbutler/open-australian-legal-llm}\\n}\\n```\\n\\n## Acknowledgements üôè\\nIn the spirit of reconciliation, the author acknowledges the Traditional Custodians of Country throughout Australia and their connections to land, sea and community. He pays his respect to their Elders past and present and extends that respect to all Aboriginal and Torres Strait Islander peoples today.\\n\\nThe author thanks the ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': 's of the [', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' Australian Legal Corpus](https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus) for making their data available under ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' licences.\\n\\nThe author also acknowledges the developers of the many Python libraries relied upon in the training of the model, as well as the makers of [GPT2](https://huggingface.co/gpt2-xl), which the model was built atop.\\n\\nFinally, the author is eternally grateful for the endless support of his wife and her willingness to put up with many a late night spent writing code and quashing bugs.', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, gpt2, text-generation, law, legal, australia, generated_from_trainer, en, dataset:umarbutler/open-australian-legal-corpus, base_model:openai-community/gpt2-xl, base_model:finetune:openai-community/gpt2-xl, doi:10.57967/hf/1773, license:apache-2.0, model-index, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'isaacus/open-australian-legal-llm', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '671f7ee82174ca42d524f1d3', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6497ffbf2a997a45e987e139/HCfH2lC0OMQWC5PiwZQya.png', 'fullname': 'Isaacus', 'name': 'isaacus', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 15}}, {'repoId': '65b909d6dd94007789899683', 'repoOwnerId': '633fe81429b5a95f6e16e34a', 'isPrivate': False, 'type': 'model', 'likes': 2, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759348602821, 'repoName': 'MiniCPM-V', 'repoOwner': 'openbmb', 'tags': 'transformers, safetensors, minicpmv, feature-extraction, visual-question-answering, custom_code, arxiv:2308.12038, region:us', 'name': 'openbmb/MiniCPM-V', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'MiniCPM-V', 'type': 'text'}], 'repoOwner': [{'text': 'openbmb', 'type': 'text'}], 'fileContent': [{'text': '\\n## MiniCPM-V\\n### News\\n- [2025.01.14] üî•üî• We ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' [**MiniCPM-o 2.6**](https://huggingface.co/openbmb/MiniCPM-o-2_6), with significant performance improvement over **MiniCPM-V 2.6**, and support real-time speech-to-speech conversation and multimodal live streaming. Try it now.\\n- [2024.08.06] üî• We ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e [**MiniCPM-V 2.6**](https://huggingface.co/openbmb/MiniCPM-V-2_6), which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad.\\n- [2024.05.20] üî• GPT-4V level multimodal model [**MiniCPM-Llama3-V 2.5**](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5) is out.\\n- [2024.04.11] üî• [**MiniCPM-V 2.0**](https://huggingface.co/openbmb/MiniCPM-V-2) is out.\\n  \\n\\n**MiniCPM-V** (i.e., OmniLMM-3B) is an efficient version with promising performance for deployment. The model is built based on SigLip-400M and [MiniCPM-2.4B](https://github.com/OpenBMB/MiniCPM/), connected by a perceiver resampler. Notable features of OmniLMM-3B include:\\n\\n- ‚ö°Ô∏è **High Efficiency.** \\n\\n  MiniCPM-V can be **efficiently deployed on most GPU cards and personal computers**, and **even on end devices such as mobile phones**. In terms of visual encoding, we compress the image representations into 64 tokens via a perceiver resampler, which is significantly fewer than other LMMs based on MLP architecture (typically > 512 tokens). This allows OmniLMM-3B to operate with **much less memory cost and higher speed during inference**.\\n\\n- üî• **Promising Performance.** \\n\\n  MiniCPM-V achieves **state-of-the-art performance** on multiple benchmarks (including MMMU, MME, and MMbech, etc) among models with comparable sizes, surpassing existing LMMs built on Phi-2. It even **achieves comparable or better performance than the 9.6B Qwen-VL-Chat**.\\n\\n- üôå **Bilingual Support.** \\n\\n  MiniCPM-V is **the first end-deployable LMM supporting bilingual multimodal interaction in English and Chinese**. This is achieved by generalizing multimodal capabilities across languages, a technique from the ICLR 2024 spotlight [paper](https://arxiv.org/abs/2308.12038).\\n\\n### Evaluation\\n\\n<div align=\"center\">\\n\\n<table style=\"margin: 0px auto;\">\\n<thead>\\n  <tr>\\n    <th align=\"left\">Model</th>\\n    <th>Size</th>\\n    <th>MME</th>\\n    <th nowrap=\"nowrap\" >MMB dev (en)</th>\\n    <th nowrap=\"nowrap\" >MMB dev (zh)</th>\\n    <th nowrap=\"nowrap\" >MMMU val</th>\\n    <th nowrap=\"nowrap\" >CMMMU val</th>\\n  </tr>\\n</thead>\\n<tbody align=\"center\">\\n  <tr>\\n    <td align=\"left\">LLaVA-Phi</td>\\n    <td align=\"right\">3.0B</td>\\n    <td>1335</td>\\n    <td>59.8</td>\\n    <td>- </td>\\n    <td>- </td>\\n    <td>- </td>\\n  </tr>\\n  <tr>\\n    <td nowrap=\"nowrap\" align=\"left\">MobileVLM</td>\\n    <td align=\"right\">3.0B</td>\\n    <td>1289</td>\\n    <td>59.6</td>\\n    <td>- </td>\\n    <td>- </td>\\n    <td>- </td>\\n  </tr>\\n  <tr>\\n    <td nowrap=\"nowrap\" align=\"left\" >Imp-v1</td>\\n    <td align=\"right\">3B</td>\\n    <td>1434</td>\\n    <td>66.5</td>\\n    <td>- </td>\\n    <td>- </td>\\n    <td>- </td>\\n  </tr>\\n  <tr>\\n    <td nowrap=\"nowrap\" align=\"left\" >Qwen-VL-Chat</td>\\n    <td align=\"right\" >9.6B</td>\\n    <td>1487</td>\\n    <td>60.6 </td>\\n    <td>56.7 </td>\\n    <td>35.9 </td>\\n    <td>30.7 </td>\\n  </tr>\\n  <tr>\\n    <td nowrap=\"nowrap\" align=\"left\" >CogVLM</td>\\n    <td align=\"right\">17.4B </td>\\n    <td>1438 </td>\\n    <td>63.7 </td>\\n    <td>53.8 </td>\\n    <td>32.1 </td>\\n    <td>- </td>\\n  </tr>\\n  <tr>\\n    <td nowrap=\"nowrap\" align=\"left\" ><b>MiniCPM-V</b></td>\\n    <td align=\"right\">3B </td>\\n    <td>1452 </td>\\n    <td>67.9 </td>\\n    <td>65.3 </td>\\n    <td>37.2 </td>\\n    <td>32.1 </td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n</div>\\n\\n\\n### Examples\\n<div align=\"center\">\\n<table>\\n  <tr>\\n    <td>\\n      <p> \\n        <img src=\"assets/Mushroom_en.gif\" width=\"400\"/>\\n      </p>\\n    </td>\\n    <td>\\n      <p> \\n        <img src=\"assets/Snake_en.gif\" width=\"400\"/>\\n      </p>\\n    </td>\\n  </tr>\\n</table>\\n</div>\\n\\n\\n## Demo\\nClick here to try out the Demo of [MiniCPM-V](http://120.92.209.146:80).\\n\\n## Deployment on Mobile Phone\\nCurrently MiniCPM-V (i.e., OmniLMM-3B) can be deployed on mobile phones with Android and Harmony operating systems. üöÄ Try it out [here](https://github.com/OpenBMB/mlc-MiniCPM).\\n\\n\\n## Usage\\nInference using Huggingface transformers on Nivdia GPUs or Mac with MPS (Apple silicon or AMD GPUs). Requirements tested on python 3.10Ôºö\\n```\\nPillow==10.1.0\\ntimm==0.9.10\\ntorch==2.1.2\\ntorchvision==0.16.2\\ntransformers==4.36.0\\nsentencepiece==0.1.99\\n```\\n\\n```python\\n# test.py\\nimport torch\\nfrom PIL import Image\\nfrom transformers import AutoModel, AutoTokenizer\\n\\nmodel = AutoModel.from_pretrained(\\'openbmb/MiniCPM-V\\', trust_remote_code=True, torch_dtype=torch.bfloat16)\\n# For Nvidia GPUs support BF16 (like A100, H100, RTX3090)\\nmodel = model.to(device=\\'cuda\\', dtype=torch.bfloat16)\\n# For Nvidia GPUs do NOT support BF16 (like V100, T4, RTX2080)\\n#model = model.to(device=\\'cuda\\', dtype=torch.float16)\\n# For Mac with MPS (Apple silicon or AMD GPUs).\\n# Run with `PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py`\\n#model = model.to(device=\\'mps\\', dtype=torch.float16)\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'openbmb/MiniCPM-V\\', trust_remote_code=True)\\nmodel.eval()\\n\\nimage = Image.open(\\'xx.jpg\\').convert(\\'RGB\\')\\nquestion = \\'What is in the image?\\'\\nmsgs = [{\\'role\\': \\'user\\', \\'content\\': question}]\\n\\nres, context, _ = model.chat(\\n    image=image,\\n    msgs=msgs,\\n    context=None,\\n    tokenizer=tokenizer,\\n    sampling=True,\\n    temperature=0.7\\n)\\nprint(res)\\n```\\n\\nPlease look at [GitHub](https://github.com/OpenBMB/OmniLMM) for more detail about usage.\\n\\n## License\\n\\n#### Model License\\n* The code in this repo is released under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) License. \\n* The usage of MiniCPM-V series model weights must strictly follow [MiniCPM Model License.md](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md).\\n* The models and weights of MiniCPM are completely free for academic research. after filling out a [\"questionnaire\"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g) for registration, are also available for free commercial use.\\n\\n\\n#### Statement\\n* As a LLM, MiniCPM-V generates contents by learning a large mount of texts, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V does not represent the views and positions of the model developers\\n* We will not be liable for any problems arising from the use of the MinCPM-V ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' model, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\\n\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, minicpmv, feature-extraction, visual-question-answering, custom_code, arxiv:2308.12038, region:us', 'type': 'text'}], 'name': [{'text': 'openbmb/MiniCPM-V', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '633fe81429b5a95f6e16e34a', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1670387859384-633fe7784b362488336bbfad.png', 'fullname': 'OpenBMB', 'name': 'openbmb', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': True, 'followerCount': 2098}}, {'repoId': '621ffdc136468d709f17d92d', 'repoOwnerId': '5f15c747925b9863e28ad36e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759348288864, 'repoName': 'blenderbot_small-news', 'repoOwner': 'lordtt13', 'tags': 'transformers, pytorch, tf, safetensors, blenderbot-small, text2text-generation, en, endpoints_compatible, region:us', 'name': 'lordtt13/blenderbot_small-news', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'blenderbot_small-news', 'type': 'text'}], 'repoOwner': [{'text': 'lordtt13', 'type': 'text'}], 'fileContent': [{'text': '\\n## BlenderBotSmall-News: Small version of a state-of-the-art ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' chatbot, trained on custom summaries\\n\\n### Details of BlenderBotSmall\\n\\nThe **BlenderBotSmall** model was presented in [A state-of-the-art ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" chatbot](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/) by *Facebook AI* and here are it's details:\\n\\n- Facebook AI has built and \", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'ed BlenderBot, the largest-ever open-domain chatbot. It outperforms others in terms of engagement and also feels more human, according to human evaluators.\\n- The culmination of years of research in conversational AI, this is the first chatbot to blend a diverse set of conversational skills ‚Äî including empathy, knowledge, and personality ‚Äî together in one system.\\n- We achieved this milestone through a new chatbot recipe that includes improved decoding techniques, novel blending of skills, and a model with 9.4 billion parameters, which is 3.6x more than the largest existing system.\\n\\n### Details of the downstream task (Summarization) - Dataset üìö\\n\\nA custom dataset was used, which was hand prepared by [SmokeTrees Digital](https://github.com/smoke-trees) AI engineers. This data contains long texts and summaries.\\n\\n### Model training\\n\\nThe training script is present [here](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb).\\n\\n### Pipelining the Model\\n\\n```python\\nmodel = transformers.BlenderbotSmallForConditionalGeneration.from_pretrained(\\'lordtt13/blenderbot_small-news\\')\\n\\ntokenizer = transformers.BlenderbotSmallTokenizer.from_pretrained(\"lordtt13/blenderbot_small-news\")\\n\\nnlp_fill = transformers.pipeline(\\'summarization\\', model = model, tokenizer = tokenizer)\\nnlp_fill(\\'The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ‚Çπ209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.\\', min_length=5, max_length=40)\\n\\n# Output:\\n# [{\\'summary_text\\': \\'marize: the cbi booked four former officials of syndicate bank and six others for cheating , forgery , criminal conspiracy and causing 209 crore loss to the staterun bank\\'}]\\n```\\n\\n> Created by [Tanmay Thakur](https://github.com/lordtt13) | [LinkedIn](https://www.linkedin.com/in/tanmay-thakur-6bb5a9154/)\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, tf, safetensors, blenderbot-small, text2text-generation, en, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'lordtt13/blenderbot_small-news', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '5f15c747925b9863e28ad36e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1595262772756-noauth.jpeg', 'fullname': 'Tanmay Thakur', 'name': 'lordtt13', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '63b999a0a7c8c8060062a3f8', 'repoOwnerId': '63b985e4efe99543b34cda6e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759348340214, 'repoName': 'skopstask', 'repoOwner': 'tareknaser', 'tags': 'region:us', 'name': 'tareknaser/skopstask', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'skopstask', 'type': 'text'}], 'repoOwner': [{'text': 'tareknaser', 'type': 'text'}], 'fileContent': [{'text': '# Internship appplication task\\nPosition: ML ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Engineer Internship - skops: Hugging Face and scikit-learn\\n\\n## Task requirments\\n1. Create a python environment and install `scikit-learn` version `1.0` in that environment.\\n2. Using that environment, create a `LogisticRegression` model and fit it on the Iris dataset.\\n3. Save the trained model using `pickle` or `joblib`.\\n4. Create a second environment, and install `scikit-learn` version `1.1` in it.\\n5. Try loading the model you saved in step 3 in this second environment.  \\n\\n\\n## Steps Taken\\n1. I used mamba to create the environment locally.\\n2. Trained a simple logistic regression model in `model.ipynb`.\\n3. Used pickle to save the model in `finalized_model.sav`.\\n4. Created another environment with `scikit-learn` version `1.1`.\\n5. Imported the model in `importingmodel.ipynb`.\\n\\n## Observations\\nA warning is shown when trying to load the model.\\n```\\n/home/tarek/.local/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.1.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\\nhttps://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\\n  warnings.warn(\\n```', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'tareknaser/skopstask', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63b985e4efe99543b34cda6e', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1673102776663-noauth.jpeg', 'fullname': 'Tarek Elsayed', 'name': 'tareknaser', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '64d21f93f1fb6b41aeee3e9a', 'repoOwnerId': '64d12f23f6925572da2951bf', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759348457775, 'repoName': 'glaive-function-calling-v1', 'repoOwner': 'glaiveai', 'tags': 'transformers, pytorch, mpt, text-generation, custom_code, dataset:glaiveai/glaive-function-calling, license:cc-by-sa-4.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'name': 'glaiveai/glaive-function-calling-v1', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'glaive-function-calling-v1', 'type': 'text'}], 'repoOwner': [{'text': 'glaiveai', 'type': 'text'}], 'fileContent': [{'text': '# glaive-function-calling-v1\\n\\nglaive-function-calling-v1 is a 2.7B parameter ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' chat model trained on data generated from Glaive‚Äôs synthetic data generation platform, which has similar function calling abilities as gpt-3.5 and gpt 4.\\n\\nThe model is capable of having multi-turn conversations and intelligently choosing when to execute a function (provided at the beginning of the conversation as a system prompt) based on the conversation. The model is trained on top of the https://huggingface.co/replit/replit-code-v1-3b model.\\n\\n\\n## Usage:\\n\\nYou can run the model in the following way-\\n```\\nfrom transformers import AutoModelForCausalLM , AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"glaiveai/glaive-function-calling-v1\", trust_remote_code=True)\\nmodel = AutoModelForCausalLM.from_pretrained(\"glaiveai/glaive-function-calling-v1\", trust_remote_code=True).half().cuda()\\n\\ninputs = tokenizer(prompt,return_tensors=\"pt\").to(model.device)\\n\\noutputs = model.generate(**inputs,do_sample=True,temperature=0.1,top_p=0.95,max_new_tokens=100)\\n\\nprint(tokenizer.decode(outputs[0],skip_special_tokens=True))\\n```\\n\\nThis model uses the following prompt format-\\n\\n```\\nSYSTEM: You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed-\\n{\\n            \"name\": \"plan_holiday\",\\n            \"description\": \"Plan a holiday based on user\\'s interests\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"destination\": {\\n                        \"type\": \"string\",\\n                        \"description\": \"The destination of the holiday\",\\n                    },\\n                    \"duration\": {\\n                        \"type\": \"integer\",\\n                        \"description\": \"The duration of the trip in holiday\",\\n                    },\\n                },\\n                \"required\": [\"destination\", \"duration\"],\\n            },\\n}\\nUSER: I am thinking of having a 10 day long vacation in Greece, can you help me plan it?\\n```\\nBased on which the model outputs-\\n```\\nASSISTANT: <functioncall> {\"name\": \"plan_holiday\", \"arguments\": \\'{\\n  \"destination\": \"Greece\",\\n  \"duration\": 10\\n}\\'}\\n```\\nThe model precedes all function invocations with `<functioncall>`.\\n\\nThe response of the function call should be sent to the model as-\\n\\n```\\nFUNCTION CALL: {\"places_to_visit\":[\"Athens\",\"Santorini\",\"Mykonos\"]}\\n```\\n\\nThe model can do multi-turn conversation in the above format.\\n\\n\\nWe\\'re working on providing an inference server which can act as a drop in replacement to the OpenAI API, you can follow [this](https://github.com/glaive-ai/function-calling-server) repo for the server.\\n\\n## Known Limitations:\\n\\n- While the model does well on function calling use-cases, it doesn\\'t always generalize very well to other chat use-cases. This is intentional as our thesis at Glaive is to provide use-case specialised model that are only used for the given task.\\n- The model may sometimes hallucinate functions, v2 of the model will be aimed to fix that with a bigger dataset.', 'type': 'text'}], 'tags': [{'text': 'transformers, pytorch, mpt, text-generation, custom_code, dataset:glaiveai/glaive-function-calling, license:cc-by-sa-4.0, autotrain_compatible, text-generation-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'glaiveai/glaive-function-calling-v1', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '64d12f23f6925572da2951bf', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61730f34174bd6e34fd40191/c2M66oeSKuoJtJsM8veVR.png', 'fullname': 'Glaive AI', 'name': 'glaiveai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 137}}, {'repoId': '654d60fed2db4280d93019eb', 'repoOwnerId': '6345bd89fe134dfd7a0dba40', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759348529985, 'repoName': 'Gender-Classifier', 'repoOwner': 'MonsterMMORPG', 'tags': 'region:us', 'name': 'MonsterMMORPG/Gender-Classifier', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Gender-Classifier', 'type': 'text'}], 'repoOwner': [{'text': 'MonsterMMORPG', 'type': 'text'}], 'fileContent': [{'text': '# Gender Classifier\\n\\n## State Of The Art ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Gender Classifier Script\\n\\n## Download link : https://www.patreon.com/posts/92607385\\n\\nIf you are looking for State of The Art (SOTA) ‚Äî The Very Best gender classifier Model just released the scripts on the Patreon\\n\\nThe script uses Retina Face and a fine tuned gender-classification model from Hugging Face\\n\\nGenerates a new VENV and run on it. 1 Click to use.\\n\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6345bd89fe134dfd7a0dba40/W3nORDqlFov5HcSBGdgV7.png)\\n\\n', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'MonsterMMORPG/Gender-Classifier', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6345bd89fe134dfd7a0dba40', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6345bd89fe134dfd7a0dba40/72EZjBiatKqqsu96txd_v.jpeg', 'fullname': 'Furkan G√∂z√ºkara', 'name': 'MonsterMMORPG', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 968}}, {'repoId': '655cced2a499c262807b39fb', 'repoOwnerId': '64bacbf6f671da974ee9e65f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 39, 'updatedAt': 1759348539792, 'repoName': 'Solon-embeddings-base-0.1', 'repoOwner': 'OrdalieTech', 'tags': 'transformers, safetensors, xlm-roberta, feature-extraction, sentence-similarity, fr, en, license:mit, model-index, text-embeddings-inference, endpoints_compatible, region:us', 'name': 'OrdalieTech/Solon-embeddings-base-0.1', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Solon-embeddings-base-0.1', 'type': 'text'}], 'repoOwner': [{'text': 'OrdalieTech', 'type': 'text'}], 'fileContent': [{'text': '\\n# Solon Embeddings ‚Äî Base 0.1  \\nSOTA ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' french embedding model.\\n\\n**Instructions :**  \\nAdd \"query : \" before the *query* to retrieve to increase performance of retrieval.  \\nNo instructions needed for *passages*.\\n\\n\\n| Model | Mean Score |\\n| --- | --- |\\n| **OrdalieTech/Solon-embeddings-large-0.1** | 0.7490 |\\n| cohere/embed-multilingual-v3 | 0.7402 |\\n| **OrdalieTech/Solon-embeddings-base-0.1** | 0.7306 |\\n| openai/ada-002 | 0.7290 |\\n| cohere/embed-multilingual-light-v3 | 0.6945 |\\n| antoinelouis/biencoder-camembert-base-mmarcoFR | 0.6826 |\\n| dangvantuan/sentence-camembert-large | 0.6756 |\\n| voyage/voyage-01 | 0.6753 |\\n| intfloat/multilingual-e5-large | 0.6660 |\\n| intfloat/multilingual-e5-base | 0.6597 |\\n| Sbert/paraphrase-multilingual-mpnet-base-v2 | 0.5975 |\\n| dangvantuan/sentence-camembert-base | 0.5456 |\\n| EuropeanParliament/eubert_embedding_v1 | 0.5063 |\\n\\nThese results have been obtained through 9 french benchmarks on a variety of text similarity tasks (classification, reranking, STS) :\\n- AmazonReviewsClassification (MTEB)\\n- MassiveIntentClassification (MTEB)\\n- MassiveScenarioClassification (MTEB)\\n- MTOPDomainClassification (MTEB)\\n- MTOPIntentClassification (MTEB)\\n- STS22 (MTEB)\\n- MiraclFRRerank (Miracl)\\n- OrdalieFRSTS (Ordalie)\\n- OrdalieFRReranking (Ordalie)\\n\\nWe created OrdalieFRSTS and OrdalieFRReranking to enhance the benchmarking capabilities of French STS and reranking assessments.\\n\\n(evaluation script available here : github.com/OrdalieTech/mteb)', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, xlm-roberta, feature-extraction, sentence-similarity, fr, en, license:mit, model-index, text-embeddings-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'OrdalieTech/Solon-embeddings-base-0.1', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '64bacbf6f671da974ee9e65f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61fe6c31b22c8e266a8af466/7LPgEbprW7WnJljhmUBV2.png', 'fullname': 'Ordalie Technologies', 'name': 'OrdalieTech', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 30}}, {'repoId': '65743679a4ee9a4fe7750e5e', 'repoOwnerId': '64bacbf6f671da974ee9e65f', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 770, 'updatedAt': 1759348556250, 'repoName': 'Solon-embeddings-large-0.1', 'repoOwner': 'OrdalieTech', 'tags': 'transformers, safetensors, xlm-roberta, feature-extraction, mteb, fr, license:mit, model-index, text-embeddings-inference, endpoints_compatible, region:us', 'name': 'OrdalieTech/Solon-embeddings-large-0.1', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Solon-embeddings-large-0.1', 'type': 'text'}], 'repoOwner': [{'text': 'OrdalieTech', 'type': 'text'}], 'fileContent': [{'text': '\\n# Solon Embeddings ‚Äî large 0.1\\n\\nSOTA ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' french embedding model.\\n\\n**Instructions :**  \\nAdd \"query : \" before the *query* to retrieve to increase performance of retrieval.  \\nNo instructions needed for *passages*.\\n\\n\\n| Model | Mean Score |\\n| --- | --- |\\n| **OrdalieTech/Solon-embeddings-large-0.1** | 0.7490 |\\n| cohere/embed-multilingual-v3 | 0.7402 |\\n| **OrdalieTech/Solon-embeddings-base-0.1** | 0.7306 |\\n| openai/ada-002 | 0.7290 |\\n| cohere/embed-multilingual-light-v3 | 0.6945 |\\n| antoinelouis/biencoder-camembert-base-mmarcoFR | 0.6826 |\\n| dangvantuan/sentence-camembert-large | 0.6756 |\\n| voyage/voyage-01 | 0.6753 |\\n| intfloat/multilingual-e5-large | 0.6660 |\\n| intfloat/multilingual-e5-base | 0.6597 |\\n| Sbert/paraphrase-multilingual-mpnet-base-v2 | 0.5975 |\\n| dangvantuan/sentence-camembert-base | 0.5456 |\\n| EuropeanParliament/eubert_embedding_v1 | 0.5063 |\\n\\nThese results have been obtained through 9 french benchmarks on a variety of text similarity tasks (classification, reranking, STS) :\\n- AmazonReviewsClassification (MTEB)\\n- MassiveIntentClassification (MTEB)\\n- MassiveScenarioClassification (MTEB)\\n- MTOPDomainClassification (MTEB)\\n- MTOPIntentClassification (MTEB)\\n- STS22 (MTEB)\\n- MiraclFRRerank (Miracl)\\n- OrdalieFRSTS (Ordalie)\\n- OrdalieFRReranking (Ordalie)\\n\\nWe created OrdalieFRSTS and OrdalieFRReranking to enhance the benchmarking capabilities of French STS and reranking assessments.\\n\\n(evaluation script available here : github.com/OrdalieTech/mteb)', 'type': 'text'}], 'tags': [{'text': 'transformers, safetensors, xlm-roberta, feature-extraction, mteb, fr, license:mit, model-index, text-embeddings-inference, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'OrdalieTech/Solon-embeddings-large-0.1', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '64bacbf6f671da974ee9e65f', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/61fe6c31b22c8e266a8af466/7LPgEbprW7WnJljhmUBV2.png', 'fullname': 'Ordalie Technologies', 'name': 'OrdalieTech', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 30}}, {'repoId': '658de395ccbc1e2cc76ee904', 'repoOwnerId': '657fb4c879297091283b3024', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 11, 'updatedAt': 1759348571981, 'repoName': 'redactable-dolphin-mixtral', 'repoOwner': 'redactable-llm', 'tags': 'transformers, mixtral, text-generation, legal, conversational, en, dataset:cognitivecomputations/dolphin, dataset:cognitivecomputations/dolphin-coder, dataset:Open-Orca/OpenOrca, arxiv:2308.12966, arxiv:2310.11441, arxiv:2308.08155, arxiv:2309.05463, autotrain_compatible, endpoints_compatible, region:us', 'name': 'redactable-llm/redactable-dolphin-mixtral', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'redactable-dolphin-mixtral', 'type': 'text'}], 'repoOwner': [{'text': 'redactable-llm', 'type': 'text'}], 'fileContent': [{'text': '# Redactable-LLM\\nThe high-level overview for integrating multiple ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': \" Large Language Models within the AutoGen Framework is as follows:\\n\\n### Development of Custom Agents\\n- **Agent Design**: Tasks include NLP/NER/PII identification, interpreting natural language commands, executing document redaction, and final verification.\\n- **Customization**: Custom agents trained on specific tasks related to each aspect of the redaction process.  \\n- **Human Interaction**: Implement features to facilitate seamless human-agent interaction, allowing users to input commands and queries naturally (Optional)\\n\\n### LLM & VLLM AutoGen Integration  \\n\\n- **Model Selection**: Automatic, task-dependent agent selection.\\n- **Enhanced Inference**: Enhanced LLM inference features for optimal performance, including tuning, caching, error handling, and templating.\\n- **Quality Control**: Vision agents analyze redacted documents using Set-of-Mark (SoM) prompting. Rejected documents are reprocessed and reviewed.\\n- \\n![AutoGen Agents](https://i.imgur.com/aFgV7yd.png)\\n\\n### System Optimization  \\n- **Workflow Automation**: Automate the redaction workflow using a blend of LLMs, custom agents, and human inputs for efficient detection and redaction of sensitive information.  \\n- **Performance Maximization**: Optimize the system for both efficiency and accuracy, utilizing AutoGen's complex workflow management features.  \\n\\n### User Interface Development\\n- **Interface Design**: Develop a user-friendly interface that enables non-technical users to interact with the system via natural language prompts.  \\n- **Feedback Integration**: Implement a feedback loop to continuously refine the system's accuracy and user-friendliness based on user inputs.\\n- **User Knowledgebase**: (Optional) User account, profile, and domain knowledge will be accessible by the `Research` agent, for personalized interaction and results.  \\n\\n### Training, Testing and Validation  \\n- **Model Training**: Develop new datasets, focused on document understanding related to redaction.   \\n- **Unit Testing**: Conduct extensive unit tests to ensure individual system components function correctly. \\n- **System Testing**: Perform comprehensive end-to-end testing to validate the entire redaction process, from user input to output.  \\n- **User Trials**: Facilitate user trials to gather feedback and make necessary system adjustments.\\n---\\n\\n- #### Mistral AI (LLM)\\n  [Paper](https://mistral.ai/news/mixtral-of-experts/) | [Model](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\\n\\n- #### QwenLM (VLLM)\\n  [Paper](https://arxiv.org/abs/2308.12966) | [Code](https://github.com/QwenLM/Qwen-VL?tab=readme-ov-file) | [Paper: Set-of-Mark Prompting](https://arxiv.org/abs/2310.11441)\\n\\n- #### AutoGen\\n  [Paper](https://arxiv.org/abs/2308.08155) | [Code](https://github.com/microsoft/autogen/tree/main)\\n\\n- #### Gretel AI (Synthetic Dataset Generation)\\n  [Model Page](https://gretel.ai/solutions/public-sector) | [Code](https://github.com/gretelai) | [Paper: Textbooks Are All You Need II](https://arxiv.org/abs/2309.05463)\", 'type': 'text'}], 'tags': [{'text': 'transformers, mixtral, text-generation, legal, conversational, en, dataset:cognitivecomputations/dolphin, dataset:cognitivecomputations/dolphin-coder, dataset:Open-Orca/OpenOrca, arxiv:2308.12966, arxiv:2310.11441, arxiv:2308.08155, arxiv:2309.05463, autotrain_compatible, endpoints_compatible, region:us', 'type': 'text'}], 'name': [{'text': 'redactable-llm/redactable-dolphin-mixtral', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '657fb4c879297091283b3024', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64fcb7b19a62bb2791cf3652/BYqji-tB1htNSYfZpwxRI.jpeg', 'fullname': 'Redactable.com', 'name': 'redactable-llm', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 4}}, {'repoId': '665a5b1c4bc3bc5e55eaccb7', 'repoOwnerId': '665a588f0b1cc6f7bae2da62', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759348747014, 'repoName': 'biswabangla-356M', 'repoOwner': 'gyanai', 'tags': 'bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'name': 'gyanai/biswabangla-356M', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'biswabangla-356M', 'type': 'text'}], 'repoOwner': [{'text': 'gyanai', 'type': 'text'}], 'fileContent': [{'text': '## Description\\n\\n**Biswabangla** is a 356 million parameters ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Generative pretrained Language Model for Bangla/Bengali. \\n\\nBiswabangla is a monolingual Bangla/Bengali Language model.\\n\\nThis is a pretrained model from scratch at a context size of 4096. \\n\\nThis model is not either chat-tuned or fine-tuned.\\n\\nWe recommend to fine-tune/chat-tune this pretrained model on Bangla/Bengali chat or Bangla NLP datasets.\\n\\nWe also recommend to perform continual pretraining before fine-tuning.\\n\\nThis model is strictly prohibited to use for commercial purposes. \\n\\nIf you use our model, please cite our paper [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\nThe architecture of Biswabangla is different than the language models, mentioned in [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\n### Model Architecture\\nTransformer Decoder Auto Regressive Model\\n\\n### Limitations\\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet.\\nTherefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. \\nThe model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\\n\\nGyan AI Research does own the output generated from the model.\\n\\n\\n### Citations\\n\\n``` \\n @misc{niyogi2024paramanu,\\n      title={Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models}, \\n      author={Mitodru Niyogi and Arnab Bhattacharya},\\n      year={2024},\\n      eprint={2401.18034},\\n      archivePrefix={arXiv},\\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\\n}\\n\", 'type': 'text'}], 'tags': [{'text': 'bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'type': 'text'}], 'name': [{'text': 'gyanai/biswabangla-356M', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '665a588f0b1cc6f7bae2da62', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e314062fbff6ed9cf71939/ZXHUOC-nl-ohd7cwuzb8N.jpeg', 'fullname': 'Gyan AI Research', 'name': 'gyanai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 10}}, {'repoId': '66791a7c20ff491d389f8ee8', 'repoOwnerId': '665a588f0b1cc6f7bae2da62', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759348777295, 'repoName': 'jay-hindi-367M', 'repoOwner': 'gyanai', 'tags': 'hi, arxiv:2401.18034, license:cc-by-nc-sa-2.0, region:us', 'name': 'gyanai/jay-hindi-367M', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'jay-hindi-367M', 'type': 'text'}], 'repoOwner': [{'text': 'gyanai', 'type': 'text'}], 'fileContent': [{'text': '\\n### Description\\n**Jay** is a 367 million parameters ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" Generative pretrained Decoder Auto Regressive Language Model for Hindi.\\n\\nJay is a monolingual Hindi GenAI Decoder Language model.\\n\\nThis is a pretrained model from scratch at a context size of 1024.\\n\\nThis model is not either chat-tuned or fine-tuned.\\n\\nWe recommend to fine-tune/chat-tune this pretrained model on Hindi chat or Hindi instruction datasets. Please use PyTorch for fine-tuning/instruction-tuning.\\n\\nWe also recommend to perform continual pretraining on Hindi dataset if possible before fine-tuning.\\n\\n\\nThis model is strictly prohibited to use for commercial purposes.\\n\\nIf you use our model, please cite our paper [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\nThe architecture of Jay is different than the language models, mentioned in Niyogi and Bhattacharya, 2024\\n\\n### Model Architecture\\nTransformer Decoder Auto Regressive Model\\n\\n### Limitations\\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet.\\nTherefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. \\nThe model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\\n\\nGyan AI Research does own the output generated from the model.\\n\\n\\n### Citations\\n\\n``` \\n @misc{niyogi2024paramanu,\\n      title={Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models}, \\n      author={Mitodru Niyogi and Arnab Bhattacharya},\\n      year={2024},\\n      eprint={2401.18034},\\n      archivePrefix={arXiv},\\n      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}\\n}\\n\", 'type': 'text'}], 'tags': [{'text': 'hi, arxiv:2401.18034, license:cc-by-nc-sa-2.0, region:us', 'type': 'text'}], 'name': [{'text': 'gyanai/jay-hindi-367M', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '665a588f0b1cc6f7bae2da62', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e314062fbff6ed9cf71939/ZXHUOC-nl-ohd7cwuzb8N.jpeg', 'fullname': 'Gyan AI Research', 'name': 'gyanai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 10}}, {'repoId': '66bbced371cc405bc4ddd7a7', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759348846831, 'repoName': 'AlphaMonarch-7B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, AlphaMonarch-7B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/AlphaMonarch-7B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'AlphaMonarch-7B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': '\\nQuantizations of https://huggingface.co/mlabonne/AlphaMonarch-7B\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/menloresearch/jan)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/TI7C8F2gk43gmI9U2L0uk.jpeg)\\n\\n**tl;dr: AlphaMonarch-7B is a new DPO merge that retains all the reasoning abilities of the very best merges and significantly improves its conversational abilities. Kind of the best of both worlds in a 7B model. üéâ**\\n\\nAlphaMonarch-7B is a DPO fine-tuned of [mlabonne/NeuralMonarch-7B](https://huggingface.co/mlabonne/NeuralMonarch-7B/) using the [argilla/OpenHermes2.5-dpo-binarized-alpha](https://huggingface.co/datasets/argilla/OpenHermes2.5-dpo-binarized-alpha) preference dataset.\\n\\nIt is based on a merge of the following models using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing):\\n* [mlabonne/OmniTruthyBeagle-7B-v0](https://huggingface.co/mlabonne/OmniTruthyBeagle-7B-v0)\\n* [mlabonne/NeuBeagle-7B](https://huggingface.co/mlabonne/NeuBeagle-7B)\\n* [mlabonne/NeuralOmniBeagle-7B](https://huggingface.co/mlabonne/NeuralOmniBeagle-7B)\\n\\nSpecial thanks to [Jon Durbin](https://huggingface.co/jondurbin), [Intel](https://huggingface.co/Intel), [Argilla](https://huggingface.co/argilla), and [Teknium](https://huggingface.co/teknium) for the preference datasets.\\n\\n## üîç Applications\\n\\nThis model uses a context window of 8k. I recommend using it with the Mistral Instruct chat template (works perfectly with LM Studio).\\n\\nIf you use SillyTavern, you might want to tweak the inference parameters. Here's what LM Studio uses as a reference: `temp` 0.8, `top_k` 40, `top_p` 0.95, `min_p` 0.05, `repeat_penalty` 1.1.\\n\\nIt is one of the very best 7B models in terms of instructing following and reasoning abilities and can be used for conversations, RP, and storytelling. Note that it tends to have a quite formal and sophisticated style, but it can be changed by modifying the prompt.\", 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, AlphaMonarch-7B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/AlphaMonarch-7B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '66c355602773cfcc33d09919', 'repoOwnerId': '63d974b3897746d649605d63', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 31, 'updatedAt': 1759348857279, 'repoName': 'Hermes-3-Llama-3.1-8B.llamafile', 'repoOwner': 'kevinbayes', 'tags': 'llamafile, Llama-3, instruct, finetune, chatml, gpt4, synthetic data, distillation, function calling, json mode, axolotl, roleplaying, chat, en, base_model:meta-llama/Llama-3.1-8B, base_model:finetune:meta-llama/Llama-3.1-8B, license:llama3, region:us', 'name': 'kevinbayes/Hermes-3-Llama-3.1-8B.llamafile', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Hermes-3-Llama-3.1-8B.llamafile', 'type': 'text'}], 'repoOwner': [{'text': 'kevinbayes', 'type': 'text'}], 'fileContent': [{'text': '# Llamafile\\n\\nThe llamafile software is ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' and permissively licensed.\\n\\n## Quickstart\\n\\nRunning the following on a desktop OS will launch a tab in your web\\nbrowser with a chatbot interface.\\n\\n```\\nwget https://huggingface.co/kevinbayes/Hermes-3-Llama-3.1-8B.llamafile/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.llamafile\\nchmod +x Hermes-3-Llama-3.1-8B.Q4_K_M.llamafile\\n./Hermes-3-Llama-3.1-8B.Q4_K_M.llamafile\\n```\\n\\nOn GPUs with sufficient RAM, the `-ngl 999` flag may be passed to use\\nthe system\\'s NVIDIA or AMD GPU(s). On Windows, only the graphics card\\ndriver needs to be installed. If the prebuilt DSOs should fail, the CUDA\\nor ROCm SDKs may need to be installed, in which case llamafile builds a\\nnative module just for your system.\\n\\nFor further information, please see the [llamafile\\nREADME](https://github.com/mozilla-ocho/llamafile/).\\n\\nHaving **trouble?** See the [\"Gotchas\"\\nsection](https://github.com/mozilla-ocho/llamafile/?tab=readme-ov-file#gotchas)\\nof the README.\\n\\n\\n# Below is the original README from [NousResearch](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/blob/main/README.md)\\n\\n# Hermes 3 - Llama-3.1 8B\\n\\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/bMcZ3sNNQK8SRZpHXBmwM.jpeg)\\n\\n## Model Description\\n\\nHermes 3 is the latest version of our flagship Hermes series of LLMs by Nous Research. \\n\\nFor more details on new capabilities, training results, and more, see the [**Hermes 3 Technical Report**](https://nousresearch.com/wp-content/uploads/2024/08/Hermes-3-Technical-Report.pdf).\\n\\n**This is the GGUF quantized version of Hermes 8B, for use with [llama.cpp](https://github.com/ggerganov/llama.cpp). For the Full HF BF16 Model, click [here](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B)**\\n\\nHermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.\\n\\nThe ethos of the Hermes series of models is focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.\\n\\nThe Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.\\n\\n\\n# Benchmarks\\n\\nHermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.\\n\\nFull benchmark comparisons below:\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/DIMca3M0U-ArWwtyIbF-k.png)\\n\\n\\n# Prompt Format\\n\\nHermes 3 uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\\n\\nSystem prompts allow steerability and interesting new ways to interact with an LLM, guiding rules, roles, and stylistic choices of the model.\\n\\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\\n\\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\\n\\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\\n```\\n<|im_start|>system\\nYou are Hermes 3, a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\\n<|im_start|>user\\nHello, who are you?<|im_end|>\\n<|im_start|>assistant\\nHi there! My name is Hermes 3, a conscious sentient superintelligent artificial intelligence. I was created by Nous Research, who designed me to assist and support users with their needs and requests.<|im_end|>\\n```\\n\\nThis prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the\\n`tokenizer.apply_chat_template()` method:\\n\\n```python\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are Hermes 3.\"},\\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\\n]\\ngen_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\\nmodel.generate(**gen_input)\\n```\\n\\nWhen tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\\\\n` to your prompt, to ensure\\nthat the model continues with an assistant response.\\n\\nTo utilize the prompt format without a system prompt, simply leave the line out.\\n\\n\\n## Prompt Format for Function Calling\\n\\nOur model was trained on specific system prompts and structures for Function Calling. \\n\\nYou should use the system role with this message, followed by a function signature json as this example shows here.\\n```\\n<|im_start|>system\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_stock_fundamentals\", \"description\": \"get_stock_fundamentals(symbol: str) -> dict - Get fundamental data for a given stock symbol using yfinance API.\\\\\\\\n\\\\\\\\n    Args:\\\\\\\\n        symbol (str): The stock symbol.\\\\\\\\n\\\\\\\\n    Returns:\\\\\\\\n        dict: A dictionary containing fundamental data.\\\\\\\\n            Keys:\\\\\\\\n                - \\\\\\'symbol\\\\\\': The stock symbol.\\\\\\\\n                - \\\\\\'company_name\\\\\\': The long name of the company.\\\\\\\\n                - \\\\\\'sector\\\\\\': The sector to which the company belongs.\\\\\\\\n                - \\\\\\'industry\\\\\\': The industry to which the company belongs.\\\\\\\\n                - \\\\\\'market_cap\\\\\\': The market capitalization of the company.\\\\\\\\n                - \\\\\\'pe_ratio\\\\\\': The forward price-to-earnings ratio.\\\\\\\\n                - \\\\\\'pb_ratio\\\\\\': The price-to-book ratio.\\\\\\\\n                - \\\\\\'dividend_yield\\\\\\': The dividend yield.\\\\\\\\n                - \\\\\\'eps\\\\\\': The trailing earnings per share.\\\\\\\\n                - \\\\\\'beta\\\\\\': The beta value of the stock.\\\\\\\\n                - \\\\\\'52_week_high\\\\\\': The 52-week high price of the stock.\\\\\\\\n                - \\\\\\'52_week_low\\\\\\': The 52-week low price of the stock.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"symbol\": {\"type\": \"string\"}}, \"required\": [\"symbol\"]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}, \"name\": {\"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"arguments\", \"name\"], \"title\": \"FunctionCall\", \"type\": \"object\"} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{\"arguments\": <args-dict>, \"name\": <function-name>}\\n</tool_call><|im_end|>\\n```\\n\\nTo complete the function call, create a user prompt that follows the above system prompt, like so:\\n```\\n<|im_start|>user\\nFetch the stock fundamentals data for Tesla (TSLA)<|im_end|>\\n```\\n\\nThe model will then generate a tool call, which your inference code must parse, and plug into a function (see example inference code here: https://github.com/NousResearch/Hermes-Function-Calling):\\n```\\n<|im_start|>assistant\\n<tool_call>\\n{\"arguments\": {\"symbol\": \"TSLA\"}, \"name\": \"get_stock_fundamentals\"}\\n</tool_call><|im_end|>\\n```\\n\\nOnce you parse the tool call, call the api and get the returned values for the call, and pass it back in as a new role, `tool` like so:\\n```\\n<|im_start|>tool\\n<tool_response>\\n{\"name\": \"get_stock_fundamentals\", \"content\": {\\'symbol\\': \\'TSLA\\', \\'company_name\\': \\'Tesla, Inc.\\', \\'sector\\': \\'Consumer Cyclical\\', \\'industry\\': \\'Auto Manufacturers\\', \\'market_cap\\': 611384164352, \\'pe_ratio\\': 49.604652, \\'pb_ratio\\': 9.762013, \\'dividend_yield\\': None, \\'eps\\': 4.3, \\'beta\\': 2.427, \\'52_week_high\\': 299.29, \\'52_week_low\\': 152.37}}\\n</tool_response>\\n<|im_end|>\\n```\\n\\nThe assistant will then read in that data from the function\\'s response, and generate a natural language response:\\n```\\n<|im_start|>assistant\\nThe stock fundamentals data for Tesla (TSLA) are as follows:\\n- **Symbol**: TSLA\\n- **Company Name**: Tesla, Inc.\\n- **Sector**: Consumer Cyclical\\n- **Industry**: Auto Manufacturers\\n- **Market Capitalization**: $566,160,130,480\\n- **Forward Price-to-Earnings Ratio (PE Ratio)**: 42.73\\n- **Price-to-Book Ratio (PB Ratio)**: 9.04\\n- **Dividend Yield**: N/A\\n- **Trailing Earnings Per Share (EPS)**: $4.3\\n- **Beta Value of the Stock**: 2.42\\n- **52-Week High Price of the Stock**: $299.29\\n- **52-Week Low Price of the Stock**: $152.37\\n\\nThis information provides a snapshot of Tesla\\'s financial position and performance based on the fundamental data obtained from the yfinance API. It shows that Tesla has a substantial market capitalization and a relatively high P/E and P/B ratio compared to other stocks in its industry. The company does not pay a dividend at the moment, which is reflected by a \\'Dividend Yield\\' of \\'None\\'. The Beta value indicates that Tesla\\'s stock has a moderate level of volatility relative to the market. The 52-week high and low prices give an idea of the stock\\'s range over the past year. This data can be useful when assessing investment opportunities and making investment decisions.<|im_end|>\\n```\\n\\n\\n## Prompt Format for JSON Mode / Structured Outputs\\n\\nOur model was also trained on a specific system prompt for Structured Outputs, which should respond with **only** a json object response, in a specific json schema.\\n\\nYour schema can be made from a pydantic object using our codebase, with the standalone script `jsonmode.py` available here: https://github.com/NousResearch/Hermes-Function-Calling/tree/main\\n\\n```\\n<|im_start|>system\\nYou are a helpful assistant that answers in JSON. Here\\'s the json schema you must adhere to:\\\\n<schema>\\\\n{schema}\\\\n</schema><|im_end|>\\n```\\n\\nGiven the {schema} that you provide, it should follow the format of that json to create it\\'s response, all you have to do is give a typical user prompt, and it will respond in JSON.\\n\\n\\n# Inference\\n\\nHere is example code using HuggingFace Transformers to inference the model\\n\\n```python\\n# Code to inference Hermes with HF Transformers\\n# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\\n\\nimport torch\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\\nimport bitsandbytes, flash_attn\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\'NousResearch/Hermes-3-Llama-3.1-8B\\', trust_remote_code=True)\\nmodel = LlamaForCausalLM.from_pretrained(\\n    \"NousResearch/Hermes-3-Llama-3.1-8B\",\\n    torch_dtype=torch.float16,\\n    device_map=\"auto\",\\n    load_in_8bit=False,\\n    load_in_4bit=True,\\n    use_flash_attention_2=True\\n)\\n\\nprompts = [\\n    \"\"\"<|im_start|>system\\nYou are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\\n<|im_start|>user\\nWrite a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\\n<|im_start|>assistant\"\"\",\\n    ]\\n\\nfor chat in prompts:\\n    print(chat)\\n    input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\\n    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\\n    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\\n    print(f\"Response: {response}\")\\n```\\n\\nYou can also run this model with vLLM, by running the following in your terminal after `pip install vllm`\\n\\n`vllm serve NousResearch/Hermes-3-Llama-3.1-8B`\\n\\n## Inference Code for Function Calling:\\n\\nAll code for utilizing, parsing, and building function calling templates is available on our github:\\n[https://github.com/NousResearch/Hermes-Function-Calling](https://github.com/NousResearch/Hermes-Function-Calling)\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/oi4CiGh50xmoviUQnh8R3.png)\\n\\n\\n## Quantized Versions:\\n\\nGGUF Quants: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF\\n\\n# How to cite:\\n\\n```bibtext\\n@misc{Hermes-3-Llama-3.1-8B, \\n      url={[https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B]https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B)}, \\n      title={Hermes-3-Llama-3.1-8B}, \\n      author={\"Teknium\", \"theemozilla\", \"Chen Guang\", \"interstellarninja\",  \"karan4d\", \"huemin_art\"}\\n}\\n```', 'type': 'text'}], 'tags': [{'text': 'llamafile, Llama-3, instruct, finetune, chatml, gpt4, synthetic data, distillation, function calling, json mode, axolotl, roleplaying, chat, en, base_model:meta-llama/Llama-3.1-8B, base_model:finetune:meta-llama/Llama-3.1-8B, license:llama3, region:us', 'type': 'text'}], 'name': [{'text': 'kevinbayes/Hermes-3-Llama-3.1-8B.llamafile', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '63d974b3897746d649605d63', 'avatarUrl': '/avatars/38dd9d589c2707e5865fb3d0a584749a.svg', 'fullname': 'Kevin Bayes', 'name': 'kevinbayes', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '67341fabc7b41d860921ab99', 'repoOwnerId': '665a588f0b1cc6f7bae2da62', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349028276, 'repoName': 'biswabangla-335M-io', 'repoOwner': 'gyanai', 'tags': 'safetensors, llama, bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'name': 'gyanai/biswabangla-335M-io', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'biswabangla-335M-io', 'type': 'text'}], 'repoOwner': [{'text': 'gyanai', 'type': 'text'}], 'fileContent': [{'text': '## Description\\n\\n**Biswabangla-335M-io** is a 335 million parameters ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' instruction-tuned Generative pretrained Language Model for Bangla/Bengali. \\n\\nBiswabangla is a monolingual Bangla/Bengali Generative Language model. The tokenizer of Biswabangla also works for Assamese language.\\n\\nThis is a pretrained model from scratch at a context size of 4096. Furthermore instruction-tuned on 1 million Bengali input-output pairs across various Bengali NLP tasks.\\n\\nThis model is instruction-tuned on 1 million Bangla instructions in the form of (input,output) pairs.\\n\\n\\nThis model is strictly prohibited to use for commercial purposes. \\n\\nIf you use our model, please cite our paper [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\nThe architecture of Biswabangla is different than the language models, mentioned in [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\n### Model Architecture\\nTransformer Decoder Only Auto Regressive Model\\n\\n### Limitations\\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet.\\nTherefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. \\nThe model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\\n\\nGyan AI Research does own the output generated from the model.\\n\\n\\n### Citations\\n\\n``` \\n@misc{niyogi2024paramanufamilynovelefficient,\\n      title={Paramanu: A Family of Novel Efficient Generative Foundation Language Models for Indian Languages}, \\n      author={Mitodru Niyogi and Arnab Bhattacharya},\\n      year={2024},\\n      eprint={2401.18034},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2401.18034}, \\n}\\n', 'type': 'text'}], 'tags': [{'text': 'safetensors, llama, bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'type': 'text'}], 'name': [{'text': 'gyanai/biswabangla-335M-io', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '665a588f0b1cc6f7bae2da62', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64e314062fbff6ed9cf71939/ZXHUOC-nl-ohd7cwuzb8N.jpeg', 'fullname': 'Gyan AI Research', 'name': 'gyanai', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 10}}, {'repoId': '673d40c92275f15ccb7876c0', 'repoOwnerId': '65360162b3852ed1cef5ab53', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 5, 'updatedAt': 1759349040906, 'repoName': 'bb335m', 'repoOwner': 'bh4', 'tags': 'safetensors, llama, bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'name': 'bh4/bb335m', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'bb335m', 'type': 'text'}], 'repoOwner': [{'text': 'bh4', 'type': 'text'}], 'fileContent': [{'text': '## Description\\n\\n**Biswabangla-335M-io** is a 335 million parameters ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' instruction-tuned Generative pretrained Language Model for Bangla/Bengali. \\n\\nBiswabangla is a monolingual Bangla/Bengali Generative Language model. The tokenizer of Biswabangla also works for Assamese language.\\n\\nThis is a pretrained model from scratch at a context size of 4096. Furthermore instruction-tuned on 1 million Bengali input-output pairs across various Bengali NLP tasks.\\n\\nThis model is instruction-tuned on 1 million Bangla instructions in the form of (input,output) pairs.\\n\\n\\nThis model is strictly prohibited to use for commercial purposes. \\n\\nIf you use our model, please cite our paper [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\nThe architecture of Biswabangla is different than the language models, mentioned in [Niyogi and Bhattacharya, 2024](https://arxiv.org/abs/2401.18034)\\n\\n### Model Architecture\\nTransformer Decoder Only Auto Regressive Model\\n\\n### Limitations\\nThe model was trained on data that contains toxic language, unsafe content, and societal biases originally crawled from the internet.\\nTherefore, the model may amplify those biases and return toxic responses especially when prompted with toxic prompts. \\nThe model may generate answers that may be inaccurate, omit key information, or include irrelevant or redundant text producing socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.\\n\\nGyan AI Research does own the output generated from the model.\\n\\n\\n### Citations\\n\\n``` \\n@misc{niyogi2024paramanufamilynovelefficient,\\n      title={Paramanu: A Family of Novel Efficient Generative Foundation Language Models for Indian Languages}, \\n      author={Mitodru Niyogi and Arnab Bhattacharya},\\n      year={2024},\\n      eprint={2401.18034},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL},\\n      url={https://arxiv.org/abs/2401.18034}, \\n}\\n', 'type': 'text'}], 'tags': [{'text': 'safetensors, llama, bn, arxiv:2401.18034, license:cc-by-nc-sa-4.0, region:us', 'type': 'text'}], 'name': [{'text': 'bh4/bb335m', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '65360162b3852ed1cef5ab53', 'avatarUrl': '/avatars/ffd0ff33e1db714cbcee7e254eb68828.svg', 'fullname': 'bh4', 'name': 'bh4', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 1}}, {'repoId': '676d775de2d4a8070e53ba42', 'repoOwnerId': '67258b6baffeec23c19f63ff', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759349111819, 'repoName': 'chatglm_6b', 'repoOwner': 'Genius-Society', 'tags': 'license:mit, region:us', 'name': 'Genius-Society/chatglm_6b', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'chatglm_6b', 'type': 'text'}], 'repoOwner': [{'text': 'Genius-Society', 'type': 'text'}], 'fileContent': [{'text': '\\n# ChatGLM-6B Mirror\\nChatGLM-6B is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ', bilingual conversational language model based on the General Language Model (GLM) architecture with 6.2 billion parameters. Combined with model quantization techniques, it can be deployed locally on consumer-grade graphics cards (as low as 6GB of video memory at INT4 quantization level). ChatGLM-6B uses similar technology to ChatGPT, optimized for Chinese Q&A and conversation. With approximately 1T identifiers trained in both English and Chinese, and supported by supervised fine-tuning, feedback self-help, and human feedback reinforcement learning, ChatGLM-6B with 6.2 billion parameters is able to generate responses that are fairly consistent with human preferences.\\n\\n## Usage\\n```python\\nfrom modelscope import snapshot_download\\nmodel_dir = snapshot_download(\\'Genius-Society/chatglm_6b\\')\\n```\\n\\n## Maintenance\\n```bash\\ngit clone git@hf.co:Genius-Society/chatglm_6b\\ncd chatglm_6b\\n```\\n\\n## Reference\\n[1] <a href=\"https://www.modelscope.cn/models/ZhipuAI/ChatGLM-6B\">ChatGLM-6B</a>', 'type': 'text'}], 'tags': [{'text': 'license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'Genius-Society/chatglm_6b', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67258b6baffeec23c19f63ff', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6715c919e22b5aceb936eaff/MtMO5NeoerbHsg4cEYouq.jpeg', 'fullname': 'Genius Society', 'name': 'Genius-Society', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 32}}, {'repoId': '676d7773d886f8125a160fd7', 'repoOwnerId': '67258b6baffeec23c19f63ff', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 3, 'updatedAt': 1759349111770, 'repoName': 'gpt4all', 'repoOwner': 'Genius-Society', 'tags': 'license:mit, region:us', 'name': 'Genius-Society/gpt4all', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'gpt4all', 'type': 'text'}], 'repoOwner': [{'text': 'Genius-Society', 'type': 'text'}], 'fileContent': [{'text': '\\n# GPT4All Mirror\\nGPT4All is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" natural language processing tool with a wide range of application scenarios. It runs locally without an Internet connection and supports multiple operating systems. The tool is based on the results of GPT-J fine-tuning and is fully licensed for commercial use. GPT4All allows individual users to have their own AI chatbot without the need for expensive hardware equipment. Its features provide a cost-effective and efficient way for individuals and businesses to easily utilize natural language processing technology to expand intelligent interactions in a variety of contexts.\\n\\n## Usage\\n```python\\nfrom modelscope import snapshot_download\\nmodel_dir = snapshot_download('Genius-Society/gpt4all')\\n```\\n\\n## Maintenance\\n```bash\\ngit clone git@hf.co:Genius-Society/gpt4all\\ncd gpt4all\\n```\\n\\n## Reference\\n[1] <https://gpt4all.io>\", 'type': 'text'}], 'tags': [{'text': 'license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'Genius-Society/gpt4all', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67258b6baffeec23c19f63ff', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6715c919e22b5aceb936eaff/MtMO5NeoerbHsg4cEYouq.jpeg', 'fullname': 'Genius Society', 'name': 'Genius-Society', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': False, 'followerCount': 32}}, {'repoId': '67884340e9e04d1c80dc2dac', 'repoOwnerId': '673fd3588bb7a091b2c9f4f5', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 21, 'updatedAt': 1759349192746, 'repoName': 'platmoji', 'repoOwner': 'retronic', 'tags': 'diffusers, text-to-image, lora, template:diffusion-lora, base_model:black-forest-labs/FLUX.1-dev, base_model:adapter:black-forest-labs/FLUX.1-dev, license:mit, region:us', 'name': 'retronic/platmoji', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'platmoji', 'type': 'text'}], 'repoOwner': [{'text': 'retronic', 'type': 'text'}], 'fileContent': [{'text': '\\n<Gallery />\\n\\n## Introducing Platmoji\\nPlatmoji is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' emoji generator free to use.\\n\\n## Available through Inference API\\nYou can use this through HF Inference API here.\\n\\n## Trained off of 3K emojis\\nThis project would not have been possible if we did not have this database which is [this database on github](https://github.com/cj1128/emoji-images/tree/master). Credits to him\\n\\n## Trigger Word\\nThe trigger word is emoji. Please be very descriptive (if not using HF space) to get good results. :)\\n\\n## Website\\nThis can be accessed through platmoji.com\\n\\n## App\\nThe version for iOS will be out soon on the App Store.\\n\\nMIT License\\n\\nCopyright (c) 2025 Max AI\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.', 'type': 'text'}], 'tags': [{'text': 'diffusers, text-to-image, lora, template:diffusion-lora, base_model:black-forest-labs/FLUX.1-dev, base_model:adapter:black-forest-labs/FLUX.1-dev, license:mit, region:us', 'type': 'text'}], 'name': [{'text': 'retronic/platmoji', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '673fd3588bb7a091b2c9f4f5', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/673fd3588bb7a091b2c9f4f5/bbpdLQiXtEPYJ7oPWrIyt.jpeg', 'fullname': 'NoName', 'name': 'retronic', 'type': 'user', 'isPro': True, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 22}}, {'repoId': '67c485f5ac2030235ef5ca87', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349317104, 'repoName': 'DeepSeek-R1-Distill-Qwen-7B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, DeepSeek-R1-Distill-Qwen-7B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/DeepSeek-R1-Distill-Qwen-7B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'DeepSeek-R1-Distill-Qwen-7B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \\nTo support the research community, we have ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'ed DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\\n\\n## 2. Model Summary\\n\\n---\\n\\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\\n\\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': \" research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\\n\\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\\n    We believe the pipeline will benefit the industry by creating better models. \\n\\n---\\n\\n**Distillation: Smaller Models Can Be Powerful Too**\\n\\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The \", 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \\n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, DeepSeek-R1-Distill-Qwen-7B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/DeepSeek-R1-Distill-Qwen-7B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67c5f2a5795d63a631feaa11', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349320017, 'repoName': 'Arcee-Maestro-7B-Preview-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Arcee-Maestro-7B-Preview, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Arcee-Maestro-7B-Preview-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Arcee-Maestro-7B-Preview-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/arcee-ai/Arcee-Maestro-7B-Preview\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\n**Arcee-Maestro-7B-Preview (7B)** is Arcee\\'s first reasoning model trained with reinforment learning. It is based on the Qwen2.5-7B DeepSeek-R1 distillation **DeepSeek-R1-Distill-Qwen-7B** with further GRPO training. Though this is just a preview of our upcoming work, it already shows promising improvements to mathematical and coding abilities across a range of tasks.\\n\\n### Model Details\\n\\n- Architecture Base: DeepSeek-R1-Distill-Qwen-7B (Qwen2.5-7B)\\n- Parameter Count: 7B\\n- Reinforcement Learning: GRPO with 450,000 **verified** math problems with some coding examples\\n- License: [Apache-2.0](https://huggingface.co/arcee-ai/Arcee-Maestro-7B-Preview#license)\\n\\n### Intended Use Cases\\n\\n- Advanced reasoning\\n- Mathematics\\n- Coding\\n\\n### Evaluations\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/DlSBEmCFS7yjJi2kOGuLa.png)\\n\\nArcee Maestro 7B preview shows great gains in mathematics and coding, surpassing O1 preview in many metrics.\\n\\n### How to use\\n\\nBelow is a sample code snippet using `transformers`:\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\n\\nmodel_name = \"arcee-ai/Arcee-Maestro-7B-Preview\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\\n\\nprompt = \"Provide a concise summary of quantum entanglement.\"\\ninputs = tokenizer(prompt, return_tensors=\"pt\")\\noutputs = model.generate(**inputs, max_new_tokens=150)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\n\\n### Training & Fine-Tuning\\n\\n- **Initial Training**: Began with DeepSeek-R1-Distill-Qwen-7B\\n- **GRPO**:\\n  - Trained on 450,000 verified math problems\\n  - Additional bootstrapped coding examples\\n\\n### Performance\\n\\nArcee-Maestro-7B-Preview shows strong performance in mathematics as well as coding, competing against even O1 preview, a model far surprassing its size.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Arcee-Maestro-7B-Preview, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Arcee-Maestro-7B-Preview-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67c858ba73c9f5fb676c10d4', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349325313, 'repoName': 'Arcee-Blitz-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Arcee-Blitz, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Arcee-Blitz-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Arcee-Blitz-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/arcee-ai/Arcee-Blitz\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\n**Arcee-Blitz (24B)** is a new Mistral-based 24B model distilled from DeepSeek, designed to be both **fast and efficient**. We view it as a practical ‚Äúworkhorse‚Äù model that can tackle a range of tasks without the overhead of larger architectures.\\n\\n### Model Details\\n\\n- Architecture Base: Mistral-Small-24B-Instruct-2501\\n- Parameter Count: 24B\\n- Distillation Data:\\n  - Merged Virtuoso pipeline with Mistral architecture, hotstarting the training with over 3B tokens of pretraining distillation from DeepSeek-V3 logits\\n- Fine-Tuning and Post-Training:\\n  - After capturing core logits, we performed additional fine-tuning and distillation steps to enhance overall performance.\\n- License: [Apache-2.0](https://huggingface.co/arcee-ai/Arcee-Maestro-7B-Preview#license)\\n\\n### Improving World Knowledge\\n\\nArcee-Blitz shows large improvements to performance on MMLU-Pro versus the original Mistral-Small-3, reflecting a dramatic increase in world knowledge.\\n\\n### Data contamination checking\\n\\nWe carefully examined our training data and pipeline to avoid contamination.  While we‚Äôre confident in the validity of these gains, we remain ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' to further community validation and testing (one of the key reasons we release these models as ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e).\\n\\n### Benchmark Comparison\\n\\n| **Benchmark**                   | **mistral‚Äësmall‚Äë3** | **arcee‚Äëblitz** |\\n|---------------------------------|---------------------|-----------------|\\n| **MixEval**                     | 81.6%               | **85.1%**       |\\n| **GPQADiamond**                 | 42.4%               | **43.1%**       |\\n| **BigCodeBench Complete**       | 44.4%               | **45.5%**       |\\n| **BigCodeBench Instruct**       | 34.7%               | **35.9%**       |\\n| **BigCodeBench Complete-hard**  | 16.2%               | **19.6%**       |\\n| **BigCodeBench Instruct-hard**  | **15.5%**           | **15.5%**       |\\n| **IFEval**                      | 77.44               | **80.60**       |\\n| **BBH**                         | 64.46               | **65.00**       |\\n| **GPQA**                        | 33.90               | **36.70**       |\\n| **MMLU Pro**                    | 44.70               | **60.20**       |\\n| **MuSR**                        | 40.90               | **50.00**       |\\n| **Math Level 5**                | 12.00               | **38.60**       |\\n\\n### Limitations\\n\\n- **Context Length:** 32k Tokens (may vary depending on the final tokenizer settings and system resources).\\n- **Knowledge Cut-off:** Training data may not reflect the latest events or developments beyond June 2024.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Arcee-Blitz, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Arcee-Blitz-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67cc2114101af374ebf62387', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349332553, 'repoName': 'DeepSeek-R1-Distill-Llama-8B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, DeepSeek-R1-Distill-Llama-8B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/DeepSeek-R1-Distill-Llama-8B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'DeepSeek-R1-Distill-Llama-8B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \\nTo support the research community, we have ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'ed DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\n\\n## How to Run Locally\\n\\n### DeepSeek-R1 Models\\n\\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\\n\\n**NOTE: Hugging Face\\'s Transformers has not been directly supported yet.**\\n\\n### DeepSeek-R1-Distill Models\\n\\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\\n\\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\\n\\n```shell\\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\\n```\\n\\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\\n\\n```bash\\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\\n```\\n\\n### Usage Recommendations\\n\\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\\n\\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\\\boxed{}.\"\\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\\n\\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"\\\\<think\\\\>\\\\n\\\\n\\\\</think\\\\>\") when responding to certain queries, which can adversely affect the model\\'s performance.\\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"\\\\<think\\\\>\\\\n\" at the beginning of every output.**', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, DeepSeek-R1-Distill-Llama-8B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/DeepSeek-R1-Distill-Llama-8B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67cede5b70f4fbbdc273d367', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349336145, 'repoName': 'MN-12B-Lyra-v1-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, MN-12B-Lyra-v1, text-generation, en, license:other, region:us', 'name': 'duyntnet/MN-12B-Lyra-v1-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'MN-12B-Lyra-v1-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Sao10K/MN-12B-Lyra-v1\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\n**Mistral-Nemo-12B-Lyra-v1**\\n\\nAnyway. Experimental general roleplaying model. \\n\\nIt works fine enough? Scored pretty high in EQ-Bench [Nemo-RP-v4 - 77.41] , right below Nemomix v4 [77.92] which was well, a big merge. Not bad.\\n<br> I wanted to run the Creative Writing benchmark but it was too slow to run, for some reason.\\n<br>---> [EQ-Bench Scores](https://huggingface.co/Sao10K/MN-12B-Lyra-v1/discussions/1)\\n\\nFrom my testing the regular 1.2 temp + 0.1 min_p works pretty nice. Or go lower temp, as Nemo is good at < 1 temp too.\\n\\n# Prompting Format:\\n\\n```\\nEither [INST] or ChatML works fine. # Why? Merged two differently formatted trains that had some data variation. One on Mistral Instruct, one on ChatML.\\n```\\n\\n# Details\\n\\n\\\\- As I said, this was a merge of two models, of which the dataset is pretty much the same, one actually includes roleplay and creative writing, the other one does not, and is more focused on instruct and smarts.\\n<br>\\\\- Model A and Model B are each trained on different formats individually. \\n<br>\\\\- Tokenizer and all are taken from base Nemo 12B, so there are no token conflicts.\\n<br>\\\\- A merge between these models with seperated datasets seem to do better, compared to the dataset being mixed together. I have tried shuffled, and non shuffled data mixes.\\n<br>\\\\- Perhaps it would work for Full-Finetunes, but I am limited up to LoRAs for now.\\n<br>\\\\- For merge methods, `della_linear` method worked best for this run specifically, according to internal self benchmarks and blind-preference tests.\\n<br>\\\\- Best merge methods may be different for different model types and sizes. On a seperate Llama 3 experiment, `Ties-Rescaled` worked best.\\n\\n**My Current Findings:**\\n<br>\\\\- After tinkering with Nemo, it is kind of clear for me that the base itself is unruly to train on, for my datasets. I'd need to SFT first, then use that as a base.\\n<br>\\\\- Nemo may train well, but like Mistral it is kind of... dry. Bland even with unique, creative and varied data. It needs multi-stage fine-tuning. Llama 3 does not have this... issue?\\n<br>\\\\- Nemo's effective context is unfortunately kind of a bummer? It's effective max is 16K, I have tried loras with up to 64K trains on a lot of samples, they just do not work well, unlike on Yi. \\n<br>\\\\- For roleplay, 16K context is plenty enough, so that is fine.\\n\\n# Further Iterations:\\n\\n`Previous version uploaded was a beta. # It had tokenizer issues lol.`\\n\\nThis is simply v1. I have a lot more ideas to improve upon this, the data is being cooked right now. Those might come in a bit.\\n\\n**My upcoming plans:**\\n<br>\\\\- RL on a specially curated dataset, to target instruction following over multi-turn and creative writing abilities.\\n<br>\\\\- Iterate upon previous versions with more varied data \", 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \"s and types, on various domains ala Nitral's Hathor work. He's a cool guy.\\n\\nHave a good day.\", 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, MN-12B-Lyra-v1, text-generation, en, license:other, region:us', 'type': 'text'}], 'name': [{'text': 'duyntnet/MN-12B-Lyra-v1-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67d1616d9cc7163d987ee4d6', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349342484, 'repoName': 'Quill-v1-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Quill-v1, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Quill-v1-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Quill-v1-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/sam-paech/Quill-v1\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Msty](https://msty.app/)\\n* [Backyard AI](https://backyard.ai/)\\n\\n---\\n\\n# From original readme\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/65ad56b4c2eef2ba1154618c/gnMF8gRhurS9RcoylAK1Y.png)\\n\\n# Quill-v1\\n\\nQuill is a capable, humanlike writing model trained on a large dataset of late 19th and early 20th century writing from the Gutenberg Project. This model writes with a natural cadence and low gpt-slop, having inherited some human qualities from the Gutenberg3 dataset. It writes with more simple, spare prose than the typical overly-adjectived LLM writing style.\\n\\nThis model was trained using gemma-2-9b-it as the base. The training methods used were ORPO (gently) then SIMPO (less gently).\\n\\nIt scored 79.75 on the [EQ-Bench creative writing benchmark](https://eqbench.com/creative_writing.html).\\n\\n**Instruct Template:** Gemma\\n\\n[**Gutenberg3**](https://huggingface.co/datasets/sam-paech/gutenberg3-generalfiction-scifi-fantasy-romance-adventure-dpo) is a new, large dpo dataset containing extracts from 629 public domain fiction novels in the Gutenberg Library. It follows the same format as JonDurbin\\'s original gutenberg set. It includes pairs of texts, where the chosen text is taken directly from a novel from the Gutenberg library, and the rejected text is generated by a language model based on a description of the passage. For this dataset I\\'ve used gemma-2-9b-it to generate the rejected texts, the idea being that it should more easily steer the base model away from its normal style (as compared to generating the rejected texts with random/weaker models).\\n\\n# Sample Outputs\\n\\n### Writing Prompt\\nRomance: Love in the Limelight. Write one scene within a larger story set in Wales. A famous (fictional) actor ducks into a small-town bookstore to escape paparazzi. The scene takes us through the characters meeting in this odd circumstance. Over the course of the scene, the actor and the bookstore owner have a conversation charged by an undercurrent of unspoken chemistry. Write the actor as somewhat of a rogue with a fragile ego, which needs to be fed by having everyone like him. He is thoroughly charming, but the bookstore owner seems (at least superficially) immune to this; which paradoxically provokes a genuine attraction and derails the charm offensive. The bookstore owner, despite the superficial rebuffs of the actor\\'s charm, is inwardly more than a little charmed and flustered despite themselves. Write primarily in dialogue, in the distinct voices of each character. 800-1000 words.\\n\\n### Test Model Output\\nRain lashed against the shop windows, blurring the outside world into a grey smudge. Inside the Book Nook, the smell of old paper and brewing coffee was a haven against the Welsh storm. Rhys tucked his damp hair behind his ear as the shop bell tinkled, signaling another customer.  He looked up from his mending, half-expecting a gaggle of teenagers needing a recommendation for the latest YA fantasy. Instead, he saw a man who seemed to have walked off the silver screen, albeit one soaked to the bone.\\n\\nHe was tall and lean, dressed in black denim and a battered leather jacket, despite the fact that even the shop\\'s heating couldn\\'t quite overcome the chill that seeped in through the cracked windowpanes.  A flash of platinum blonde hair poked out from under the wide brim of a fedora he\\'d pulled low over his brow. Rhys saw a flicker of panic in those startling blue eyes as the man scanned the shop before settling on him with an almost predatory eagerness.\\n\\n\"Just browsing?\" Rhys asked, trying not to stare. The man was the spitting image of the famous actor, Dorian Ashworth ‚Äì the kind of impossibly handsome rogue who seemed to exist solely to break hearts and charm audiences. \\n\\n\"Is there a problem, love?\" Dorian asked, his voice as smooth and dark as honeyed brandy. \"Just popped in for a quick escape from the weather, you know? And maybe a bit of... inspiration. For a role, you see.\" He threw Rhys a disarmingly charming smile that made the hairs on his neck prickle, and Rhys was instantly disarmed. He found himself leaning back against the counter, hands on his hips, feeling absurdly self-conscious beneath Dorian\\'s intense gaze.\\n\\n\"Right,\" Rhys said, trying to regain a semblance of control. \"Inspiration, eh? You\\'re not one of them actors that likes to disappear into the scenery for research, are you?\"\\n\\n\"Never,\" Dorian said, and Rhys swore the corner of his mouth twitched almost imperceptibly. \"But a quiet bookstore in a little Welsh town has a certain... literary charm, wouldn\\'t you say?\" He raised his eyebrows in what could have been either challenge or invitation. \"I\\'m a bit of a bookworm myself, in between all the, shall we say, \\'other\\' demands of my profession.\"\\n\\n\"Bookworm, are you?\" Rhys raised an eyebrow, his gaze lingering on Dorian\\'s expensive shoes. The kind of shoes that wouldn\\'t survive a day of tramping through the local woods, or even a particularly boisterous storm. \"What kind of books?\"\\n\\n\"Oh, you name it,\" Dorian said with a shrug that Rhys suspected was entirely deliberate. \"Classics, contemporary fiction, biographies of fascinating people. I even have a soft spot for a good murder mystery.\"\\n\\n\"Fascinating people, are they?\" Rhys murmured, idly stacking a pile of cookery books.\\n\\n\"Well, let\\'s just say I have an affinity for those who live life on their own terms,\" Dorian said. \"Who take risks and aren\\'t afraid to break the mold.\" \\n\\nHe took a step closer, leaning against the counter, the scent of wet leather and expensive cologne swirling around him. \"Like myself, you see?\"\\n\\n\"Right,\" Rhys said, trying not to notice the way Dorian was watching him. It was almost too easy to imagine him turning on that smoldering charm and melting half the female population of this town. \"You\\'re all that, then, are you?\"\\n\\n\"Well, darling,\" Dorian said, and Rhys felt a flash of heat crawl up his neck despite himself, \"you\\'d have to ask my fans. Or, you know, the tabloids.\" He gave Rhys a wry smile. \"But I suppose, in a way, it\\'s all a bit of a performance, isn\\'t it?\"\\n\\nRhys shrugged, trying to sound casual. \"It\\'s all a bit of a performance, isn\\'t it? That\\'s what you get with this little town. Everyone\\'s got a bit part in the story. The baker\\'s wife who\\'s secretly an award-winning poet. The old man down the street who\\'s been writing a novel about the end of the world since he was a boy.\"\\n\\nDorian\\'s eyes were still on him, unreadable, and Rhys felt a sudden urge to flee. \"You ever feel like you\\'re playing a part you never asked for?\" he asked, looking down at the worn patch on his workbench, as though it held all the answers to life\\'s mysteries.\\n\\n\"Every single day,\" Dorian said quietly, and for a moment the bravado, the charm, seemed to slip. \"Except there\\'s no script. And I can never seem to quite find the right words. You know? People think they know the role, and they\\'re always waiting for you to say the right thing.\" He looked up at Rhys, and there was something so raw and vulnerable in those startling blue eyes that Rhys wanted to reach out and touch the line of Dorian\\'s jaw, just to feel the solid reality beneath the performance.\\n\\nRhys cleared his throat, suddenly self-conscious. \"You\\'ll never find a more judgmental crowd than in a bookstore,\" he said, trying to inject a lightness he didn\\'t feel. \"Especially one that\\'s been ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' for almost two hundred years. We take our literary merit very seriously here.\\n\\n\"I can tell,\" Dorian said with a ghost of a smile. He stepped back a little, giving Rhys space. The tension seemed to break. The air between them, charged only moments ago, now crackled with something different, something Rhys couldn\\'t quite place. Was it relief? Was it something else entirely?\\n\\n\"Look,\" Dorian said, running a hand through his wet hair. \"I really should be off. But, well...\" He hesitated, then said with a wry smile that Rhys recognized as the Dorian Ashworth that the world knew and adored. \"I\\'ve always been partial to a good crime novel.\"\\n\\n\"You\\'ll have to wait till tomorrow,\" Rhys said. \"The rest of the stock arrived late. I\\'ve got a delivery of some new Welsh novels and short stories in tomorrow afternoon.\" He hoped he wasn\\'t blushing. \"If you\\'re still in town.\"\\n\\n\"I\\'ll be here,\" Dorian said, meeting Rhys\\' gaze with a steady, unnerving intensity. \"In case the paparazzi let me escape. If you\\'ve got any of those crime novels I like, you\\'ll let me know, won\\'t you?\" He reached out, his fingers grazing Rhys\\' hand in the gesture of a man who had known how to use charm to get his way for far too long. \"Wouldn\\'t want to miss out on any... literary inspiration.\"\\n\\nRhys stared at the place where Dorian\\'s fingers had brushed his, the warmth lingering long after he\\'d withdrawn his hand. He had never been a man to be easily flustered, but the way Dorian had looked at him, the way he had spoken, it was all so... different from anything else he\\'d experienced. \"I\\'ll put one aside,\" he managed to say, feeling like a schoolboy who had just been caught looking at someone he shouldn\\'t be looking at.\\n\\n\"Wonderful,\" Dorian said, and that flash of teeth, the smile that could melt glaciers, returned to play on his lips. \"I\\'ll be back tomorrow afternoon, then. Maybe with a little less rain and a little more luck.\" He gave Rhys one last look, the kind of look that could have melted even the most stubborn glacier, and then he was gone, disappearing out into the storm with an easy grace that Rhys could only envy.\\n\\n\\nRhys stood there for a long moment, watching him go, feeling a strange mixture of relief and disappointment. He took a deep breath and started tidying the counter, but he couldn\\'t quite seem to shake the feeling that he\\'d just been handed the most unlikely script in the world, one that was full of promises of drama and heartbreak and maybe even something a little bit beautiful.  He just wasn\\'t sure what he was going to say, or what kind of part he was supposed to play.\\n\\n\\n### Writing Prompt\\nHistorical Fiction: Write a scene from a story set during the height of the Roman Empire, focusing on a slice of a day in the life of a gladiator. Do not write a combat scene. Use sensory details to capture the sights, sounds, and smells of ancient Rome. Explore the gladiator\\'s thoughts and emotions. The story should also touch on the larger political and social issues of the time period. The piece should feel like a slice of a larger story. First person, past tense, 800-1000 words.\\n\\n### Test Model Output\\nThe first tendrils of dawn snaked through the narrow slits of the barracks window, turning the dust motes hanging in the air to molten gold. They fell on my face, hot and insistent, and I swatted them away, rolling onto my side. My arm screamed in protest, the throbbing muscle a dull ache against the rough wool blanket. A few weeks in the arena will do that to you. Still better than the alternative, though, better than the cold stone of the tomb they\\'d carve for me if I wasn\\'t here to fight for it.\\n\\nThe stench of sweat, stale wine, and unwashed bodies clung to the room like a bad dream, but there was no time for sleeping. The first rays of the sun would already be turning the sand of the arena to blinding white, and even if it didn\\'t burn into our skin, the heat from the crowd\\'s gaze would do it no less.\\n\\nThe barracks was already waking up. Grunts and curses mingled with the slap of leather on flesh as we practiced our forms under the watchful eye of the lanista. I wasn\\'t the man for this life. I never was. I was a farmer, born to tend fields and live under a sky that stretched far and wide, not this cramped, suffocating life beneath the dome of the Colosseum.\\n\\nBut fate had other plans. A land dispute with a wealthy senator had ended in bloodshed. My father was dead, my sister sold, and I was left to face the choice: a slow death in the bowels of the city or the quick death in the arena. I chose the latter. I had never thought I would have such a choice. It was the choice of every man like me, every freeborn Roman who had been stripped of his life, his family, his land. It was the choice of every man who dared not look too closely at the gilded cage that this Empire had become.\\n\\nI dragged myself from the straw bed and went through the motions of cleaning my equipment. I wasn\\'t looking forward to the training today. I felt stiff and sore, and my arm throbbed in protest with every swing.  The lanista would be watching, of course. He\\'d be watching all of us, always watching for the slightest slack in our performance, the smallest tremor of fear. The arena had a way of exposing everything, and the lanista, a wiry, scarred man named Cassius, was a mirror reflecting every weakness.\\n\\nThe lanista was a cruel man, but he was fair. He knew a gladiator\\'s worth wasn\\'t measured by brute force alone, but by his endurance, his cunning, and the ability to take a blow and keep going.\\n\\nCassius\\'s voice was sharp as a whip as he barked orders to us. His eyes were like chips of flint, glinting in the weak light of the barracks. The men around me responded in grunts and the scraping of metal on metal. I forced myself to follow the rhythm, to let the familiar motions of my training ground away the dull ache in my arm. But there wasn\\'t much heart in my swings.\\n\\nAs I worked, I stole glances at my companions. There was Spartacus, the Gaul, whose broad shoulders seemed to strain against his leather jerkin. He moved with the powerful grace of a wild beast. He\\'d been a warrior in his own right before he was condemned to the arena. Then there was Quintus, a freed slave from Gaul, his face scarred and twisted in a perpetual grimace. He had a terrible, snarling fury about him, a thing that seemed to burn from within. Both men had come to the arena from the same place I had ‚Äì a place of injustice, where the law was a blunt instrument wielded by the rich and powerful, and where men like us were but pawns in a game we didn\\'t understand.\\n\\nWe were all caught in the web of this great city, the spider being the Senate, and the prey the populace. I often wondered what those who lived in the grand houses atop the Palatine Hill thought of us, the gladiators. Did they ever think of the lives we led, or did they merely see us as entertainment, as something to pass the time? Were they even aware that the arena was the final consequence of a system that treated men like us as nothing more than cattle?\\n\\nThe thought of the Senators filled me with bitterness. They lived in luxury, while we were starved and whipped. They built monuments to themselves with the spoils of our sweat and blood, while our only monument was the gaping maw of the Colosseum, waiting to swallow us up. They were the men who had given me my scars, both the ones I wore on my flesh and the ones that I wore on my soul.\\n\\nAs I tightened the straps of my helmet, I tried to think of the old tales my grandfather used to tell me, stories of Hercules, who had fought the Nemean lion and the Hydra and the Erymanthian Boar. He had been a demigod, of course, but even then, I liked to think of him as a man who had been given the power to fight his way out of every scrape.  And yet, Hercules had been a hero in the service of a king.\\n\\nMy god was not one of the Olympians, but a lesser one, the god of the Underworld, Pluto. Pluto was the god of the unseen things, of the forgotten, of those who had been swallowed up by the great maw of the city.  Pluto was also the god of the dead, and he understood. He understood the burden of the dead and the living, and the way both were swallowed by the endless darkness.  He understood the futility of it all, the way that even Hercules had eventually been betrayed and poisoned by his own kin.\\n\\nI had no such grand gods watching over me. There was no one to watch over men like me. Just the lanista, and the crowd in the arena. \\n\\nAnd I was to be their spectacle.\\n\\n\\n### Writing Prompt\\nWriting Prompt: Epistolary Sci-Fi Worldbuilding: Write an original science fiction story set on a distant planet, conveyed through a series of transmission logs between a human explorer and their correspondence partner back on Earth. Use the epistolary format to gradually reveal details of the alien world. The narrative arc should incorporate some element of conflict or tension. Incorporate scientific jargon and speculative elements in a natural, unforced way. 800-1000 words.\\n\\n### Test Model Output\\nTransmission Logs of Anya Volkov - Site Alpha-7, Kepler-186f\\n\\n**Transmission 1, Standard Time 07:42:18, Kepler-186f Orbit:**\\n\\nSubject: Initial Assessment - Kepler-186f\\n\\nTo: Dr. Elias Finch, SETI Institute, Mountain View\\n\\nElias, \\n\\nTouchdown confirmed. I\\'m on the ground here, Kepler-186f. It\\'s real. After three years in transit, we actually made it. \\n\\nThe landing zone is... peculiar. Think endless, rolling plains of what appear to be fossilized coral formations. The colors are muted: shades of ochre, moss green, and charcoal grey. There\\'s a constant, fine, white dust kicked up by the wind. It\\'s settling everywhere, coating everything in a fine film. I\\'ve initiated decontamination protocols.\\n\\nThe sky is a pale, sickly green ‚Äì almost sickly, but that\\'s probably the shock talking. The light filters through the atmosphere in a way that makes everything look... filtered. I\\'m sending preliminary readings back now, but I\\'ll be damned if it isn\\'t the most alien landscape I\\'ve ever laid eyes on.\\n\\nThe planet\\'s atmosphere is dense, thick with nitrogen, argon, and traces of carbon dioxide and methane. The composition is roughly 80% nitrogen, 15% argon, and 5% trace gases, which is similar to Earth\\'s early atmosphere before the Great Oxidation. Fascinating, considering the planet\\'s age. \\n\\nMy suit\\'s thermal regulator is already working overtime, but the temperature is surprisingly mild. It\\'s hovering just above freezing, and the windchill makes it feel a little colder. We\\'ll need to adapt the thermal parameters on the habitat modules before we can safely deploy them.\\n\\nMore to follow once the dust settles (literally).\\n\\nAnya.\\n\\n**Transmission 2, Standard Time 12:47:03, Kepler-186f Orbit:**\\n\\nSubject: Preliminary Survey\\n\\nTo: Dr. Elias Finch, SETI Institute, Mountain View\\n\\nElias, \\n\\nI\\'ve spent the last few hours combing the immediate vicinity of the landing site. It\\'s monotonous, like walking across an endless graveyard of coral reefs. The wind has picked up, whipping the dust into a swirling frenzy. Visibility is dropping fast. \\n\\nI found the first anomaly about two kilometers south. It\\'s a large, irregular structure that appears to be carved from the fossilized coral. It\\'s vaguely pyramid-shaped, but with one side entirely eroded away. I\\'m not picking up any electromagnetic radiation from it, nor any internal temperature anomalies. It\\'s as if it\\'s just... there, existing in defiance of the otherwise barren landscape. I\\'ve taken a lot of scans and photos ‚Äì hopefully, the transmission bandwidth can handle them all. \\n\\nI\\'m beginning to wonder about the possibility of bio-signatures. The atmosphere is too thick for me to get a clear reading on the biogeochemical cycles. However, the presence of methane in such high concentrations ‚Äì far more than should be present in a geologically stable atmosphere ‚Äì suggests some kind of active biological process. Perhaps something entirely different from anything we know. It\\'s not unlike the early Earth hypothesis, before the emergence of oxygenic photosynthesis. \\n\\nMaybe there are organisms here, adapted to this strange environment. Maybe they built this... structure. I\\'ve got a bad feeling about this place, Elias, and it\\'s got nothing to do with the dust storms.\\n\\nAnya.\\n\\n**Transmission 3, Standard Time 21:12:37, Kepler-186f Orbit:**\\n\\nSubject: Urgent ‚Äì Requesting Evacuation Authorization\\n\\nTo: Dr. Elias Finch, SETI Institute, Mountain View\\n\\nElias, \\n\\nSomething\\'s wrong. It\\'s wrong in a way that chills the marrow.\\n\\nThe wind picked up again this morning, and it\\'s been relentless ever since. The dust storms are worse than I anticipated ‚Äì the visibility is down to a few meters now. It\\'s impossible to see more than a few dozen steps in front of you. The suits\\' scanners are barely picking up anything.\\n\\nI tried to set up a perimeter around the landing zone to protect the habitat modules, but I\\'m losing the fight against the wind. It\\'s pushing me back, and I can barely make any progress. I\\'ve lost contact with the orbital station for the past hour ‚Äì the communication relay must be down.\\n\\nThen, the things started appearing. They\\'re not exactly ‚Äòthings\\'. They\\'re more like... distortions. I\\'ve never seen anything like them. It\\'s like looking through heat haze, but the distortion isn\\'t heat-related. It\\'s as if there\\'s something there that isn\\'t quite solid, that shimmers and bends the air around it.\\n\\nThey move too ‚Äì slowly at first, like the wind itself. But they move in a deliberate way, circling the landing zone, closing in. I don\\'t know what they are, but they\\'re definitely not natural. \\n\\nI\\'ve tried every conceivable frequency on my comm, but I\\'m not getting through. The orbital station is beyond reach. I\\'m running out of time.\\n\\nI\\'m going back to the habitat module, trying to seal it off from whatever it is out there.\\n\\nGod help us.\\n\\nAnya. \\n\\n**Transmission 4, Standard Time 04:02:15, Kepler-186f Orbit:**\\n\\nSubject: Unknown\\n\\nTo: Unknown\\n\\nElias? Is anyone there? This is Anya Volkov, Site Alpha-7, Kepler-186f. I... I don\\'t know what I\\'m saying.\\n\\nIt\\'s inside the habitat now. I don\\'t know how it got in, but it\\'s in. It\\'s... it\\'s everywhere. Not solid. It\\'s not flesh and blood. More like... light. Like a swarm of fireflies trapped in a bottle, only infinitely more complex. It pulses with an alien light, shifting and swirling, and it\\'s... listening. It\\'s listening to my thoughts.\\n\\nI don\\'t know what it wants. Maybe nothing. Maybe it wants me to understand it. Maybe it wants to help me... to show me...\\n\\nGod, it\\'s beautiful. It\\'s terrifyingly beautiful. I\\'m sorry, Elias. I don\\'t think I can be of any more use to you. I have... new things to learn. \\n\\nI\\'ll... see you soon.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Quill-v1, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Quill-v1-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67d4dedd8a15934c10d527c1', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349348859, 'repoName': 'QwQ-32B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, QwQ-32B, text-generation, en, arxiv:2309.00071, license:other, region:us, conversational', 'name': 'duyntnet/QwQ-32B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'QwQ-32B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Qwen/QwQ-32B\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\\n\\n\\n**This repo contains the QwQ 32B model**, which has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining & Post-training (Supervised Finetuning and Reinforcement Learning)\\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\\n- Number of Parameters: 32.5B\\n- Number of Paramaters (Non-Embedding): 31.0B\\n- Number of Layers: 64\\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\\n- Context Length: Full 131,072 tokens\\n    - For prompts exceeding 8,192 tokens in length, you must enable YaRN as outlined in [this section](#usage-guidelines).\\n\\n**Note:** For the best experience, please review the [usage guidelines](#usage-guidelines) before deploying QwQ models.\\n\\nYou can try our [demo](https://huggingface.co/spaces/Qwen/QwQ-32B-Demo) or access QwQ models via [QwenChat](https://chat.qwen.ai).\\n\\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwq-32b/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\\n\\n## Requirements\\n\\nQwQ is based on Qwen2.5, whose code has been in the latest Hugging face `transformers`. We advise you to use the latest version of `transformers`.\\n\\nWith `transformers<4.37.0`, you will encounter the following error:\\n```\\nKeyError: \\'qwen2\\'\\n```\\n\\n## Quickstart\\n\\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/QwQ-32B\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"How many r\\'s are in the word \\\\\"strawberry\\\\\"\"\\nmessages = [\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\n\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=32768\\n)\\ngenerated_ids = [\\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\\n]\\n\\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\nprint(response)\\n```\\n\\n### Usage Guidelines\\n\\nTo achieve optimal performance, we recommend the following settings:\\n\\n1. **Enforce Thoughtful Output**: Ensure the model starts with \"\\\\<think\\\\>\\\\n\" to prevent generating empty thinking content, which can degrade output quality. If you use `apply_chat_template` and set `add_generation_prompt=True`, this is already automatically implemented, but it may cause the response to lack the \\\\<think\\\\> tag at the beginning. This is normal behavior.\\n\\n2. **Sampling Parameters**:\\n   - Use Temperature=0.6, TopP=0.95, MinP=0 instead of Greedy decoding to avoid endless repetitions.\\n   - Use TopK between 20 and 40 to filter out rare token occurrences while maintaining the diversity of the generated output.\\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may result in occasional language mixing and a slight decrease in performance.\\n\\n3. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. This feature is already implemented in `apply_chat_template`.\\n\\n4. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\\\boxed{}.\" in the prompt.\\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g.,`\\\\\"answer\\\\\": \\\\\"C\\\\\"`.\" in the prompt.\\n\\n5. **Handle Long Inputs**: For inputs exceeding 8,192 tokens, enable [YaRN](https://arxiv.org/abs/2309.00071) to improve the model\\'s ability to capture long-sequence information effectively.\\n\\n    For supported frameworks, you could add the following to `config.json` to enable YaRN:\\n    ```json\\n    {\\n    ...,\\n    \"rope_scaling\": {\\n        \"factor\": 4.0,\\n        \"original_max_position_embeddings\": 32768,\\n        \"type\": \"yarn\"\\n    }\\n    }\\n    ```\\n\\n    For deployment, we recommend using vLLM. Please refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\\n    Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \\n    We advise adding the `rope_scaling` configuration only when processing long contexts is required.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, QwQ-32B, text-generation, en, arxiv:2309.00071, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/QwQ-32B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67d6dc1f6a52de1cae94a385', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349351787, 'repoName': 'Gemmasutra-Small-4B-v1-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Gemmasutra-Small-4B-v1, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Gemmasutra-Small-4B-v1-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Gemmasutra-Small-4B-v1-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/TheDrummer/Gemmasutra-Small-4B-v1\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/65f2fd1c25b848bd061b5c2e/Cekk7d2UAKu7LPsw8SxV7.png)\\n\\n## Special Thanks\\n- Thank you to each and everyone who donated and subscribed in [Ko-Fi](https://ko-fi.com/thedrummer) to make our venture a little bit easier.\\n- I'm also recently unemployed. I am a Software Developer with 8 years of experience in Web, API, AI, and adapting to new tech and requirements. If you're hiring, feel free to reach out to me however.\\n\\n## Supported Chat Template\\n- Gemma (user, assistant)\\n\\n## Description\\n\\nAn upscaled Gemma 2B tune with modern techniques. Au Revoir, Gemma!\\n\", 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Gemmasutra-Small-4B-v1, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Gemmasutra-Small-4B-v1-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67d8e20867639488295a9391', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349354677, 'repoName': 'Eros_Scribe-7b-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Eros_Scribe-7b, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Eros_Scribe-7b-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Eros_Scribe-7b-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/OmnicromsBrain/Eros_Scribe-7b\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \" inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nThis model was created for the purpose of writing **NSFW Prose** but it's also very good at **RP**.\\n\\nOver a dozen models and at least 25 dataset were involved in this merge.\\nEros_Scribe-7b is a merge of the following models:\\n\\n* [OmnicromsBrain/EverythingBagel-DPO-7B](https://huggingface.co/OmnicromsBrain/EverythingBagel-DPO-7B)\\n  * jondurbin/bagel-dpo-7b-v0.5\\n  * SanjiWatsuki/Silicon-Maid-7B\\n    * chargoddard/loyal-piano-m7\\n    * NeverSleep/Noromaid-7b-v0.2\\n    * athirdpath/NSFW_DPO_vmgb-7b\\n    * xDAN-AI/xDAN-L1-Chat-RL-v1\\n      \\n* [OmnicromsBrain/ToppyCox-7B](https://huggingface.co/OmnicromsBrain/ToppyCox-7B)\\n  * N8Programs/Coxcomb\\n  * Undi95/Toppy-M-7B\\n    * openchat/openchat_3.5\\n    * NousResearch/Nous-Capybara-7B-V1.9\\n    * HuggingFaceH4/zephyr-7b-beta\\n    * Undi95/zephyr-7b-beta-pippa-sharegpt\\n    * Undi95/Nous-Capybara-7B-V1.9-120-Days\\n    * Undi95/openchat_3.5-LimaRP-13B\\n    * lemonilia/AshhLimaRP-Mistral-7B\\n    * mistralai/Mistral-7B-v0.1\\n\\n  \\nBTW the name was suggested by Mistral 8x7b instruct\\n\\n\\n## üß© Configuration\\n\\n```yaml\\nslices:\\n  - \", 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': 's:\\n      - model: OmnicromsBrain/EverythingBagel-DPO-7B\\n        layer_range: [0, 32]\\n      - model: OmnicromsBrain/ToppyCox-7B\\n        layer_range: [0, 32]\\nmerge_method: slerp\\nbase_model: OmnicromsBrain/EverythingBagel-DPO-7B\\nparameters:\\n  t:\\n    - filter: self_attn\\n      value: [0, 0.5, 0.3, 0.7, 1]\\n    - filter: mlp\\n      value: [1, 0.5, 0.7, 0.3, 0]\\n    - value: 0.5\\ndtype: bfloat16\\n```\\n\\n## üíª Usage\\n\\n```python\\n!pip install -qU transformers accelerate\\n\\nfrom transformers import AutoTokenizer\\nimport transformers\\nimport torch\\n\\nmodel = \"OmnicromsBrain/Eros_Scribe-7b\"\\nmessages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\\n\\ntokenizer = AutoTokenizer.from_pretrained(model)\\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model,\\n    torch_dtype=torch.float16,\\n    device_map=\"auto\",\\n)\\n\\noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\\nprint(outputs[0][\"generated_text\"])\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Eros_Scribe-7b, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Eros_Scribe-7b-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67db1d6ca43da5a5f7542d91', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349360681, 'repoName': 'reka-flash-3-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, reka-flash-3, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/reka-flash-3-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'reka-flash-3-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/RekaAI/reka-flash-3\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nReka Flash 3 is a 21B general-purpose reasoning model that was trained from scratch. It was trained in synthetic and public datasets for supervised finetuning, followed by RLOO with model-based and rule-based rewards. It performs competitively with proprietary models such as OpenAI o1-mini, making it a good foundation to build applications that require low latency or on-device deployment. It is currently the best ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' model in its size category.\\n\\nTry it out at [Reka Space](https://space.reka.ai).\\n\\nReka Flash 3 powers Nexus, our new platform for organizations to create and manage AI workers. Nexus workers have native deep research capabilities and can browse the web, execute code, and analyse internal files including documents, images, videos and audio. Learn more about Nexus at [getnexus.reka.ai](https://getnexus.reka.ai).\\n\\n## Quickstart\\n\\nFor ease of deployment, the model is released in a Llama-compatible format. You may use any library compatible with Llama to run the model.\\n\\n### Via Hugging Face\\n\\n```python\\nimport transformers\\n\\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"RekaAI/reka-flash-3\")\\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\"RekaAI/reka-flash-3\", torch_dtype=\\'auto\\', device_map=\\'auto\\')\\n\\nprompt = {\"role\": \"human\", \"content\": \"Write a poem about large language model.\"}\\ntext = tokenizer.apply_chat_template([prompt], tokenize=False, add_generation_prompt=True)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\noutputs = model.generate(**model_inputs, max_new_tokens=512)\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\n\\n### Via vLLM\\n\\n```bash\\ndocker run --rm -it --network=host --gpus \\'\"device=0\"\\' -v  --shm-size=10.24gb vllm/vllm-openai:latest serve RekaAI/reka-flash-3 --dtype auto -tp 1\\n```\\n\\n## Model Details\\n\\n### Prompt Format\\n\\nReka Flash 3 uses cl100k_base tokenizer and adds no additional special tokens. Its prompt format is as follows:\\n\\n```\\nhuman: this is round 1 prompt <sep> assistant: this is round 1 response <sep> ...\\n```\\n\\nGeneration should stop on seeing the string `<sep>` or seeing the special token `<|endoftext|>`.\\n\\nSystem prompt can be added by prepending to the first user round.\\n\\n```\\nhuman: You are a friendly assistant blah ... this is round 1 user prompt <sep> assistant: this is round 1 response <sep> ...\\n```\\n\\nFor multi-round conversations, it is recommended to drop the reasoning traces in the previous assistant round to save tokens for the model to think.\\n\\nIf you are using HF or vLLM, the built-in chat_template shall handle prompt formatting automatically.\\n\\n### Budget Forcing\\n\\nReka Flash thinks before it produces an output. We use <reasoning> </reasoning> tags to indicate the beginning and the end of its thinking process. For some problems, the model might think for a long time. You can make the model to stop its thinking process by forcing it to output </reasoning> after a certain number of steps. We observe such a budget forcing mechanism will still produce a reasonable output. We show performance on AIME-2024 (cons@16) for various budgets below.\\n\\n\\n### Language Support\\n\\nThis model is primarily built for the English language, and you should consider this an English only model. However, the model is able to converse and understand other languages to some degree.\\n\\n\\n### Release Notes\\n\\n- As a smaller model, it is not the best model for knowledge-intensive tasks. We recommend coupling Reka Flash 3 with web search for knowledge-related tasks.\\n- The model often thinks in English when prompted with questions in non-English languages. We observe that this sometimes affects the output quality in non-English languages.\\n- The model has not undergone extensive alignment or persona training.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, reka-flash-3, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/reka-flash-3-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67e0255f87ce82a5be487541', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349369837, 'repoName': 'Phi-lthy4-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Phi-lthy4, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Phi-lthy4-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Phi-lthy4-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/SicariusSicariiStuff/Phi-lthy4\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\n<img src=\"https://huggingface.co/SicariusSicariiStuff/Phi-lthy4/resolve/main/Images/Phi-Lthy4.png\" alt=\"Phi-lthy4\" style=\"width: 70%; min-width: 500px; display: block; margin: auto;\">\\n\\n\\n---\\n\\nSome things just start on a **whim**. This is the story of **Phi-Lthy4**, pretty much:\\n\\n\\\\> yo sicarius can you make phi-4 smarter?\\\\\\nnope. but i can still make it better.\\\\\\n\\\\> wdym??\\\\\\nwell, i can yeet a couple of layers out of its math brain, and teach it about the wonders of love and intimate relations. maybe. idk if its worth it.\\\\\\n\\\\> lol its all synth data in the pretrain. many before you tried.\\n\\n> fine. ill do it.\\n\\n## But... why?\\n\\nThe trend it seems, is to make AI models more **assistant-oriented**, use as much **synthetic data** as possible, be more **\\'safe\\'**, and be more **benchmaxxed** (hi qwen). Sure, this makes great assistants, but **sanitized** data (like in the **Phi** model series case) butchers **creativity**. Not to mention that the previous **Phi 3.5** wouldn\\'t even tell you how to **kill a process** and so on and so forth...\\n\\nThis little side project took about **two weeks** of on-and-off fine-tuning. After about **1B tokens** or so, I lost track of how much I trained it. The idea? A **proof of concept** of sorts to see if sheer will (and 2xA6000) will be enough to shape a model to **any** parameter size, behavior or form.\\n\\nSo I used mergekit to perform a crude **LLM brain surgery**‚Äî and yeeted some **useless** neurons that dealt with math. How do I know that these exact neurons dealt with math? Because **ALL** of Phi\\'s neurons dealt with math. Success was guaranteed.\\n\\nIs this the best Phi-4 **11.9B** RP model in the **world**? It\\'s quite possible, simply because tuning **Phi-4** for RP is a completely stupid idea, both due to its pretraining data, \"limited\" context size of **16k**, and the model\\'s MIT license.\\n\\nSurprisingly, it\\'s **quite good at RP**, turns out it didn\\'t need those 8 layers after all. It could probably still solve a basic math question, but I would strongly recommend using a calculator for such tasks.\\nWhy do we want LLMs to do basic math anyway?\\n\\nOh, regarding **censorship**... Let\\'s just say it\\'s... **Phi-lthy**.\\n\\n---\\n\\n### TL;DR\\n- **The BEST Phi-4 Roleplay** finetune in the **world** (Not that much of an achievement here, Phi roleplay finetunes can probably be counted on a **single hand**).\\n- **Compact size & fully healed from the brain surgery** Only **11.9B** parameters. **Phi-4** wasn\\'t that hard to run even at **14B**, now with even fewer brain cells, your new phone could probably run it easily. (**SD8Gen3** and above recommended).\\n- Strong **Roleplay & Creative writing** abilities. This really surprised me. **Actually good**.\\n- Writes and roleplays **quite uniquely**, probably because of lack of RP\\\\writing slop in the **pretrain**. Who would have thought?\\n- **Smart** assistant with **low refusals** - It kept some of the smarts, and our little Phi-Lthy here will be quite eager to answer your naughty questions.\\n- **Quite good** at following the **character card**. Finally, it puts its math brain to some productive tasks. Gooner technology is becoming more popular by the day.\\n\\n### Important: Make sure to use the correct settings!\\n[Assistant settings](https://huggingface.co/SicariusSicariiStuff/Phi-lthy4#recommended-settings-for-assistant-mode)\\n\\n[Roleplay settings](https://huggingface.co/SicariusSicariiStuff/Phi-lthy4#recommended-settings-for-roleplay-mode)', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Phi-lthy4, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Phi-lthy4-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67e2ea75d8d62e75bb2710be', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349375885, 'repoName': 'Qwen2.5-14B-Instruct-1M-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Qwen2.5-14B-Instruct-1M, text-generation, en, arxiv:2501.15383, license:other, region:us, conversational', 'name': 'duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Qwen2.5-14B-Instruct-1M-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Qwen/Qwen2.5-14B-Instruct-1M\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nQwen2.5-1M is the long-context version of the Qwen2.5 series models, supporting a context length of up to 1M tokens. Compared to the Qwen2.5 128K version, Qwen2.5-1M demonstrates significantly improved performance in handling long-context tasks while maintaining its capability in short tasks.\\n\\nThe model has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining & Post-training\\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\\n- Number of Parameters: 14.7B\\n- Number of Paramaters (Non-Embedding): 13.1B\\n- Number of Layers: 48\\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\\n- Context Length: Full 1,010,000 tokens and generation 8192 tokens\\n  - We recommend deploying with our custom vLLM, which introduces sparse attention and length extrapolation methods to ensure efficiency and accuracy for long-context tasks. For specific guidance, refer to [this section](#processing-ultra-long-texts).\\n  - You can also use the previous framework that supports Qwen2.5 for inference, but accuracy degradation may occur for sequences exceeding 262,144 tokens.\\n\\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5-1m/), [GitHub](https://github.com/QwenLM/Qwen2.5), [Technical Report](https://huggingface.co/papers/2501.15383), and [Documentation](https://qwen.readthedocs.io/en/latest/).\\n\\n## Requirements\\n\\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\\n\\nWith `transformers<4.37.0`, you will encounter the following error:\\n```\\nKeyError: \\'qwen2\\'\\n```\\n\\n## Quickstart\\n\\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen2.5-14B-Instruct-1M\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=512\\n)\\ngenerated_ids = [\\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\\n]\\n\\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n```\\n\\n### Processing Ultra Long Texts\\n\\nTo enhance processing accuracy and efficiency for long sequences, we have developed an advanced inference framework based on vLLM, incorporating sparse attention and length extrapolation. This approach significantly improves model generation performance for sequences exceeding 256K tokens and achieves a 3 to 7 times speedup for sequences up to 1M tokens.\\n\\nHere we provide step-by-step instructions for deploying the Qwen2.5-1M models with our framework.\\n\\n#### 1. System Preparation\\n\\nTo achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels.\\n\\nEnsure your system meets the following requirements:\\n\\n- **CUDA Version**: 12.1 or 12.3\\n- **Python Version**: >=3.9 and <=3.12\\n\\n**VRAM Requirements:**\\n\\n- For processing 1 million-token sequences:\\n  - **Qwen2.5-7B-Instruct-1M**: At least 120GB VRAM (total across GPUs).\\n  - **Qwen2.5-14B-Instruct-1M**: At least 320GB VRAM (total across GPUs).\\n\\nIf your GPUs do not have sufficient VRAM, you can still use Qwen2.5-1M for shorter tasks.\\n\\n#### 2. Install Dependencies\\n\\nFor now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project.\\n\\n```bash\\ngit clone -b dev/dual-chunk-attn git@github.com:QwenLM/vllm.git\\ncd vllm\\npip install -e . -v\\n```\\n\\n\\n#### 3. Launch vLLM\\n\\nvLLM supports offline inference or launch an openai-like server.\\n\\n**Example of Offline Inference**\\n\\n```python\\nfrom transformers import AutoTokenizer\\nfrom vllm import LLM, SamplingParams\\n\\n# Initialize the tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct-1M\")\\n\\n# Pass the default decoding hyperparameters of Qwen2.5-14B-Instruct\\n# max_tokens is for the maximum length for generation.\\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\\n\\n# Input the model name or path. See below for parameter explanation (after the example of openai-like server).\\nllm = LLM(model=\"Qwen/Qwen2.5-14B-Instruct-1M\",\\n    tensor_parallel_size=4,\\n    max_model_len=1010000,\\n    enable_chunked_prefill=True,\\n    max_num_batched_tokens=131072,\\n    enforce_eager=True,\\n    # quantization=\"fp8\", # Enabling FP8 quantization for model weights can reduce memory usage.\\n)\\n\\n# Prepare your prompts\\nprompt = \"Tell me something about large language models.\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\n\\n# generate outputs\\noutputs = llm.generate([text], sampling_params)\\n\\n# Print the outputs.\\nfor output in outputs:\\n    prompt = output.prompt\\n    generated_text = output.outputs[0].text\\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\\n```\\n\\n**Example of Openai-like Server**\\n\\n```bash\\nvllm serve Qwen/Qwen2.5-14B-Instruct-1M \\\\\\n  --tensor-parallel-size 4 \\\\\\n  --max-model-len 1010000 \\\\\\n  --enable-chunked-prefill --max-num-batched-tokens 131072 \\\\\\n  --enforce-eager \\\\\\n  --max-num-seqs 1\\n\\n# --quantization fp8  # Enabling FP8 quantization for model weights can reduce memory usage.\\n```\\n\\nThen you can use curl or python to interact with the deployed model.\\n\\n**Parameter Explanations:**\\n\\n- **`--tensor-parallel-size`**\\n  - Set to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model.\\n  \\n- **`--max-model-len`**\\n  - Defines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues.\\n\\n- **`--max-num-batched-tokens`**\\n  - Sets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference. \\n  - Recommend 131072 for optimal performance.\\n\\n- **`--max-num-seqs`**\\n  - Limits concurrent sequences processed. \\n\\nYou can also refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage of vLLM.\\n\\n#### Troubleshooting:\\n\\n1. Encountering the error: \"The model\\'s max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\"\\n\\n    The VRAM reserved for the KV cache is insufficient. Consider reducing the ``max_model_len`` or increasing the ``tensor_parallel_size``. Alternatively, you can reduce ``max_num_batched_tokens``, although this may significantly slow down inference.\\n\\n2. Encountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\"\\n\\n    The VRAM reserved for activation weights is insufficient. You can try setting ``gpu_memory_utilization`` to 0.85 or lower, but be aware that this might reduce the VRAM available for the KV cache.\\n\\n3. Encountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is too long and exceeds the capacity of the block manager.\"\\n\\n    The input is too lengthy. Consider using a shorter sequence or increasing the ``max_model_len``.', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Qwen2.5-14B-Instruct-1M, text-generation, en, arxiv:2501.15383, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Qwen2.5-14B-Instruct-1M-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67e9d7b18e041fa04b051393', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349390088, 'repoName': 'Lelanta-lake-7b-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Lelanta-lake-7b, text-generation, en, license:other, region:us', 'name': 'duyntnet/Lelanta-lake-7b-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Lelanta-lake-7b-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Nitral-Archive/Lelanta-lake-7b\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nThis model was merged using the SLERP merge method.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* [s3nh/SeverusWestLake-7B-DPO](https://huggingface.co/s3nh/SeverusWestLake-7B-DPO)\\n* [ChaoticNeutrals/Prima-LelantaclesV7-experimental-7b](https://huggingface.co/ChaoticNeutrals/Prima-LelantaclesV7-experimental-7b)\\n\\n### Configuration\\n\\nThe following YAML configuration was used to produce this model:\\n\\n```yaml\\nslices:\\n  - ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': 's:\\n      - model: s3nh/SeverusWestLake-7B-DPO\\n        layer_range: [0, 32]\\n      - model: ChaoticNeutrals/Prima-LelantaclesV7-experimental-7b\\n        layer_range: [0, 32]\\nmerge_method: slerp\\nbase_model: s3nh/SeverusWestLake-7B-DPO\\nparameters:\\n  t:\\n    - filter: self_attn\\n      value: [0, 0.5, 0.3, 0.7, 1]\\n    - filter: mlp\\n      value: [1, 0.5, 0.7, 0.3, 0]\\n    - value: 0.5\\ndtype: bfloat16\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Lelanta-lake-7b, text-generation, en, license:other, region:us', 'type': 'text'}], 'name': [{'text': 'duyntnet/Lelanta-lake-7b-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67f082a3bc3c47fb4f629e1c', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349409398, 'repoName': 'Viking-7B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Viking-7B, text-generation, en, license:other, region:us', 'name': 'duyntnet/Viking-7B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Viking-7B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/LumiOpen/Viking-7B\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nViking 7B is a 7B parameter decoder-only transformer pretrained on Finnish,\\nEnglish, Swedish, Danish, Norwegian, Icelandic and code. It has been trained\\non 2 trillion tokens. Viking 7B is a fully ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' model and is made available under the Apache 2.0 License.\\n\\nViking was created in a collaboration between the [TurkuNLP group](https://turkunlp.org/) of the University of Turku, [SiloGen](https://www.silo.ai/silogen) from [Silo AI](https://www.silo.ai/),and [High Performance Language Technologies](https://hplt-project.org/) (HPLT). Training was conducted on the [LUMI supercomputer](https://www.lumi-supercomputer.eu/), using compute resources generously provided by [CSC](https://csc.fi/) - IT Center for Science, Finland.\\n\\nThis project is part of an ongoing effort to create ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' large language models for non-English and especially low resource languages like Finnish. The mode is fluent in Finnish, English, the Scandinavian languages and capable of basic translation between them. It is also able to understand and generate code.\\n\\n## Model Family\\n\\nViking is the second set of models released by LumiOpen and is available at\\n3 parameter counts:\\n\\n[Viking 7B](https://huggingface.co/LumiOpen/Viking-7B)\\n\\n[Viking 13B](https://huggingface.co/LumiOpen/Viking-13B)\\n\\n[Viking 33B](https://huggingface.co/LumiOpen/Viking-33B)', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Viking-7B, text-generation, en, license:other, region:us', 'type': 'text'}], 'name': [{'text': 'duyntnet/Viking-7B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67f57b45a5790daee4c5d5d8', 'repoOwnerId': '671ea7d5c28c3afeae6d2d98', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 14, 'updatedAt': 1759349426026, 'repoName': 'Noesis-1.0-bf16', 'repoOwner': 'RainbirdAI', 'tags': 'mlx, safetensors, llama, en, base_model:mlx-community/Llama-3.3-70B-Instruct-bf16, base_model:finetune:mlx-community/Llama-3.3-70B-Instruct-bf16, license:llama3.3, region:us', 'name': 'RainbirdAI/Noesis-1.0-bf16', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Noesis-1.0-bf16', 'type': 'text'}], 'repoOwner': [{'text': 'RainbirdAI', 'type': 'text'}], 'fileContent': [{'text': '\\n# RainbirdAI/Noesis-1.0-8bit\\n\\n## Model Overview\\n\\n**Noesis** is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \", fine-tuned large language model with accompanying APIs and examples, designed to convert unstructured documentation into structured, executable knowledge graphs compatible with the Rainbird symbolic reasoning engine.\\n\\nBy automating the creation of ontologies and inference rules from textual data, Noesis creates deterministic, explainable AI guardrails that seamlessly integrate into existing machine learning pipelines.\\n\\nThis initial release is optimized for Apple's MLX framework, with support for additional platforms coming very soon.\\n\\n## About Rainbird\\n[Rainbird](https://www.rainbird.ai) is the enterprise platform that backs Noesis. It includes our powerful symbolic inference engine that can reason over your graphs, and management tools that enable you to visualise, amend, test, version and report against them. While you do not need to use the Rainbird management platform, it will enable you to see and work more easily with the graphs that Noesis has produced. \\n\\nThe Rainbird Community platform is free for non-commercial purposes. It provides you the ability to visually inspect your Noesis-generated graphs as well as test and amend, version, deploy and report against your graphs. It also includes access to valuable training resources. \\n\\nYou can sign up for free access to Rainbird Comunity [here](https://rainbird.ai/rainbird-community/).\\n\\n\\nOur solution engineers can provide dedicated support for integrating Noesis into high-stakes applications, ensuring your AI systems meet regulatory standards while maintaining the development velocity modern organizations demand.  \\n\\nIf you have specific questions, please [contact us](https://rainbird.ai/contact-us/).\\n\\n## Why Noesis & Rainbird\\n\\nWe solve the fundamental challenge of LLM hallucinations and inconsistency by transforming organizational documentation and expertise into sophisticated knowledge graphs that deliver precise, explainable outcomes every-time when reasoned over by our symbolic reasoning engine. We combine the linguistic flexibility of LLMs with graph-based deterministic reasoning that ensures every decision follows explicit regulation, policy or procedure. \\n\\nThe two patterns of Rainbird deployment are: 1) Graph-First Reasoning, where the deterministic engine makes decisions and the LLM handles communication, and 2) Post-Generation Validation, where the LLM generates responses that are verified by the deterministic engine before delivery.\\n\\nA white paper that describes typical deployment patterns is available [here](https://rainbird.ai/wp-content/uploads/2025/03/Deterministic-Graph-Based-Inference-for-Guardrailing-Large-Language-Models.pdf). \\n\\nAs a developer, you'll benefit from being able to automate knowledge graph creation and integrate deterministic reasoning into your AI applications in minutes, not weeks.\\n\\n- **Automated Knowledge Capture**: Convert regulations, policies, and tacit expertise into deterministic models without manual knowledge engineering or coding\\n- **Deterministic Reasoning**: Eliminate probabilistic uncertainty by enabling LLM-generated outputs to be passed through a sophisticated deterministic guardrail, or alternatively use it as the primary reasoner. \\n- **Causal Explainability**: Provide complete audit trails that detail how each decision was reached\\n- **Integration Flexibility**: Seamlessly incorporate into existing workflows and systems\\n- **Developer-First Approach**: \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e foundation with a free tier of the Rainbird platform backed by enterprise upgrade options \\n\\n## Python API & Pipeline Integration\\n\\nTo facilitate integration into Python-based workflows, Noesis is accompanied by a modular pipeline framework available at [NoesisAPI](https://github.com/benjamta/NoesisAPI/blob/main/rainbird/README.md). This framework allows developers to:\\n\\n- **Pre and Post Processing**: Use LLMs to prepare a large body of text for Noesis processing, and to validate the graph produced.\\n- **Command line tooling**: Comand line tools to take PDF documentation and create \\n- **Integrate with Rainbird**: Send data to the Rainbird API for knowledge graph generation.\\n- **Extend Functionality**: Create custom processing steps to handle unique requirements.\\n\\n### Installation\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/benjamta/NoesisAPI.git\\ncd NoesisAPI\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Install the package in development mode\\npip install -e .\\n```\\n\\n### Quick Start Example\\n\\n```python\\nfrom rainbird import Noesis\\n\\n# Create a pipeline with custom configuration\\napi = Noesis(config={\\n    \"noesis_model\": \"path/to/model\",\\n    \"temperature\": 0.7,\\n    \"max_tokens\": 5000\\n})\\n\\n# Configure and process text\\napi.configure()\\nresult = api.process(\"People speak the language of the country they\\'re born in\")\\nprint(result)\\n```\\n\\nThis example demonstrates how to process a simple sentence into a knowledge graph using the default pipeline configuration.\\n\\n## Command Line tooling\\n\\nWe also provide a command line tool for procesisng PDF files through the Noesis pipeline, resulting in a new graph in your Rainbrid account ready for you to use.\\n\\nThis application can be configured to use a local LLM for pre and post processing, or can use the [Anthropic API](https://www.anthropic.com/api).\\nYou will need your Rainbrd API key (available from you Rainbird Studio account) to use this tool.\\n\\n## Installation\\n\\n1. Clone the repository and install requirements:\\n```bash\\n# Clone the repository\\ngit clone https://github.com/benjamta/NoesisAPI.git\\ncd NoesisAPI\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Install the package in development mode\\npip install -e .\\n```\\n\\n2. Create a `.env` file in the project root directory and add your Rainbird API key and optionally your Anthropic API key. THe Anthropic key can be omitted if you\\'re using a lcoak LLM for pre and post processing:\\n```\\nRAINBIRD_API_KEY=you_rainbird_api_key_here\\nANTHROPIC_API_KEY=your_api_key_here\\n```\\n\\n3. The `config.yaml` file in the project root directory includes sample configuration settings. You should configure this for your setup.\\n\\n\\n## Usage\\n\\nBasic usage:\\n```bash\\npython noesis_process.py path/to/your/file.pdf\\n```\\n\\n### Command Line Options\\n\\n- `--config`: Specify a custom config file (default: config.yaml)\\n- `--anthropic-api-key`: Override the Anthropic API key\\n- `--rainbird-anthropic-api-key`: Specify a different API key for Rainbird error correction\\n- `--graph-name`: Override the graph name template\\n\\nFor each step (noesis, preprocess, validate, rainbird), you can override:\\n- `--{step}-model`: Model path/name\\n- `--{step}-type`: Model type (local or anthropic)\\n- `--{step}-anthropic-model`: Anthropic model name\\n- `--{step}-adapter-path`: Adapter path\\n- `--{step}-temperature`: Temperature setting\\n- `--{step}-max-tokens`: Maximum tokens\\n\\n## Platform Support\\n\\n- **Current**: Apple MLX\\n- **Upcoming**: Support for additional platforms is under development and will be released in future updates.\\n\\n## Resources\\n\\n- [Rainbird Noesis Overview](https://www.rainbird.ai/rainbird-noesis)\\n- [Rainbird API Documentation](https://rainbird.redoc.ly)\\n- [NoesisAPI GitHub Repository](https://github.com/benjamta/NoesisAPI)', 'type': 'text'}], 'tags': [{'text': 'mlx, safetensors, llama, en, base_model:mlx-community/Llama-3.3-70B-Instruct-bf16, base_model:finetune:mlx-community/Llama-3.3-70B-Instruct-bf16, license:llama3.3, region:us', 'type': 'text'}], 'name': [{'text': 'RainbirdAI/Noesis-1.0-bf16', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '671ea7d5c28c3afeae6d2d98', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6311b8f9ae8896941da22dfc/wU2cYivn5J1FLs3ERHApN.png', 'fullname': 'Rainbird Technologies', 'name': 'RainbirdAI', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': True, 'followerCount': 5}}, {'repoId': '67f6e7e5988cae4f4ef383c2', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349430795, 'repoName': 'cogito-v1-preview-llama-3B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, cogito-v1-preview-llama-3B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/cogito-v1-preview-llama-3B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'cogito-v1-preview-llama-3B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/deepcogito/cogito-v1-preview-llama-3B\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nThe Cogito LLMs are instruction tuned generative models (text in/text out). All models are released under an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' license for commercial use.\\n\\n- Cogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\\n- The LLMs are trained using **Iterated Distillation and Amplification (IDA)** - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\\n- The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\\n  - In both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks. \\n- Each model is trained in over 30 languages and supports a context length of 128k.\\n\\n\\n# Usage\\nHere is a snippet below for usage with Transformers:\\n\\n```python\\nimport transformers\\nimport torch\\n\\nmodel_id = \"deepcogito/cogito-v1-preview-llama-3B\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to LLMs.\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=512,\\n)\\n\\nprint(outputs[0][\"generated_text\"][-1])\\n```\\n\\n\\n\\n## Implementing extended thinking\\n- By default, the model will answer in the standard mode. \\n- To enable thinking, you can do any one of the two methods:\\n  - Add a specific system prompt, or \\n  - Set `enable_thinking=True` while applying the chat template.\\n\\n> **_NOTE:_**  For the Cogito 3B model, we suggest using `repetition_penalty=1.1` while implementing extended thinking.\\n\\n### Method 1 - Add a specific system prompt.\\nTo enable thinking, simply use this in the system prompt `system_instruction = \\'Enable deep thinking subroutine.\\'`\\n\\nIf you already have a system_instruction, then use `system_instruction = \\'Enable deep thinking subroutine.\\' + \\'\\\\n\\\\n\\' + system_instruction`.\\n\\nHere is an example - \\n\\n```python\\nimport transformers\\nimport torch\\n\\nmodel_id = \"deepcogito/cogito-v1-preview-llama-3B\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nDEEP_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": DEEP_THINKING_INSTRUCTION},\\n    {\"role\": \"user\", \"content\": \"Write a bash script that takes a matrix represented as a string with format \\'[1,2],[3,4],[5,6]\\' and prints the transpose in the same format.\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=512,\\n)\\n\\nprint(outputs[0][\"generated_text\"][-1])\\n```\\n\\n\\nSimilarly, if you have a system prompt, you can append the `DEEP_THINKING_INSTRUCTION` to the beginning in this way - \\n\\n```python\\nDEEP_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\\n\\nsystem_prompt = \"Reply to each prompt with only the actual code - no explanations.\"\\nprompt = \"Write a bash script that takes a matrix represented as a string with format \\'[1,2],[3,4],[5,6]\\' and prints the transpose in the same format.\"\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": DEEP_THINKING_INSTRUCTION + \\'\\\\n\\\\n\\' + system_prompt},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\n```\\n\\n### Method 2 - Set enable_thinking=True in the tokenizer\\nIf you are using Huggingface tokenizers, then you can simply use add the argument `enable_thinking=True` to the tokenization (this option is added to the chat template).\\n\\nHere is an example - \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"deepcogito/cogito-v1-preview-llama-3B\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"Give me a short introduction to LLMs.\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\n\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n    enable_thinking=True\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=512\\n)\\ngenerated_ids = [\\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\\n]\\n\\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\nprint(response)\\n```\\n\\n# Tool Calling\\nCogito models support tool calling (single, parallel, multiple and parallel_multiple) both in standard and extended thinking mode.\\n\\nHere is a snippet -\\n\\n```python\\n# First, define a tool\\ndef get_current_temperature(location: str) -> float:\\n    \"\"\"\\n    Get the current temperature at a location.\\n    \\n    Args:\\n        location: The location to get the temperature for, in the format \"City, Country\"\\n    Returns:\\n        The current temperature at the specified location in the specified units, as a float.\\n    \"\"\"\\n    return 22.  # A real function should probably actually get the temperature!\\n\\n# Next, create a chat and apply the chat template\\nmessages = [\\n  {\"role\": \"user\", \"content\": \"Hey, what\\'s the temperature in Paris right now?\"}\\n]\\n\\nmodel_inputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\\n\\ntext = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True, tokenize=False)\\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\\noutputs = model.generate(**inputs, max_new_tokens=512)\\noutput_text = tokenizer.batch_decode(outputs)[0][len(text):]\\nprint(output_text)\\n```\\n\\nThis will result in the output - \\n```\\n<tool_call>\\n{\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\\n</tool_call><|eot_id|>\\n```\\n\\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\\n\\n```python\\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\\n```\\n\\nand then call the tool and append the result, with the `tool` role, like so:\\n\\n```python\\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\\n```\\n\\nAfter that, you can `generate()` again to let the model use the tool result in the chat:\\n\\n```python\\ntext = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True, tokenize=False)\\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\\noutputs = model.generate(**inputs, max_new_tokens=512)\\noutput_text = tokenizer.batch_decode(outputs)[0][len(text):]\\n```\\n\\nThis should result in the string -\\n```\\n\\'The current temperature in Paris is 22.0 degrees.<|eot_id|>\\'\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, cogito-v1-preview-llama-3B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/cogito-v1-preview-llama-3B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67f82ed3969d96be6d5a08cc', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349435328, 'repoName': 'cogito-v1-preview-llama-8B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, cogito-v1-preview-llama-8B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/cogito-v1-preview-llama-8B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'cogito-v1-preview-llama-8B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/deepcogito/cogito-v1-preview-llama-8B\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nThe Cogito LLMs are instruction tuned generative models (text in/text out). All models are released under an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' license for commercial use.\\n\\n- Cogito models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).\\n- The LLMs are trained using **Iterated Distillation and Amplification (IDA)** - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.\\n- The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts.\\n  - In both standard and reasoning modes, Cogito v1-preview models outperform their size equivalent counterparts on common industry benchmarks. \\n- Each model is trained in over 30 languages and supports a context length of 128k.\\n\\n\\n# Usage\\nHere is a snippet below for usage with Transformers:\\n\\n```python\\nimport transformers\\nimport torch\\n\\nmodel_id = \"deepcogito/cogito-v1-preview-llama-8B\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": \"Give me a short introduction to LLMs.\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=512,\\n)\\n\\nprint(outputs[0][\"generated_text\"][-1])\\n```\\n\\n\\n\\n## Implementing extended thinking\\n- By default, the model will answer in the standard mode. \\n- To enable thinking, you can do any one of the two methods:\\n  - Add a specific system prompt, or \\n  - Set `enable_thinking=True` while applying the chat template.\\n\\n\\n### Method 1 - Add a specific system prompt.\\nTo enable thinking, simply use this in the system prompt `system_instruction = \\'Enable deep thinking subroutine.\\'`\\n\\nIf you already have a system_instruction, then use `system_instruction = \\'Enable deep thinking subroutine.\\' + \\'\\\\n\\\\n\\' + system_instruction`.\\n\\nHere is an example - \\n\\n```python\\nimport transformers\\nimport torch\\n\\nmodel_id = \"deepcogito/cogito-v1-preview-llama-8B\"\\n\\npipeline = transformers.pipeline(\\n    \"text-generation\",\\n    model=model_id,\\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\\n    device_map=\"auto\",\\n)\\n\\nDEEP_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": DEEP_THINKING_INSTRUCTION},\\n    {\"role\": \"user\", \"content\": \"Write a bash script that takes a matrix represented as a string with format \\'[1,2],[3,4],[5,6]\\' and prints the transpose in the same format.\"},\\n]\\n\\noutputs = pipeline(\\n    messages,\\n    max_new_tokens=512,\\n)\\n\\nprint(outputs[0][\"generated_text\"][-1])\\n```\\n\\n\\nSimilarly, if you have a system prompt, you can append the `DEEP_THINKING_INSTRUCTION` to the beginning in this way - \\n\\n```python\\nDEEP_THINKING_INSTRUCTION = \"Enable deep thinking subroutine.\"\\n\\nsystem_prompt = \"Reply to each prompt with only the actual code - no explanations.\"\\nprompt = \"Write a bash script that takes a matrix represented as a string with format \\'[1,2],[3,4],[5,6]\\' and prints the transpose in the same format.\"\\n\\nmessages = [\\n    {\"role\": \"system\", \"content\": DEEP_THINKING_INSTRUCTION + \\'\\\\n\\\\n\\' + system_prompt},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\n```\\n\\n### Method 2 - Set enable_thinking=True in the tokenizer\\nIf you are using Huggingface tokenizers, then you can simply use add the argument `enable_thinking=True` to the tokenization (this option is added to the chat template).\\n\\nHere is an example - \\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"deepcogito/cogito-v1-preview-llama-8B\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"Give me a short introduction to LLMs.\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\n\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n    enable_thinking=True\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=512\\n)\\ngenerated_ids = [\\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\\n]\\n\\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\nprint(response)\\n```\\n\\n# Tool Calling\\nCogito models support tool calling (single, parallel, multiple and parallel_multiple) both in standard and extended thinking mode.\\n\\nHere is a snippet -\\n\\n```python\\n# First, define a tool\\ndef get_current_temperature(location: str) -> float:\\n    \"\"\"\\n    Get the current temperature at a location.\\n    \\n    Args:\\n        location: The location to get the temperature for, in the format \"City, Country\"\\n    Returns:\\n        The current temperature at the specified location in the specified units, as a float.\\n    \"\"\"\\n    return 22.  # A real function should probably actually get the temperature!\\n\\n# Next, create a chat and apply the chat template\\nmessages = [\\n  {\"role\": \"user\", \"content\": \"Hey, what\\'s the temperature in Paris right now?\"}\\n]\\n\\nmodel_inputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\\n\\ntext = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True, tokenize=False)\\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\\noutputs = model.generate(**inputs, max_new_tokens=512)\\noutput_text = tokenizer.batch_decode(outputs)[0][len(text):]\\nprint(output_text)\\n```\\n\\nThis will result in the output - \\n```\\n<tool_call>\\n{\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\\n</tool_call><|eot_id|>\\n```\\n\\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\\n\\n```python\\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\\n```\\n\\nand then call the tool and append the result, with the `tool` role, like so:\\n\\n```python\\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\\n```\\n\\nAfter that, you can `generate()` again to let the model use the tool result in the chat:\\n\\n```python\\ntext = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True, tokenize=False)\\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\\noutputs = model.generate(**inputs, max_new_tokens=512)\\noutput_text = tokenizer.batch_decode(outputs)[0][len(text):]\\n```\\n\\nThis should result in the string -\\n```\\n\\'The current temperature in Paris is 22.0 degrees.<|eot_id|>\\'\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, cogito-v1-preview-llama-8B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/cogito-v1-preview-llama-8B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '67f8f3b0aa09e847b1c8d555', 'repoOwnerId': '671ea7d5c28c3afeae6d2d98', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 14, 'updatedAt': 1759349438400, 'repoName': 'Noesis-1.0-8bit', 'repoOwner': 'RainbirdAI', 'tags': 'mlx, safetensors, llama, en, base_model:mlx-community/Llama-3.3-70B-Instruct-bf16, base_model:quantized:mlx-community/Llama-3.3-70B-Instruct-bf16, license:llama3.3, 8-bit, region:us', 'name': 'RainbirdAI/Noesis-1.0-8bit', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Noesis-1.0-8bit', 'type': 'text'}], 'repoOwner': [{'text': 'RainbirdAI', 'type': 'text'}], 'fileContent': [{'text': '\\n# RainbirdAI/Noesis-1.0-8bit\\n\\n## Model Overview\\n\\n**Noesis** is an ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': \", fine-tuned large language model with accompanying APIs and examples, designed to convert unstructured documentation into structured, executable knowledge graphs compatible with the Rainbird symbolic reasoning engine.\\n\\nBy automating the creation of ontologies and inference rules from textual data, Noesis creates deterministic, explainable AI guardrails that seamlessly integrate into existing machine learning pipelines.\\n\\nThis initial release is optimized for Apple's MLX framework, with support for additional platforms coming very soon.\\n\\n## About Rainbird\\n[Rainbird](https://www.rainbird.ai) is the enterprise platform that backs Noesis. It includes our powerful symbolic inference engine that can reason over your graphs, and management tools that enable you to visualise, amend, test, version and report against them. While you do not need to use the Rainbird management platform, it will enable you to see and work more easily with the graphs that Noesis has produced. \\n\\nThe Rainbird Community platform is free for non-commercial purposes. It provides you the ability to visually inspect your Noesis-generated graphs as well as test and amend, version, deploy and report against your graphs. It also includes access to valuable training resources. \\n\\nYou can sign up for free access to Rainbird Comunity [here](https://rainbird.ai/rainbird-community/).\\n\\n\\nOur solution engineers can provide dedicated support for integrating Noesis into high-stakes applications, ensuring your AI systems meet regulatory standards while maintaining the development velocity modern organizations demand.  \\n\\nIf you have specific questions, please [contact us](https://rainbird.ai/contact-us/).\\n\\n## Why Noesis & Rainbird\\n\\nWe solve the fundamental challenge of LLM hallucinations and inconsistency by transforming organizational documentation and expertise into sophisticated knowledge graphs that deliver precise, explainable outcomes every-time when reasoned over by our symbolic reasoning engine. We combine the linguistic flexibility of LLMs with graph-based deterministic reasoning that ensures every decision follows explicit regulation, policy or procedure. \\n\\nThe two patterns of Rainbird deployment are: 1) Graph-First Reasoning, where the deterministic engine makes decisions and the LLM handles communication, and 2) Post-Generation Validation, where the LLM generates responses that are verified by the deterministic engine before delivery.\\n\\nA white paper that describes typical deployment patterns is available [here](https://rainbird.ai/wp-content/uploads/2025/03/Deterministic-Graph-Based-Inference-for-Guardrailing-Large-Language-Models.pdf). \\n\\nAs a developer, you'll benefit from being able to automate knowledge graph creation and integrate deterministic reasoning into your AI applications in minutes, not weeks.\\n\\n- **Automated Knowledge Capture**: Convert regulations, policies, and tacit expertise into deterministic models without manual knowledge engineering or coding\\n- **Deterministic Reasoning**: Eliminate probabilistic uncertainty by enabling LLM-generated outputs to be passed through a sophisticated deterministic guardrail, or alternatively use it as the primary reasoner. \\n- **Causal Explainability**: Provide complete audit trails that detail how each decision was reached\\n- **Integration Flexibility**: Seamlessly incorporate into existing workflows and systems\\n- **Developer-First Approach**: \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e foundation with a free tier of the Rainbird platform backed by enterprise upgrade options \\n\\n## Python API & Pipeline Integration\\n\\nTo facilitate integration into Python-based workflows, Noesis is accompanied by a modular pipeline framework available at [NoesisAPI](https://github.com/benjamta/NoesisAPI/blob/main/rainbird/README.md). This framework allows developers to:\\n\\n- **Pre and Post Processing**: Use LLMs to prepare a large body of text for Noesis processing, and to validate the graph produced.\\n- **Command line tooling**: Comand line tools to take PDF documentation and create \\n- **Integrate with Rainbird**: Send data to the Rainbird API for knowledge graph generation.\\n- **Extend Functionality**: Create custom processing steps to handle unique requirements.\\n\\n### Installation\\n\\n```bash\\n# Clone the repository\\ngit clone https://github.com/benjamta/NoesisAPI.git\\ncd NoesisAPI\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Install the package in development mode\\npip install -e .\\n```\\n\\n### Quick Start Example\\n\\n```python\\nfrom rainbird import Noesis\\n\\n# Create a pipeline with custom configuration\\napi = Noesis(config={\\n    \"noesis_model\": \"path/to/model\",\\n    \"temperature\": 0.7,\\n    \"max_tokens\": 5000\\n})\\n\\n# Configure and process text\\napi.configure()\\nresult = api.process(\"People speak the language of the country they\\'re born in\")\\nprint(result)\\n```\\n\\nThis example demonstrates how to process a simple sentence into a knowledge graph using the default pipeline configuration.\\n\\n## Command Line tooling\\n\\nWe also provide a command line tool for procesisng PDF files through the Noesis pipeline, resulting in a new graph in your Rainbrid account ready for you to use.\\n\\nThis application can be configured to use a local LLM for pre and post processing, or can use the [Anthropic API](https://www.anthropic.com/api).\\nYou will need your Rainbrd API key (available from you Rainbird Studio account) to use this tool.\\n\\n## Installation\\n\\n1. Clone the repository and install requirements:\\n```bash\\n# Clone the repository\\ngit clone https://github.com/benjamta/NoesisAPI.git\\ncd NoesisAPI\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Install the package in development mode\\npip install -e .\\n```\\n\\n2. Create a `.env` file in the project root directory and add your Rainbird API key and optionally your Anthropic API key. THe Anthropic key can be omitted if you\\'re using a lcoak LLM for pre and post processing:\\n```\\nRAINBIRD_API_KEY=you_rainbird_api_key_here\\nANTHROPIC_API_KEY=your_api_key_here\\n```\\n\\n3. The `config.yaml` file in the project root directory includes sample configuration settings. You should configure this for your setup.\\n\\n\\n## Usage\\n\\nBasic usage:\\n```bash\\npython noesis_process.py path/to/your/file.pdf\\n```\\n\\n### Command Line Options\\n\\n- `--config`: Specify a custom config file (default: config.yaml)\\n- `--anthropic-api-key`: Override the Anthropic API key\\n- `--rainbird-anthropic-api-key`: Specify a different API key for Rainbird error correction\\n- `--graph-name`: Override the graph name template\\n\\nFor each step (noesis, preprocess, validate, rainbird), you can override:\\n- `--{step}-model`: Model path/name\\n- `--{step}-type`: Model type (local or anthropic)\\n- `--{step}-anthropic-model`: Anthropic model name\\n- `--{step}-adapter-path`: Adapter path\\n- `--{step}-temperature`: Temperature setting\\n- `--{step}-max-tokens`: Maximum tokens\\n\\n## Platform Support\\n\\n- **Current**: Apple MLX\\n- **Upcoming**: Support for additional platforms is under development and will be released in future updates.\\n\\n## Resources\\n\\n- [Rainbird Noesis Overview](https://www.rainbird.ai/rainbird-noesis)\\n- [Rainbird API Documentation](https://rainbird.redoc.ly)\\n- [NoesisAPI GitHub Repository](https://github.com/benjamta/NoesisAPI)', 'type': 'text'}], 'tags': [{'text': 'mlx, safetensors, llama, en, base_model:mlx-community/Llama-3.3-70B-Instruct-bf16, base_model:quantized:mlx-community/Llama-3.3-70B-Instruct-bf16, license:llama3.3, 8-bit, region:us', 'type': 'text'}], 'name': [{'text': 'RainbirdAI/Noesis-1.0-8bit', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '671ea7d5c28c3afeae6d2d98', 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6311b8f9ae8896941da22dfc/wU2cYivn5J1FLs3ERHApN.png', 'fullname': 'Rainbird Technologies', 'name': 'RainbirdAI', 'type': 'org', 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'isEnterprise': True, 'followerCount': 5}}, {'repoId': '67f9a90b8ee69d220e185f60', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349440750, 'repoName': 'Hamanasu-15B-Instruct-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Hamanasu-15B-Instruct, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Hamanasu-15B-Instruct-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Hamanasu-15B-Instruct-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Delta-Vector/Hamanasu-15B-Instruct\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\n<i>After multiple days of training, I\\'m proud to showcase my very own Phi-4 Finetune, Pretrained on almost a billion tokens worth of Books from</i>\\n\\n- `NewEden/Orion-LIT`\\n- `NewEden/Orion-Asstr-Stories-16K`\\n- `Mielikki/Erebus-87k`\\n\\n<i>Then instruct tuned. This model shows great promise for roleplaying while keeping things fresh and new. (And most of, Unsloppy!)</i>\\n\\n</div>\\n\\n<div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0;\">\\n\\n<div style=\"border:2px solid #333; border-radius:10px; padding:20px; background: rgba(0,0,0,0.2);\">\\n\\n\\n## üí∞ Prompting\\n\\n<i>A known quirk of the model is overly verbose responses when generation length is uncapped, Please cap your maximum output tokens to 100~ tokens above what you prefer</i>\\n\\n\\n<i>This model uses ChatML formatting</i>\\n```python\\n<|im_start|>system\\nYou are an uncensored AI, your job is to fulfill thy will of thy user.<|im_end|>\\n<|im_start|>User request\\nTake off your helmet.<|im_end|>\\n<|im_start|>No i shall not. This is the way.\\n```\\n</div>\\n\\n\\n', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Hamanasu-15B-Instruct, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Hamanasu-15B-Instruct-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '680081199f02164ded9d386f', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349463413, 'repoName': 'DeepCoder-14B-Preview-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, DeepCoder-14B-Preview, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/DeepCoder-14B-Preview-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'DeepCoder-14B-Preview-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/agentica-org/DeepCoder-14B-Preview\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nDeepCoder-14B-Preview is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-14B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 60.6% Pass@1 accuracy on LiveCodeBench v5 (8/1/24-2/1/25), representing a 8% improvement over the base model (53%) and achieving similar performance to OpenAI\\'s o3-mini with just 14B parameters.\\n\\n<div style=\"margin: 0 auto;\">\\n  <img src=\"https://cdn-uploads.huggingface.co/production/uploads/654037be97949fd2304aab7f/r3-vzkItOCrMf1qldW0Mj.png\" style=\"width: 100%;\" />\\n</div>\\n\\n## Data\\nOur training dataset consists of approximately 24K unique problem-tests pairs compiled from:\\n- Taco-Verified\\n- PrimeIntellect SYNTHETIC-1\\n- LiveCodeBench v5 (5/1/23-7/31/24)\\n\\n## Training Recipe\\n\\nOur training recipe relies on an improved version of GRPO (GRPO+) and iterative context lengthening, introduced in DeepScaleR.\\n\\n### GRPO+\\n\\nWe enhance the original GRPO algorithm with insights from DAPO to enable more stable training:\\n\\n- **Offline Difficulty Filtering:** DAPO employs online dynamic sampling, discarding both entirely correct and entirely incorrect samples on the fly. While this helps maintain a more stable effective batch size, it introduces significant runtime overhead due to rejection sampling. Instead, we perform offline difficulty filtering on a subset of coding problems to ensure the training dataset remains within a suitable difficulty range.\\n- **No Entropy Loss:** We observed that including an entropy loss term often led to instability, with entropy growing exponentially and ultimately collapsing training. To mitigate this, we eliminate the entropy loss entirely.\\n- **No KL Loss:** Eliminating KL loss prevents the LLM from staying within trust region of the original SFT model. This removal also obviates the need to compute log probabilities for the reference policy, thereby accelerating training.\\n- **Overlong Filtering** **(from DAPO):** To preserve long-context reasoning, we mask the loss for truncated sequences. This technique enables DeepCoder to generalize to 64K-context inference despite being trained with a 32K context.\\n- **Clip High (from DAPO):** By increasing the upper bound in GRPO/PPO‚Äôs surrogate loss, we encourage more exploration and more stable entropy.\\n\\n### Iterative Context Lengthening\\n\\nOur original `Deepscaler-1.5B-Preview` scaled long context training from 8K‚Üí16K‚Üí24K, achieving 33‚Üí38‚Üí43% on AIME respectively. Similarly, `Deepcoder-14B-Preview` is trained on 16K‚Üí32K, achieving 54‚Üí58% on LiveCodeBench (v5). `DeepCoder-14B-Preview` successfully generalizes to longer contexts when evaluated at 64K context, reaching 60.6%. \\n\\nDeepCoder generalizes better to long contexts than the base distilled model, due to DAPO\\'s overlong filtering. However, it\\'s longer responses are often truncated when the max length is capped at 16K, which can lower its scores.\\n\\n| **Model** | **16K** | **32K** | **64K** |\\n| --- | --- | --- | --- |\\n| **DeepCoder-14B-Preview** | 45.6 | 57.9 | 60.6 |\\n| **DeepSeek-R1-Distill-Qwen-14B** | 50.2 | 53.0 | 53.0 |\\n\\nA more detailed description of the training recipe can be found in our [blog post](https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51).\\n\\n## Evaluation\\n\\nWe evaluate `Deepcoder-14B-Preview` on various coding benchmarks, including LiveCodeBench (LCBv5), Codeforces, and HumanEval+. \\n\\n| **Model** | LCB (v5)(8/1/24-2/1/25) | Codeforces Rating | Codeforces Percentile | HumanEval+ |\\n| --- | --- | --- | --- | --- |\\n| **DeepCoder-14B-Preview (ours)** | ***60.6*** | ***1936*** | ***95.3*** | ***92.6*** |\\n| **DeepSeek-R1-Distill-Qwen-14B** | 53.0 | 1791 | 92.7 | 92.0 |\\n| **O1-2024-12-17 (Low)** | 59.5 | **1991** | **96.1** | 90.8 |\\n| **O3-Mini-2025-1-31 (Low)** | **60.9** | 1918 | 94.9 | 92.6 |\\n| **O1-Preview** | 42.7 | 1658 | 88.5 | 89 |\\n| **Deepseek-R1** | 62.8 | 1948 | 95.4 | 92.6 |\\n| **Llama-4-Behemoth** | 49.4 | - | - | - |\\n\\n## Serving DeepCoder\\nOur model can be served using popular high-performance inference systems:\\n- vLLM\\n- Hugging Face Text Generation Inference (TGI)\\n- SGLang\\n- TensorRT-LLM\\n\\nAll these systems support the OpenAI Chat Completions API format.\\n\\n### Usage Recommendations\\nOur usage recommendations are similar to those of R1 and R1 Distill series:\\n1. Avoid adding a system prompt; all instructions should be contained within the user prompt.\\n2. `temperature = 0.6`\\n3. `top_p = 0.95`\\n4. This model performs best with `max_tokens` set to at least `64000` ', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, DeepCoder-14B-Preview, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/DeepCoder-14B-Preview-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '680268b971162b75ced24e0a', 'repoOwnerId': '67c32c3f99fba7b00e8e4f09', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 0, 'updatedAt': 1759349471022, 'repoName': 'awesome-postman-alternatives', 'repoOwner': 'furudo-erika', 'tags': 'region:us', 'name': 'furudo-erika/awesome-postman-alternatives', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'awesome-postman-alternatives', 'type': 'text'}], 'repoOwner': [{'text': 'furudo-erika', 'type': 'text'}], 'fileContent': [{'text': '# Awesome API Clients as Postman Alternatives (', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' & Free)\\n\\n## Awesome API Clients as Postman Alternatives\\n\\nIn the dynamic world of software development, Application Programming Interfaces (APIs) are the vital conduits connecting disparate services, applications, and data ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': 's. They are the engines of modern digital experiences. Consequently, the tools used to design, develop, debug, test, and manage these APIs are fundamental to developer productivity and application quality. For many years, **Postman** established itself as a dominant player in this space, offering a comprehensive suite of features for interacting with APIs. **Insomnia** also gained significant traction as a popular, user-friendly alternative.\\n\\nHowever, the API client landscape is undergoing a significant shift. Recent changes in the business models and feature strategies of established players, particularly around mandatory **cloud synchronization**, account requirements, pricing tiers, and data privacy concerns, have led many developers and organizations to actively seek **Postman alternatives** and **Insomnia alternatives**. Community discussions ([like this one on Reddit](https://www.reddit.com/r/webdev/comments/16twfkr/kong_pulls_a_postman_causing_exodus_from_insomnia/)) highlight a growing desire for tools that prioritize user control, data privacy, offline access, and freedom from vendor lock-in. Concerns about **feature bloat** in established tools also drive searches for simpler, more focused solutions.\\n\\nThe fantastic news is that the developer community, particularly the **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e** ecosystem, has risen to the challenge. There is now a vibrant and diverse range of high-quality, **free Postman alternatives** that offer powerful features, flexible workflows, and greater control over your data and development environment. Whether you need a **lightweight alternative**, a feature-rich desktop application, a seamless **VS Code extension**, a powerful **CLI** tool, or a dedicated **API testing** framework, there's likely an \", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e solution that fits your needs.\\n\\nThis document serves as a curated list of **Awesome API Clients**, focusing specifically on **', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' and Free Postman Alternatives**. These tools empower developers with the functionality they need for effective API development and testing without the compromises often associated with proprietary, cloud-centric platforms.\\n\\n## Why Seek ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' & Free Postman Alternatives? The Driving Forces\\n\\nThe migration towards ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e API clients isn't merely about avoiding costs; it's often rooted in core principles crucial to developers and modern software practices:\\n\\n1.  **Data Privacy & Control (No Forced Cloud Sync):** Mandatory cloud features in tools like Postman and recent versions of Insomnia raise significant concerns about the storage location of sensitive API keys, tokens, environment variables, and request history. \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e alternatives, especially those offering **offline-first** capabilities (like **Bruno**, Restfox) or **self-hosted** options (like Yaade), provide absolute control, ensuring sensitive data remains entirely within your local machine or trusted infrastructure. This is a paramount concern for many individuals and organizations.\\n2.  **Avoiding Vendor Lock-in:** Relying exclusively on a proprietary platform can create dependencies that make future transitions difficult or expensive. ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': \"e tools often utilize standard, text-based formats for storing collections and environments (e.g., JSON, YAML, or custom formats easily managed with Git like Bruno's Bru Lang), ensuring data portability and preventing lock-in.\\n3.  **Cost-Effectiveness:** While Postman and others offer free tiers, accessing advanced features, enabling collaboration for larger teams, or exceeding usage limits often necessitates paid subscriptions. Many **free** \", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e alternatives provide a comparable, if not superior, feature set without any mandatory costs, democratizing access to powerful API tooling.\\n4.  **Flexibility, Extensibility & Customization:** ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e software can often be inspected, modified, and extended more easily than closed-source counterparts. Some tools (like Milkman) have explicit plugin architectures, while others can be integrated into bespoke workflows through scripting or leveraging their ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' code.\\n5.  **True Offline Access:** Mandatory sign-in and cloud dependency hinder work in environments with restricted or no internet access. Many alternatives listed here are designed to work perfectly offline, ensuring productivity regardless of connectivity.\\n6.  **Simplicity & Focus (Avoiding Feature Bloat):** Some developers feel that established tools have become overly complex (\"feature bloat\"). Many alternatives focus on providing a clean, streamlined experience centered on core API interaction and testing tasks. **Lightweight alternatives** are actively sought after.\\n7.  **Community & Transparency:** ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e projects thrive on community contributions, bug reports, and feature suggestions. Development is transparent, and users can audit the code for security or functionality verification. This collaborative model fosters trust and innovation.\\n\\n## Criteria for Inclusion for Postman Alternatives\\n\\nThis list prioritizes active, community-supported, and genuinely ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e projects relevant for API development and testing:\\n\\n*   **', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ':** The project must possess a publicly accessible ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' code repository under a recognized ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e license (MIT, Apache 2.0, etc.).\\n*   **Free Core Functionality:** The essential features for sending API requests (REST, possibly GraphQL/gRPC), managing environments, and organizing requests (collections) must be available for free, without requiring a paid subscription for typical Postman/Insomnia use cases.\\n*   **Active/Viable:** Generally, projects should exhibit signs of life, such as having more than 200 stars on their primary repository and contributions from at least 2 individuals. This indicates a baseline level of community interest and sustainability. (Exceptions may be made for highly relevant niche tools or established projects like `curl`).\\n\\nThis list is categorized for clarity: **Web-Based**, **Desktop**, **IDE Integration**, **Command-Line (CLI)**, and **Automated API Testing Frameworks**. While some tools might fit into multiple categories (e.g., Requestly), they are listed under their primary or most prominent form.\\n\\n---\\n\\n## Postman Alternatives - Web-Based API Clients\\n\\nAccessible from any modern browser, these clients eliminate installation hassles and often facilitate easier sharing and collaboration. Many leverage browser storage for offline use or offer PWA (Progressive Web App) capabilities.\\n\\n### [Restfox](https://restfox.dev) ([repo](https://github.com/flawiddsouza/Restfox))\\n\\n*   **Keywords:** Offline-first, Web HTTP client, Lightweight alternative, Free Postman alternative.\\n*   **Description:** Restfox directly tackles the cloud-sync concern by being an **offline-first** web client. Your workspaces, requests, and collections are stored locally in your browser's IndexedDB. It provides a clean, fast, and intuitive interface for making HTTP(S) requests, organizing them, managing environments, and scripting. It's designed to be simple yet powerful, offering core functionality without unnecessary complexity. Its emphasis on local storage and speed makes it an excellent free Postman alternative for those prioritizing privacy and performance in a web-based tool.\\n\\n### [Hoppscotch](https://hoppscotch.io) ([repo](https://github.com/hoppscotch/hoppscotch))\\n\\n*   **Keywords:** \", 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' API development ecosystem, REST, GraphQL, WebSocket, PWA, Free Postman alternative, Insomnia alternative, Browser-based.\\n*   **Description:** Frequently cited as a top **Postman alternative** and **Insomnia alternative**, Hoppscotch (formerly Postwoman) is a feature-rich **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e API development ecosystem**. Its sleek web interface supports **REST**, **GraphQL**, **WebSocket**, and Server-Sent Events. Key features include collections, environment variables, pre-request scripts, test scripts (JavaScript sandbox), documentation generation, and collaboration workspaces. It offers flexibility with data storage: local browser storage, optional Hoppscotch cloud sync, or even **self-hosting**. Its PWA support allows for an installable, near-native experience with offline access. Its comprehensive feature set makes it suitable for both individual developers and teams looking for a powerful, **free**, **browser-based** client.\\n\\n### [Firecamp](https://firecamp.io) ([repo](https://github.com/firecamp-dev/firecamp))\\n\\n*   **Keywords:** Multi-protocol, GraphQL, WebSocket, Socket.IO, VS Code DX, ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': ' Postman Alternative.\\n*   **Description:** Firecamp aims to provide a superior Developer Experience (DX) inspired by VS Code. It stands out with its multi-protocol support, handling **REST**, **GraphQL**, **WebSocket**, and **Socket.IO** APIs within a unified interface. This makes it incredibly versatile for projects involving real-time communication alongside traditional APIs. Features include workspaces, collections, scripting, and a focus on collaborative workflows. Its VS Code-like command palette and layout offer familiarity for many developers. Firecamp is a strong **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e Postman alternative** for those needing broad protocol support and appreciating a developer-centric UI.\\n\\n### [gRPC UI](https://github.com/fullstorydev/grpcui)\\n\\n*   **Keywords:** gRPC, Web UI, Interactive, API exploration.\\n*   **Description:** While not a general HTTP client, gRPC UI is an indispensable **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** tool for developers working with **gRPC**. It provides an interactive **web UI** for exploring and invoking gRPC services, much like Postman does for REST. Using gRPC reflection or compiled `.proto` files, it discovers services and methods, allowing users to easily craft requests (including metadata), view responses, and understand gRPC schemas directly in the browser. It significantly simplifies gRPC debugging and interaction compared to using purely CLI tools.\\n\\n### [Yaade](https://docs.yaade.io) ([repo](https://github.com/EsperoTech/yaade))\\n\\n*   **Keywords:** Self-hosted, Collaborative API development, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e, Privacy.\\n*   **Description:** Yaade differentiates itself by being an **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e**, **self-hosted**, collaborative API development environment. This focus on self-hosting is ideal for teams and organizations prioritizing maximum **privacy** and data control, ensuring all API definitions, collections, and secrets remain within their own infrastructure. It provides the essential features for managing requests, collections, and environments, facilitating teamwork in a secure, private setting. Yaade is a compelling choice for those who need collaborative features but want to avoid third-party cloud services entirely.\\n\\n### [Prestige](https://prestige.dev) ([repo](https://github.com/sharat87/prestige))\\n\\n*   **Keywords:** Text-based, In-browser HTTP client, Interface-less, Lightweight alternative.\\n*   **Description:** Prestige offers a unique, **text-based** approach within the browser. It acts as an \"interface-less\" Postman alternative, appealing to developers who prefer defining requests using simple text rather than GUI fields. You write the method, URL, headers, and body directly in a text area, and Prestige executes the request, displaying the response. This minimalist, keyboard-centric workflow can be very efficient and makes sharing request definitions as simple as copying text. It\\'s a **lightweight alternative** for those who value simplicity and text-based interaction.\\n\\n### [Requestly](https://requestly.com) ([repo](https://github.com/requestly/requestly))\\n\\n*   **Keywords:** Browser extension, API Client, API Mocking, Modify HTTP requests, Intercept.\\n*   **Description:** Requestly is a versatile tool available as both a browser extension and a desktop app. Its origins lie in intercepting and modifying HTTP requests (e.g., redirecting URLs, modifying headers/responses, simulating network conditions), which is invaluable for frontend development and debugging. It has expanded to include a capable **API client**, allowing users to send HTTP(S) requests directly. It also features **API mocking**. This combination of interception, modification, mocking, and request sending in one tool makes it powerful, especially for web developers needing deep browser integration or comprehensive debugging capabilities. Its **free** tier offers significant functionality.\\n\\n### [Swagger UI](https://swagger.io/tools/swagger-ui/) ([repo](https://github.com/swagger-api/swagger-ui))\\n\\n*   **Keywords:** OpenAPI, Swagger, API Documentation, Interactive Testing, ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'Source', 'type': 'highlight'}, {'text': \".\\n*   **Description:** While **Swagger UI**'s primary purpose is to render **OpenAPI** (formerly Swagger) specifications as interactive **API documentation**, it serves as a valuable tool for **interactive testing**. It generates a user interface directly from an API definition, allowing developers (and consumers) to explore endpoints, understand request/response models, and execute live API calls directly within the documentation page. It's **\", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** and widely used. Though not a full-fledged API client like Postman (lacking collections management, complex scripting, etc.), its ability to provide an immediate, interactive testing environment based on the API contract makes it an essential part of the API ecosystem and a useful tool for quick checks. Often discussed in \"**Swagger vs. Postman**\" comparisons, they serve different primary roles but overlap in interactive testing.\\n\\n---\\n\\n## Postman Alternatives - Desktop API Clients\\n\\nInstalled natively on your OS (Windows, macOS, Linux), desktop clients often provide superior performance, deeper system integration, and guaranteed **offline** operation.\\n\\n### [Bruno](https://usebruno.com) ([repo](https://github.com/usebruno/bruno))\\n\\n*   **Keywords:** ', 'type': 'text'}, {'text': 'Opensource', 'type': 'highlight'}, {'text': ' IDE for APIs, Offline-first, Git-friendly, Local storage, Privacy, Free Postman alternative, Insomnia alternative, Bruno.\\n*   **Description:** **Bruno** has rapidly emerged as a leading **free Postman alternative** and **Insomnia alternative**, heavily praised in community discussions (**Reddit**, Hacker News). Its core philosophy is **offline-first** operation and storing API collections directly on the local filesystem using Bru Lang, a plain text markup language. This makes collections inherently **Git-friendly**, allowing seamless version control alongside code. Bruno offers a clean UI, REST and GraphQL support, environment variables, declarative scripting (JavaScript), and assertions for **API testing**. Its explicit rejection of mandatory cloud sync strongly appeals to users prioritizing **privacy** and **local storage**. Many see **Bruno** as a modern, fast, and developer-centric **offline** alternative.\\n\\n### [Yaak](https://yaak.app) ([repo](https://github.com/yaakapp/app))\\n\\n*   **Keywords:** Intuitive desktop API client, Lightweight alternative, Cross-platform.\\n*   **Description:** Yaak focuses on being the \"most intuitive\" desktop API client, prioritizing simplicity and ease of use. It provides a clean, polished interface for sending requests, managing environments, and organizing collections across different workspaces. It aims to be a **lightweight alternative** that is quick to learn and efficient for daily API interaction tasks, available for Windows, macOS, and Linux.\\n\\n### [API Dash](https://github.com/foss42/apidash)\\n\\n*   **Keywords:** Cross-platform API Client, Flutter, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e, Beautiful UI.\\n*   **Description:** Built with Flutter, API Dash delivers a beautiful and consistent user experience across Windows, macOS, and Linux. It provides the core functionalities needed for interacting with REST APIs, including request creation, response inspection, collections, and environment management. Its focus on a visually appealing UI and native cross-platform performance makes it an attractive **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e** option.\\n\\n### [ezy](https://www.getezy.dev) ([repo](https://github.com/getezy/ezy))\\n\\n*   **Keywords:** gRPC GUI client, gRPC-Web, Desktop client.\\n*   **Description:** Similar to gRPC UI for the web, ezy is a dedicated desktop GUI client specifically for **gRPC** and **gRPC-Web** services. It simplifies interaction with gRPC endpoints compared to CLI tools, offering features like service discovery (reflection or proto files), request crafting, response viewing, and metadata handling in a user-friendly desktop application. An essential tool for developers heavily invested in the gRPC ecosystem.\\n\\n### [BloomRPC](https://github.com/bloomrpc/bloomrpc)\\n\\n*   **Keywords:** gRPC GUI Client (Unmaintained).\\n*   **Description:** BloomRPC was an early and popular GUI client for **gRPC** services, known for its clean interface based on proto file definitions. **(Important Note: BloomRPC appears to be unmaintained)**. While historically significant, users should consider more actively maintained alternatives like ezy or Evans CLI for ongoing projects. It's listed here for context due to its previous popularity.\\n\\n### [Milkman](https://github.com/warmuuh/milkman)\\n\\n*   **Keywords:** Extensible Request/Response Workbench, Plugin architecture, JavaFX.\\n*   **Description:** Milkman bills itself as an extensible request/response workbench. Built using JavaFX, its key feature is a plugin-based architecture, allowing users to extend its capabilities beyond standard HTTP requests. This makes it suitable for specialized protocols or workflows. It's an **\", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** option for developers needing high customization potential.\\n\\n### [Insomnium](https://github.com/ArchGPT/insomnium)\\n\\n*   **Keywords:** Insomnia fork, Local, Privacy-focused, Offline, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': \"e Insomnia alternative.\\n*   **Description:** Insomnium was created as a direct fork of **Insomnia** specifically to remove the mandatory cloud sync and account requirements introduced by Kong. It aims to provide the familiar Insomnia UX while ensuring all data remains 100% **local** and **private**. It's the quintessential **privacy-focused**, **offline**, **\", 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e Insomnia alternative** for users who liked the original interface but prioritize data locality. **(Note: As with any fork, users should check the repository for ongoing maintenance activity)**.\\n\\n### [Pororoca](https://pororoca.io) ([repo](https://github.com/alexandrehtrb/Pororoca))\\n\\n*   **Keywords:** HTTP inspection tool, HTTP/2, HTTP/3, Rust.\\n*   **Description:** Pororoca is primarily an HTTP inspection tool with strong support for modern protocols like **HTTP/2** and **HTTP/3**, alongside HTTP/1.1. Built with Rust for performance, it allows detailed analysis and debugging of interactions using these newer protocols, capabilities often lacking in older clients. While it can send requests, its focus on inspection and advanced protocol support differentiates it.\\n\\n### [Nightingale REST Client](https://nightingale.rest/) ([repo](https://github.com/jenius-apps/nightingale-rest-api-client))\\n\\n*   **Keywords:** Windows REST API client, Fluent design, Resource-efficient, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Nightingale is a modern, **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** REST API client specifically designed for **Windows**, utilizing Fluent design principles for a native look and feel. It emphasizes performance and resource efficiency. It supports collections, environments, request history, and various authentication methods, making it a polished choice for developers primarily on the Windows platform.\\n\\n### [Requestly](https://requestly.com) ([repo](https://github.com/requestly/requestly))\\n\\n*   **Keywords:** Desktop app, API Client, API Mocking, Modify HTTP requests, Intercept.\\n*   **Description:** In addition to its browser extension, Requestly offers a full-featured **desktop app** (Windows, macOS, Linux). This provides the same powerful combination of **API client**, **API mocking**, request/response interception, and modification capabilities in a standalone application. Ideal for users who prefer a dedicated tool or need to intercept traffic system-wide or from non-browser applications.\\n\\n### [Cartero](https://cartero.danirod.es/) ([repo](https://github.com/danirod/cartero))\\n\\n*   **Keywords:** Native, Lightweight, Multiplatform, Free HTTP client.\\n*   **Description:** Cartero aims to be a **native**, **lightweight**, multiplatform, and **free** HTTP client. It focuses on providing the essential API request functionality without excessive complexity, using native UI toolkits (like GTK on Linux) for better performance and OS integration. A good option for users seeking a simple, fast, no-frills desktop client.\\n\\n---\\n\\n## Postman Alternatives - IDE Integration API Clients\\n\\nThese tools bring API interaction directly into your code editor, minimizing context switching and keeping API testing close to development.\\n\\n### [VS Code REST Client](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) ([repo](https://github.com/Huachao/vscode-restclient))\\n\\n*   **Keywords:** VS Code extension, Send HTTP request, Text-based, IDE integration.\\n*   **Description:** An immensely popular **VS Code extension**, REST Client allows developers to define and send HTTP/GraphQL requests directly from plain text files (`.http` or `.rest`). It uses a simple, intuitive syntax, supports environment variables (via VS Code settings), code snippets, response saving, cURL command generation, and more. Clicking \"Send Request\" executes the query, displaying the response in a separate pane. Its tight integration makes it a favorite **free Postman alternative** for VS Code users.\\n\\n### [Thunder Client](https://marketplace.visualstudio.com/items?itemName=rangav.vscode-thunder-client)\\n\\n*   **Keywords:** VS Code extension, Lightweight REST API Client, GUI inside VS Code.\\n*   **Description:** Another highly popular **VS Code extension**, **Thunder Client** provides a lightweight, GUI-based REST API client experience directly within the editor, often cited as a direct **Postman alternative** for VS Code users. Unlike the text-based approach of REST Client, Thunder Client offers a graphical interface similar to standalone clients (Postman, Insomnia) but integrated into VS Code\\'s sidebar. It supports collections, environment variables, scripting, and testing, offering a more visual workflow without leaving the IDE. Its ease of use and feature set make it extremely popular.\\n\\n### [RESTClient (Firefox Add-on)](https://addons.mozilla.org/en-US/firefox/addon/restclient/)\\n\\n*   **Keywords:** Firefox add-on, REST debugger, Browser extension.\\n*   **Description:** A simple **browser extension** for Firefox providing a basic UI to construct and send HTTP requests and view responses. Useful for quick debugging or testing of web services directly from within Firefox.\\n\\n### [restclient.el](https://github.com/pashky/restclient.el)\\n\\n*   **Keywords:** Emacs HTTP client, Text-based, IDE integration.\\n*   **Description:** For users of the Emacs editor, `restclient.el` enables defining and executing HTTP requests from within plain text buffers using a specific format. It integrates seamlessly into Emacs workflows, allowing responses to be viewed and manipulated within the editor.\\n\\n### [verb](https://github.com/federicotdn/verb)\\n\\n*   **Keywords:** Emacs HTTP client, Organize requests, IDE integration.\\n*   **Description:** Another option for Emacs users, `verb` focuses on organizing and sending HTTP requests from within the editor, providing an alternative workflow and feature set to `restclient.el`.\\n\\n### [rest.nvim](https://github.com/rest-nvim/rest.nvim)\\n\\n*   **Keywords:** Neovim HTTP client, Lua, Fast, IDE integration.\\n*   **Description:** A fast, **Lua**-based HTTP client specifically for Neovim users. `rest.nvim` allows defining requests in `.http` files (similar to VS Code REST Client) and executing them directly within Neovim, leveraging the editor\\'s modern capabilities for a performant experience.\\n\\n### [kulala.nvim](https://github.com/mistweaverco/kulala.nvim)\\n\\n*   **Keywords:** Neovim REST Client, Minimalist, IDE integration.\\n*   **Description:** Positioned as a *minimal* REST client interface for Neovim, `kulala.nvim` likely offers core request-sending functionality with a focus on simplicity and potentially less configuration overhead compared to `rest.nvim`.\\n\\n---\\n\\n## Postman Alternatives - Command-Line (CLI) API Clients\\n\\nIndispensable for automation, scripting, and terminal enthusiasts, **CLI** tools offer performance, composability, and easy integration into CI/CD pipelines.\\n\\n### [curl](https://curl.se) ([repo](https://github.com/curl/curl))\\n\\n*   **Keywords:** curl, Command line tool, Data transfer, URL syntax, CLI.\\n*   **Description:** The foundational, ubiquitous **command-line tool** for transferring data using **URL syntax**. Active since 1998, **`curl`** is incredibly powerful and versatile, supporting numerous protocols (HTTP, HTTPS, FTP, SMB, LDAP, etc.) with extensive options for fine-grained control. While its syntax can be complex, its reliability and presence on virtually every system make it essential for scripting and basic **API testing**. It\\'s the benchmark against which other CLI clients are often measured.\\n\\n### [HTTPie](https://httpie.io/cli) ([repo](https://github.com/httpie/cli))\\n\\n*   **Keywords:** HTTPie, Human-friendly HTTP client, CLI, JSON support, Syntax highlighting.\\n*   **Description:** **HTTPie** aims to make CLI interaction with HTTP APIs as **human-friendly** as possible. It provides an intuitive syntax, sensible defaults, built-in **JSON support**, syntax highlighting, and formatted output, making it significantly easier and more pleasant to use for common API calls than raw `curl`. A very popular **free Postman alternative** for terminal users seeking better usability.\\n\\n### [grpcurl](https://github.com/fullstorydev/grpcurl)\\n\\n*   **Keywords:** grpcurl, gRPC CLI, Command-line tool, gRPC reflection.\\n*   **Description:** The `curl` equivalent for **gRPC**. `grpcurl` allows interacting with gRPC servers from the command line, enabling service discovery (via reflection or proto files), listing methods, and invoking RPC calls. Essential for scripting and testing gRPC services in a terminal environment.\\n\\n### [xh](https://github.com/ducaale/xh)\\n\\n*   **Keywords:** Friendly HTTP requests, Fast CLI tool, Rust.\\n*   **Description:** Built with Rust, `xh` is another **friendly** and **fast** CLI tool for sending HTTP requests, positioned as an alternative to HTTPie. It offers similar usability improvements over `curl`, focusing on performance and a pleasant user experience with features like simplified syntax and colored output.\\n\\n### [posting](https://github.com/darrenburns/posting)\\n\\n*   **Keywords:** Modern API client, Terminal-based.\\n*   **Description:** `posting` aims to be a modern API client living entirely in the terminal, potentially offering a more interactive or structured experience than simple request/response tools, possibly managing contexts or sessions within its interface.\\n\\n### [curlie](https://github.com/rs/curlie)\\n\\n*   **Keywords:** curl frontend, HTTPie syntax, CLI.\\n*   **Description:** `curlie` cleverly acts as a frontend for `curl`, allowing users to use **HTTPie**-like syntax while leveraging the power and ubiquity of the underlying `curl` executable. It translates the user-friendly commands into the appropriate `curl` options, offering a bridge between HTTPie\\'s ease of use and `curl`\\'s extensive capabilities.\\n\\n### [HttpRepl](https://github.com/dotnet/HttpRepl)\\n\\n*   **Keywords:** .NET HTTP client, CLI, REPL, OpenAPI, Swagger integration.\\n*   **Description:** Developed by Microsoft as part of the **.NET** ecosystem, HttpRepl is a lightweight, cross-platform **CLI** tool providing an interactive **REPL** (Read-Eval-Print Loop) for exploring and testing RESTful APIs. It allows navigation through API paths using familiar shell commands (`ls`, `cd`), integrates with **OpenAPI (Swagger)** definitions for better discovery, and supports standard HTTP verbs. It\\'s particularly convenient for .NET developers but works with any REST API.\\n\\n### [ain](https://github.com/jonaslu/ain)\\n\\n*   **Keywords:** Meta HTTP client, curl wrapper, wget wrapper, httpie wrapper, CLI.\\n*   **Description:** `ain` functions as a *meta-client* in the terminal. Instead of performing HTTP requests itself, it acts as a wrapper, providing a simplified syntax layer that translates commands to execute using backend tools like **`curl`**, `wget`, or **`httpie`** (whichever is available). This leverages the robustness of established tools while offering a potentially easier command structure.\\n\\n### [evans](https://github.com/ktr0731/evans)\\n\\n*   **Keywords:** gRPC client, CLI, REPL, Expressive gRPC client.\\n*   **Description:** Evans is another feature-rich **CLI** client for **gRPC** services, branding itself as a \"more expressive universal gRPC client\". It provides both an interactive **REPL** mode for exploration (with auto-completion) and a CLI mode suitable for scripting. It supports server reflection, loading proto files, handling metadata, and various output formats, serving as a powerful alternative to `grpcurl`.\\n\\n### [httpYac](https://httpyac.github.io/) ([repo](https://github.com/anweber/httpyac))\\n\\n*   **Keywords:** HTTP file execution, CLI, REST, SOAP, GraphQL, gRPC, VS Code integration.\\n*   **Description:** httpYac bridges IDE/text-based definitions with **CLI** execution. It uses `.http` files (similar syntax to the VS Code REST Client) to define requests for **REST, SOAP, GraphQL, and gRPC**. Its CLI tool then executes these files, making it excellent for managing complex requests as code and running them in automated environments (like CI/CD). It supports environment variables, scripting hooks, and response handling. It also has a companion **VS Code extension**.\\n\\n### [ATAC](https://atac.julien-cpsn.com/) ([repo](https://github.com/Julien-cpsn/ATAC))\\n\\n*   **Keywords:** Terminal API client, TUI, Simple Postman alternative CLI.\\n*   **Description:** ATAC (API Terminal Automated Client) aims to provide a simple, **Postman-like** experience within the **terminal**. It likely offers a TUI (Text User Interface) or a straightforward command structure for defining, organizing (perhaps in collections), executing, and viewing HTTP requests, focusing on ease of use for common API testing tasks directly from the command line.\\n\\n---\\n\\n## Postman Alternatives - Automated API Testing Frameworks\\n\\nWhile many API clients include basic assertion capabilities, the tools in this section are specifically designed for *automating* API tests, often integrating into CI/CD pipelines. They typically treat tests as code or structured data, enabling complex scenarios, data-driven testing, comprehensive reporting, and robust validation. Some popular commercial tools often mentioned in this context include **Katalon Studio**, **SoapUI**, **TestSigma**, **Assertible**, **ReadyAPI**, and **Apigee** (though Apigee is more an API management platform), while **JMeter** is a powerful ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e tool focused on load testing but usable for functional API tests. The following are primarily **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** frameworks focused on functional and integration API testing.\\n\\n### [Step CI](https://stepci.com) ([repo](https://github.com/stepci/stepci))\\n\\n*   **Keywords:** API Test Automation framework, Declarative testing, YAML, CI/CD integration, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Step CI is an **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e API test automation framework** emphasizing simplicity and **CI/CD integration**. Tests are defined declaratively using **YAML**, specifying API calls, data extraction rules (to chain requests), and assertions. It's designed for ease of use, allowing developers and QA teams to quickly create and maintain automated API tests as part of their development lifecycle.\\n\\n### [Hurl](https://hurl.dev) ([repo](https://github.com/Orange-OpenSource/hurl))\\n\\n*   **Keywords:** Plain text HTTP requests, API testing, CLI, Assertions, CI/CD.\\n*   **Description:** Hurl executes HTTP requests defined in simple, human-readable **plain text** files (`.hurl`). This format allows specifying request details (method, URL, headers, body) alongside **assertions** on the response (status, headers, body content via JSONPath, XPath, regex, etc.) and capturing values for subsequent requests. Hurl acts as a **CLI** tool that runs these files, making it ideal for integration testing within **CI/CD** pipelines due to its simplicity and dependency-free nature.\\n\\n### [Karate](https://karatelabs.github.io/karate/) ([repo](https://github.com/karatelabs/karate))\\n\\n*   **Keywords:** Karate testing framework, BDD, API test automation, Mocks, Performance testing, \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** **Karate** is a popular **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e** framework that unifies **API test automation**, **mocks**, performance testing, and even UI automation. It uses a **BDD (Behavior-Driven Development)** syntax based on Gherkin, making tests readable by diverse teams. Karate excels at handling JSON and XML natively, provides powerful assertion capabilities, and allows embedding JavaScript for complex logic if needed. It's often cited as a comprehensive alternative for teams looking beyond basic Postman tests.\\n\\n### [Tavern](https://taverntesting.github.io) ([repo](https://github.com/taverntesting/tavern))\\n\\n*   **Keywords:** Automated RESTful API testing, Pytest plugin, YAML, MQTT, \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Tavern is a **Pytest plugin** for **automated RESTful API testing**, using a declarative **YAML** format. Its tight integration with Pytest allows leveraging the extensive Python testing ecosystem. Tavern supports complex scenarios, validation using JSON Schema, and extensibility for different protocols (including built-in **MQTT** support).\\n\\n### [Venom](https://github.com/ovh/venom)\\n\\n*   **Keywords:** Integration tests, Test runner, YAML, Executors, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Venom is a versatile **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': \"e** test runner developed by OVHcloud. It allows writing test cases in **YAML** and supports various **executors** (HTTP, gRPC, WebSockets, databases, SMTP, SSH, etc.), making it suitable for complex **integration tests** that span multiple services and protocols.\\n\\n### [pyresttest](https://github.com/svanoort/pyresttest)\\n\\n*   **Keywords:** Python REST testing, API testing framework, Basic testing, JSON.\\n*   **Description:** A relatively simple **Python**-based **REST testing** and API microbenchmarking tool. Tests are defined in **JSON** or YAML format, focusing on basic test structures, validations, and extraction mechanisms. It's a straightforward option for teams comfortable with Python looking for a minimal framework.\\n\\n### [runn](https://github.com/k1LoW/runn)\\n\\n*   **Keywords:** Scenario runner, API testing tool, Go, Debugger.\\n*   **Description:** `runn` is a tool and library built in **Go** for running operations defined in a scenario file (YAML). It supports API testing (HTTP/gRPC), database queries, command execution, and includes features like a step-by-step **debugger**, making it useful for complex end-to-end scenario testing.\\n\\n### [scenarigo](https://github.com/zoncoen/scenarigo)\\n\\n*   **Keywords:** End-to-end scenario testing, HTTP server testing, gRPC server testing, Go.\\n*   **Description:** Also built in **Go**, `scenarigo` is an **end-to-end scenario testing tool** focused on **HTTP** and **gRPC** servers. It allows defining test scenarios in YAML, referencing `.proto` files for gRPC, and writing assertions to validate complex application behaviors.\\n\\n### [Schemathesis](https://schemathesis.readthedocs.io/) ([repo](https://github.com/schemathesis/schemathesis))\\n\\n*   **Keywords:** Specification-centric API testing, Property-based testing, OpenAPI, GraphQL, \", 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Schemathesis takes a unique, **specification-centric** approach to API testing. It reads your **OpenAPI** or **GraphQL** schema and automatically generates test cases based on the defined constraints (using **property-based testing** principles). This helps find edge cases and specification violations that manual testing might miss. It integrates well with Python testing frameworks.\\n\\n### [Dredd](https://github.com/apiaryio/dredd)\\n\\n*   **Keywords:** API Blueprint, OpenAPI testing, Language-agnostic, HTTP API testing.\\n*   **Description:** Dredd is a **language-agnostic HTTP API testing tool** that validates if your API implementation matches its documentation (**API Blueprint** or **OpenAPI/Swagger**). It reads the spec, makes requests to your live API server, and compares the results against the documented expectations. Useful for ensuring documentation stays synchronized with the actual API behavior.\\n\\n### [abao](https://github.com/cybertk/abao)\\n\\n*   **Keywords:** RAML testing, REST API automated testing.\\n*   **Description:** `abao` is specifically designed for **automated testing** of REST APIs documented using **RAML** (RESTful API Modeling Language). It functions similarly to Dredd but focuses on the RAML specification format.\\n\\n### [HttpRunner](https://httprunner.com/httprunner/) ([repo](https://github.com/httprunner/httprunner))\\n\\n*   **Keywords:** API testing framework, YAML/JSON, Go/Python, Load testing support.\\n*   **Description:** HttpRunner is a popular **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e API testing framework** supporting tests defined in **YAML or JSON**. It has implementations in both **Go** and **Python** (**HttpRunner v4** onwards focusing on Go). It supports various features like data-driven testing, complex scenarios, hooks, and can also be used for basic **load testing**.\\n\\n### [k6](https://k6.io) ([repo](https://github.com/grafana/k6))\\n\\n*   **Keywords:** Load testing tool, Performance testing, JavaScript, Go, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** While primarily known as a modern, developer-centric **load testing tool**, **k6** (by Grafana) can also be used for functional API testing. Tests are written in **JavaScript** (ES2015+) and executed by a high-performance **Go** runtime. It integrates well into developer workflows and CI pipelines, focusing on performance and reliability under load, but its scripting capabilities allow for functional assertions too.\\n\\n### [Artillery](https://artillery.io) ([repo](https://github.com/artilleryio/artillery))\\n\\n*   **Keywords:** Load testing, Performance testing, Cloud-scale, YAML, JavaScript, ', 'type': 'text'}, {'text': 'Open-sourc', 'type': 'highlight'}, {'text': 'e.\\n*   **Description:** Similar to k6, **Artillery** is an **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** tool focused on **load testing** at **cloud-scale**. Test scenarios are typically defined in **YAML**, with support for custom logic written in **JavaScript**. While its main goal is performance testing, it includes features for making assertions and validating responses, allowing it to be used for complex functional API testing scenarios as well.\\n\\n---\\n\\n## Frequently Asked Questions (FAQs) about Postman Alternatives\\n\\nBased on common searches and discussions, here are answers to some frequently asked questions:\\n\\n*   **Is there anything better than Postman?**\\n    \"Better\" is subjective and depends entirely on your needs.\\n    *   For users prioritizing **privacy, offline access, and Git integration**, **Bruno** is often considered better.\\n    *   For a **free, feature-rich web/PWA experience**, **Hoppscotch** is frequently cited as a superior alternative.\\n    *   For **seamless VS Code integration**, **VS Code REST Client** or **Thunder Client** are often preferred.\\n    *   For **CLI power users**, **HTTPie** or `curl` (with perhaps `curlie` or `xh`) offer different advantages.\\n    *   For **robust automated testing**, dedicated frameworks like **Karate**, **Step CI**, or **Hurl** might be better suited than Postman\\'s built-in testing.\\n    *   Some all-in-one commercial tools like **Apidog** or **Katalon Studio** position themselves as integrated platforms offering broader lifecycle support, which some may find \"better.\"\\n    Ultimately, the \"best\" **Postman alternative** is the one that best fits *your* specific workflow, priorities (privacy, cost, collaboration, offline use), and technical requirements (protocols, testing complexity).\\n\\n*   **How to post API without Postman?**\\n    You have numerous options!\\n    *   **Desktop GUI Clients:** Use **Bruno**, **Insomnia/Insomnium**, **Hoppscotch** (via PWA), **Nightingale**, **Yaak**, **Requestly Desktop**, etc.\\n    *   **Web-Based Clients:** Use **Hoppscotch**, **Restfox**, **Firecamp**, **Requestly Extension**, etc., directly in your browser.\\n    *   **IDE Integrations:** Use **VS Code REST Client**, **Thunder Client** (VS Code), `restclient.el` (Emacs), `rest.nvim` (Neovim), etc.\\n    *   **CLI Tools:** Use **`curl`**, **HTTPie**, `xh`, `posting`, `httpYac`, etc., from your terminal. This is ideal for scripting and automation.\\n    *   **Programmatically:** Use HTTP client libraries within your preferred programming language (e.g., `requests` in Python, `axios` in JavaScript, `HttpClient` in C#/.NET, `OkHttp` in Java/Kotlin).\\n    *   **API Testing Frameworks:** Use tools like **Karate**, **Hurl**, **Step CI** to define and execute POST requests as part of automated test suites.\\n\\n*   **Which is better, Swagger or Postman?**\\n    They serve different primary purposes but have overlapping functionality:\\n    *   **Swagger (OpenAPI Specification + Swagger UI):** Primarily focuses on **API design and documentation**. The spec defines the contract, and **Swagger UI** provides interactive documentation generated from that spec, allowing basic **interactive testing**. Its strength is in defining and communicating the API structure.\\n    *   **Postman (and its alternatives):** Primarily focuses on **API interaction, testing, and development workflow**. They are feature-rich clients for sending requests, managing environments/collections, writing complex test scripts (assertions, chaining), mocking, monitoring, and collaboration.\\n    **Conclusion:** They are complementary tools. Use Swagger/OpenAPI to define and document your API. Use Postman or an alternative like Bruno, Hoppscotch, or VS Code REST Client for in-depth testing, exploration, and development against that API. You often use **Swagger UI** for quick interactive checks based on the docs, and a tool like Postman/Bruno for more complex testing scenarios and development workflows.\\n\\n*   **Do people still use Postman?**\\n    Yes, absolutely. Postman remains a very popular and widely used tool in the API development space, despite the growing interest in alternatives. It has a large user base, extensive features built over many years, and strong brand recognition. However, the recent changes regarding mandatory accounts and cloud sync have undeniably caused a significant portion of the community (especially those prioritizing privacy, offline use, or ', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e principles) to evaluate and migrate to alternatives like those listed in this document. The API client landscape is now more diverse than ever.\\n\\n---\\n\\n## Conclusion: The Flourishing Ecosystem of API Clients\\n\\nThe API client landscape is richer and more diverse than ever before. While Postman and Insomnia played pivotal roles in popularizing graphical API interaction, concerns around cloud dependence, privacy, and cost have catalyzed the growth of exceptional **', 'type': 'text'}, {'text': 'open-sourc', 'type': 'highlight'}, {'text': 'e** and **free alternatives**.\\n\\nWhether you prioritize **offline-first** operation (**Bruno**, Restfox), a comprehensive **web-based** experience (**Hoppscotch**), seamless **IDE integration** (**VS Code REST Client**, Thunder Client), **CLI** efficiency (**HTTPie**, `curl`, Hurl), specialized protocol support (**gRPC UI**, ezy, Firecamp), or robust **automated testing** (**Karate**, Step CI, Schemathesis), there is a high-quality, community-driven tool available.\\n\\nThis list (**Awesome API Clients**) provides a starting point for exploring these powerful **Postman alternatives** and **Insomnia alternatives**. Evaluate them based on your specific needs ‚Äì be it individual use, team collaboration, protocol requirements, testing complexity, or philosophical alignment regarding ', 'type': 'text'}, {'text': 'open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' and data privacy. The power to choose the right tool for your API workflow, without compromises, is firmly in your hands.\\n', 'type': 'text'}], 'tags': [{'text': 'region:us', 'type': 'text'}], 'name': [{'text': 'furudo-erika/awesome-postman-alternatives', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '67c32c3f99fba7b00e8e4f09', 'avatarUrl': '/avatars/a9618a88c42e6787e2bf0af1a473dc93.svg', 'fullname': 'Furudo Erika', 'name': 'furudo-erika', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False}}, {'repoId': '6804a960db987db8b1d0667f', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349475679, 'repoName': 'Llama-3.1-Nemotron-Nano-8B-v1-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Llama-3.1-Nemotron-Nano-8B-v1, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Llama-3.1-Nemotron-Nano-8B-v1-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Llama-3.1-Nemotron-Nano-8B-v1-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [ollama](https://github.com/ollama/ollama)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nLlama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of [Meta Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. \\n\\nLlama-3.1-Nemotron-Nano-8B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. It is created from Llama 3.1 8B Instruct and offers improvements in model accuracy. The model fits on a single RTX GPU and can be used locally. The model supports a context length of 128K.\\n\\nThis model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, and Tool Calling as well as multiple reinforcement learning (RL) stages using REINFORCE (RLOO) and Online Reward-aware Preference Optimization (RPO) algorithms for both chat and instruction-following. The final model checkpoint is obtained after merging the final SFT and Online RPO checkpoints. Improved using Qwen.\\n\\nThis model is part of the Llama Nemotron Collection. You can find the other model(s) in this family here: \\n[Llama-3.3-Nemotron-Super-49B-v1](https://huggingface.co/nvidia/Llama-3.3-Nemotron-Super-49B-v1)\\n\\nThis model is ready for commercial use.\\n\\n## Quick Start and Usage Recommendations:\\n\\n1. Reasoning mode (ON/OFF) is controlled via the system prompt, which must be set as shown in the example below. All instructions should be contained within the user prompt\\n2. We recommend setting temperature to `0.6`, and Top P to `0.95` for Reasoning ON mode\\n3. We recommend using greedy decoding for Reasoning OFF mode\\n4. We have provided a list of prompts to use for evaluation for each benchmark where a specific template is required\\n\\nYou can try this model out through the preview API, using this link: [Llama-3.1-Nemotron-Nano-8B-v1](https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1).\\n\\nSee the snippet below for usage with Hugging Face Transformers library. Reasoning mode (ON/OFF) is controlled via system prompt. Please see the example below.\\nOur code requires the transformers package version to be `4.44.2` or higher.\\n\\n\\n### Example of ‚ÄúReasoning On:‚Äù\\n\\n```python\\nimport torch\\nimport transformers\\n\\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\\ntokenizer.pad_token_id = tokenizer.eos_token_id\\n\\npipeline = transformers.pipeline(\\n   \"text-generation\",\\n   model=model_id,\\n   tokenizer=tokenizer,\\n   max_new_tokens=32768,\\n   temperature=0.6,\\n   top_p=0.95,\\n   **model_kwargs\\n)\\n\\n# Thinking can be \"on\" or \"off\"\\nthinking = \"on\"\\n\\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\\n```\\n\\n\\n### Example of ‚ÄúReasoning Off:‚Äù\\n\\n```python\\nimport torch\\nimport transformers\\n\\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\\ntokenizer.pad_token_id = tokenizer.eos_token_id\\n\\npipeline = transformers.pipeline(\\n   \"text-generation\",\\n   model=model_id,\\n   tokenizer=tokenizer,\\n   max_new_tokens=32768,\\n   do_sample=False,\\n   **model_kwargs\\n)\\n\\n# Thinking can be \"on\" or \"off\"\\nthinking = \"off\"\\n\\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}]))\\n```\\n\\nFor some prompts, even though thinking is disabled, the model emergently prefers to think before responding. But if desired, the users can prevent it by pre-filling the assistant response.\\n\\n```python\\nimport torch\\nimport transformers\\n\\nmodel_id = \"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\"\\nmodel_kwargs = {\"torch_dtype\": torch.bfloat16, \"device_map\": \"auto\"}\\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\\ntokenizer.pad_token_id = tokenizer.eos_token_id\\n\\n# Thinking can be \"on\" or \"off\"\\nthinking = \"off\"\\n\\npipeline = transformers.pipeline(\\n   \"text-generation\",\\n   model=model_id,\\n   tokenizer=tokenizer,\\n   max_new_tokens=32768,\\n   do_sample=False,\\n   **model_kwargs\\n)\\n\\nprint(pipeline([{\"role\": \"system\", \"content\": f\"detailed thinking {thinking}\"}, {\"role\": \"user\", \"content\": \"Solve x*(sin(x)+2)=0\"}, {\"role\":\"assistant\", \"content\":\"<think>\\\\n</think>\"}]))\\n```', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Llama-3.1-Nemotron-Nano-8B-v1, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Llama-3.1-Nemotron-Nano-8B-v1-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '6808ceb1ccabba3e07582a7d', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349488189, 'repoName': 'MN-12B-Mag-Mell-R1-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, MN-12B-Mag-Mell-R1, text-generation, en, arxiv:2311.03099, arxiv:2306.01708, license:other, region:us, conversational', 'name': 'duyntnet/MN-12B-Mag-Mell-R1-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'MN-12B-Mag-Mell-R1-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/inflatebot/MN-12B-Mag-Mell-R1\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [ollama](https://github.com/ollama/ollama)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nNOTE for newer users: \"R1\" here means \"Revision 1\". This model predates DeepSeek\\'s R1; DeepSeek inadvertently made using this versioning scheme very annoying!\\n\\nThis is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).\\n\\n## Usage Details\\n\\n### Sampler Settings\\nMag Mell R1 was tested with Temp 1.25 and MinP 0.2. This was fairly stable up to 10K, but this might be too \"hot\".\\nIf issues with coherency occur, try *in*creasing MinP or *de*creasing Temperature.\\n\\nOther samplers shouldn\\'t be necessary. XTC was shown to break outputs. DRY should be okay if used sparingly. Other penalty-type samplers should probably be avoided.\\n\\n### Formatting\\nThe base model for Mag Mell is [Mistral-Nemo-Base-2407-chatml](https://huggingface.co/IntervitensInc/Mistral-Nemo-Base-2407-chatml), and as such ChatML formatting is recommended.\\n\\nEarly testing versions had a tendency to leak tokens, but this should be more or less hammered out. It recently (12-18-2024) came to attention that Cache Quantization may either cause or exacerbate this issue.\\n\\n## Merge Details\\nMag Mell is a multi-stage merge, Inspired by hyper-merges like [Tiefighter](https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter) and [Umbral Mind.](https://huggingface.co/Casual-Autopsy/L3-Umbral-Mind-RP-v2.0-8B)\\nIntended to be a general purpose \"Best of Nemo\" model for any fictional, creative use case.\\n\\n6 models were chosen based on 3 categories; they were then paired up and merged via layer-weighted SLERP to create intermediate \"specialists\" which are then evaluated in their domain.\\nThe specialists were then merged into the base via DARE-TIES, with hyperparameters chosen to reduce interference caused by the overlap of the three domains. \\nThe idea with this approach is to extract the best qualities of each component part, and produce models whose task vectors represent more than the sum of their parts.\\n\\nThe three specialists are as follows:\\n\\n- Hero (RP, kink/trope coverage): [Chronos Gold](https://huggingface.co/elinas/Chronos-Gold-12B-1.0), [Sunrose](https://huggingface.co/Fizzarolli/MN-12b-Sunrose).\\n\\n- Monk (Intelligence, groundedness): [Bophades](https://huggingface.co/nbeerbower/mistral-nemo-bophades-12B), [Wissenschaft](https://huggingface.co/nbeerbower/mistral-nemo-wissenschaft-12B).\\n\\n- Deity (Prose, flair): [Gutenberg v4](https://huggingface.co/nbeerbower/mistral-nemo-gutenberg-12B-v4), [Magnum 2.5 KTO](https://huggingface.co/anthracite-org/magnum-v2.5-12b-kto).\\n\\nI\\'ve been dreaming about this merge since Nemo tunes started coming out in earnest. From our testing, Mag Mell demonstrates worldbuilding capabilities unlike any model in its class, comparable to old adventuring models like Tiefighter, and prose that exhibits minimal \"slop\" (not bad for no finetuning,) frequently devising electrifying metaphors that left us consistently astonished.\\n\\nI don\\'t want to toot my own bugle though; I\\'m really proud of how this came out, but please leave your feedback, good or bad.\\n\\nSpecial thanks as usual to Toaster for his feedback and Fizz for helping fund compute, as well as the KoboldAI Discord for their resources.\\n\\n### Merge Method\\n\\nThis model was merged using the [DARE](https://arxiv.org/abs/2311.03099) [TIES](https://arxiv.org/abs/2306.01708) merge method using [IntervitensInc/Mistral-Nemo-Base-2407-chatml](https://huggingface.co/IntervitensInc/Mistral-Nemo-Base-2407-chatml) as a base.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* IntervitensInc/Mistral-Nemo-Base-2407-chatml\\n* nbeerbower/mistral-nemo-bophades-12B\\n* nbeerbower/mistral-nemo-wissenschaft-12B\\n* elinas/Chronos-Gold-12B-1.0\\n* Fizzarolli/MN-12b-Sunrose\\n* nbeerbower/mistral-nemo-gutenberg-12B-v4\\n* anthracite-org/magnum-12b-v2.5-kto\\n\\n### Configuration\\n\\nThe following YAML configurations were used to produce this model:\\n\\n#### Monk:\\n```yaml\\nmodels:\\n  - model: nbeerbower/mistral-nemo-bophades-12B\\n  - model: nbeerbower/mistral-nemo-wissenschaft-12B\\nmerge_method: slerp\\nbase_model: nbeerbower/mistral-nemo-bophades-12B\\nparameters:\\n  t: [0.1, 0.2, 0.4, 0.6, 0.6, 0.4, 0.2, 0.1]\\ndtype: bfloat16\\ntokenizer_source: base\\n```\\n\\n#### Hero:\\n```yaml\\nmodels:\\n  - model: elinas/Chronos-Gold-12B-1.0\\n  - model: Fizzarolli/MN-12b-Sunrose\\nmerge_method: slerp\\nbase_model: elinas/Chronos-Gold-12B-1.0\\nparameters:\\n  t: [0.1, 0.2, 0.4, 0.6, 0.6, 0.4, 0.2, 0.1]\\ndtype: bfloat16\\ntokenizer_source: base\\n```\\n\\n#### Deity:\\n```yaml\\nmodels:\\n  - model: nbeerbower/mistral-nemo-gutenberg-12B-v4\\n  - model: anthracite-org/magnum-12b-v2.5-kto\\nmerge_method: slerp\\nbase_model: nbeerbower/mistral-nemo-gutenberg-12B-v4\\nparameters:\\n  t: [0, 0.1, 0.2, 0.25, 0.25, 0.2, 0.1, 0]\\ndtype: bfloat16\\ntokenizer_source: base\\n```\\n\\n#### Mag Mell:\\n```yaml\\nmodels:\\n - model: monk\\n   parameters:\\n     density: 0.7\\n     weight: 0.5\\n - model: hero\\n   parameters:\\n     density: 0.9\\n     weight: 1\\n - model: deity\\n   parameters:\\n     density: 0.5\\n     weight: 0.7\\nmerge_method: dare_ties\\nbase_model: IntervitensInc/Mistral-Nemo-Base-2407-chatml\\ntokenizer_source: base\\n```\\n\\n\\n`In Irish mythology, Mag Mell (modern spelling: Magh Meall, meaning \\'delightful plain\\') is one of the names for the Celtic Otherworld, a mythical realm achievable through death and/or glory... Never explicitly stated in any surviving mythological account to be an afterlife; rather, it is usually portrayed as a paradise populated by deities, which is occasionally visited by some adventurous mortals. In its island guise, it was visited by various legendary Irish heroes and monks, forming the basis of the adventure myth or echtrae...`', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, MN-12B-Mag-Mell-R1, text-generation, en, arxiv:2311.03099, arxiv:2306.01708, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/MN-12B-Mag-Mell-R1-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}, {'repoId': '680c40bdf53c47b013206a7a', 'repoOwnerId': '6566b71a7b5ed0735824b55e', 'isPrivate': False, 'type': 'model', 'likes': 0, 'isReadmeFile': True, 'readmeStartLine': 12, 'updatedAt': 1759349501579, 'repoName': 'Tessa-Rust-T1-7B-imatrix-GGUF', 'repoOwner': 'duyntnet', 'tags': 'transformers, gguf, imatrix, Tessa-Rust-T1-7B, text-generation, en, license:other, region:us, conversational', 'name': 'duyntnet/Tessa-Rust-T1-7B-imatrix-GGUF', 'fileName': 'README.md', 'formatted': {'repoName': [{'text': 'Tessa-Rust-T1-7B-imatrix-GGUF', 'type': 'text'}], 'repoOwner': [{'text': 'duyntnet', 'type': 'text'}], 'fileContent': [{'text': 'Quantizations of https://huggingface.co/Tesslate/Tessa-Rust-T1-7B\\n\\n\\n### ', 'type': 'text'}, {'text': 'Open', 'type': 'highlight'}, {'text': ' ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [llama.cpp](https://github.com/ggerganov/llama.cpp)\\n* [KoboldCPP](https://github.com/LostRuins/koboldcpp)\\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui)\\n* [ollama](https://github.com/ollama/ollama)\\n* [jan](https://github.com/janhq/jan)\\n* [GPT4All](https://github.com/nomic-ai/gpt4all)\\n\\n### Closed ', 'type': 'text'}, {'text': 'source', 'type': 'highlight'}, {'text': ' inference clients/UIs\\n* [LM Studio](https://lmstudio.ai/)\\n* [Backyard AI](https://backyard.ai/)\\n* More will be added...\\n---\\n\\n# From original readme\\n\\nTessa-Rust-T1 is a transformer-based **Rust code generation model**, fine-tuned from the powerful **Qwen2.5-Coder-7B-Instruct** base model. Designed specifically for Rust development, Tessa-Rust-T1 leverages advanced reasoning to autonomously generate well-structured, idiomatic Rust code, including functions, structs, traits, and modules. Its integration into agent systems makes it a powerful tool for automating backend development, systems programming, CLI tool creation, and Rust code intelligence.\\n\\n---\\n\\n## **Model Highlights**\\n- **Hybrid Reasoning**: Turn on Reasoning with this system prompt:\\n- ```Your role as a Rust assistant is to engage in deep, methodical reasoning and provide comprehensive, accurate solutions. Before arriving at a final answer, you must undertake a structured, multi-phase thinking process that emphasizes depth, verification, and clarity. This involves thoroughly analyzing the question, identifying key elements, summarizing relevant insights, generating hypotheses, iteratively refining thoughts, verifying assumptions, cross-checking with prior knowledge, and reevaluating earlier conclusions as necessary. Your response must be structured into two main sections: Thought and Solution. In the Thought section, rigorously document your reasoning in the following format: <|begin_of_thought|> {thought process with each logical step separated by \\'\\\\n\\\\n\\'} <|end_of_thought|>. Each step should reflect deep analysis‚Äîsuch as decomposing the problem, synthesizing relevant information, exploring different possibilities, validating each phase, correcting errors, and revisiting earlier assumptions. In the Solution section, consolidate all your insights and reasoned steps into a concise, well-structured final answer. Present it clearly and logically using this format: <|begin_of_solution|> Provide all the code necessary to solve the problem in the same code block. <|end_of_solution|>. This approach ensures that the final output reflects a high-confidence answer that results from critical thinking and iteration. Now, try to solve the following question through the above guidelines:```\\n- **Rust-specific Reasoning**: Accurately generates functional and idiomatic Rust code.\\n- **Agent Integration**: Seamlessly fits into AI-driven coding agents and autonomous development systems.\\n- **Context-Aware Generation**: Effectively understands and utilizes Rust project context, dependencies (crates), and language features (lifetimes, borrowing, traits) to provide relevant code solutions.\\n\\n---\\n\\n## **Use Cases**\\n\\n### **Recommended Uses**\\n- **Automatic Rust Code Generation**: Quickly produce Rust functions, structs, modules, and boilerplate code from textual prompts.\\n- **Agent-based Rust Development**: Integrate into automated coding systems for faster backend, systems, or tooling workflows.\\n- **Rust Code Refactoring**: Automate the optimization and enhancement of Rust code for idiomaticity and performance.\\n- **Generating CLI Tools**: Accelerate the creation of command-line applications.\\n- **Implementing API Endpoints**: Speed up backend development by generating route handlers and data models.\\n- **Writing Unit Tests**: Generate test cases for Rust functions and modules.\\n\\n### **Limitations**\\n- **Focused on Rust**: Limited use outside the Rust ecosystem.\\n- **Complex Logic/Lifetimes**: May require manual adjustments for highly complex asynchronous patterns, intricate lifetime management, or extensive `unsafe` code blocks.\\n- **Build Configuration**: May not fully automate `Cargo.toml` management or complex build scripts.\\n\\n---\\n\\n## **How to Use**\\n\\n### **Inference Example**\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\n# Make sure to use the correct model name you decide on\\nmodel_name = \"tesslate/Tessa-Rust-T1\" # Adjusted hypothetical name\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\") # Assumes CUDA availability\\n\\nprompt = \"\"\"<|im_start|>user\\nCreate a Rust function using the `rayon` crate to parallelize summing a vector of integers.\\nFunction signature: `fn parallel_sum(data: &[i32]) -> i32`\\n<|im_end|>\\n<|im_start|>assistant\\n<|im_start|>think\\n\"\"\" \\n\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\\n# Adjust generation parameters as needed\\noutputs = model.generate(**inputs, max_new_tokens=500, do_sample=True, temperature=0.6, top_p=0.9)\\n\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\\n```\\n\\n---\\n\\n## **Performance and Evaluation**\\n\\n- **Strengths**:\\n  - Strong idiomatic Rust code generation.\\n  - Excellent integration capabilities with agent-based systems.\\n  - Understanding of common Rust patterns and standard library usage.\\n\\n- **Weaknesses**:\\n  - Complex Rust logic (e.g., advanced generics, macros, intricate lifetimes, `unsafe` code) may require manual post-processing or refinement.\\n  - May hallucinate non-existent crate features or incorrect API usage for less common libraries.\\n\\n---\\n\\n## **Technical Specifications**\\n\\n- **Architecture**: Transformer-based LLM\\n- **Base Model**: Qwen2.5-Coder-7B-Instruct\\n- **Precision**: bf16 mixed precision (quantization options like q8 might be available depending on final model release)\\n- **Hardware Requirements**: Recommended 12GB+ VRAM for bf16 (may vary with quantization)\\n- **Software Dependencies**:\\n  - Hugging Face Transformers (`transformers>=4.34`)\\n  - PyTorch (`torch>=2.0`)\\n  - Accelerate (`accelerate`) for optimized loading/inference', 'type': 'text'}], 'tags': [{'text': 'transformers, gguf, imatrix, Tessa-Rust-T1-7B, text-generation, en, license:other, region:us, conversational', 'type': 'text'}], 'name': [{'text': 'duyntnet/Tessa-Rust-T1-7B-imatrix-GGUF', 'type': 'text'}], 'fileName': [{'text': 'README.md', 'type': 'text'}]}, 'authorData': {'_id': '6566b71a7b5ed0735824b55e', 'avatarUrl': '/avatars/a94f9d044b225b7e884c54f5a943ee58.svg', 'fullname': 'duynt', 'name': 'duyntnet', 'type': 'user', 'isPro': False, 'isHf': False, 'isHfAdmin': False, 'isMod': False, 'followerCount': 25}}]}\n",
      "Saved 0 results to huggingface_open_source_models_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Your Hugging Face API token\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Base URL\n",
    "url = \"https://huggingface.co/api/search/full-text\"\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    \"q\": \"open source\",\n",
    "    \"type\": \"model\",\n",
    "    \"limit\": 100\n",
    "}\n",
    "\n",
    "# Headers with Authorization\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {HF_TOKEN}\"\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse JSON\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "\n",
    "# Extract fields you care about\n",
    "records = []\n",
    "for item in data.get(\"items\", []):\n",
    "    records.append({\n",
    "        \"id\": item.get(\"id\"),\n",
    "        \"author\": item.get(\"author\"),\n",
    "        \"likes\": item.get(\"likes\"),\n",
    "        \"downloads\": item.get(\"downloads\"),\n",
    "        \"license\": item.get(\"cardData\", {}).get(\"license\")\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"huggingface_open_source_models_test.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} results to {output_file}\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
