{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total models: 68\n",
      "\n",
      "jd-opensource/JSL-joysafety-v1\n",
      "opensourcerelease/DeepSeek-V3-bf16\n",
      "opensource/extract_names\n",
      "OpenSourceMentorShip/gpt4all\n",
      "haoliu/coh_llama_on_open_source_data\n",
      "Opensourced/wormgpt-24\n",
      "mixtralyanis/bart_opensource\n",
      "mixtralyanis/flant5-opensource\n",
      "mixtralyanis/flant5-opensource-and-tuned\n",
      "mrdas/open-source\n",
      "OpenSourceEnjoyer/Yi-34B-200K-bnb-4bit\n",
      "OpenSourceEnjoyer/Nous-Hermes-2-Mistral-7B-DPO-SFT-LoRA\n",
      "OpenSourceEnjoyer/Nous-Hermes-2-Mistral-7B-DPO-SFT-FP16\n",
      "openSourcerer9000/sbds-model\n",
      "OpenSourceEnjoyer/Nous-Hermes-2-Mistral-7B-DPO-SFT-GGUF-Q8\n",
      "George33/Opensourcecognix\n",
      "PedroCintra67/llm-open-source\n",
      "Pankaj001/Opensource_attack_examples\n",
      "OpenSourceEnjoyer/LLaMA-3-8B-Function-Calling-GGUF\n",
      "OpenSourceEnjoyer/LLaMA-3-8B-Function-Calling-FP16\n",
      "openclimatefix/open-source-quartz-solar-forecast\n",
      "OpenSourceEnjoyer/LLaMA-3-8B-Instruct-T-Q4_K_M-GGUF\n",
      "OpenSourceEnjoyer/LLaMA-3.1-8B-Function-Calling-GGUF\n",
      "OpenSourceEnjoyer/LLaMA-3.1-8B-Function-Calling-FP16\n",
      "OpenSourceEnjoyer/Mistral-Nemo-Function-Calling-GGUF\n",
      "OpenSourceEnjoyer/Mistral-Nemo-Function-Calling-FP16\n",
      "OpenSourceEnjoyer/LLaMA-3.1-8B-ST-GGUF\n",
      "OpenSourceEnjoyer/LLaMA-3.1-8B-Function-Calling-100-GGUF\n",
      "cqchangm/opencv-source\n",
      "jeffrey03/Pricer-FineTune-OpenSource-2024-10-23_08.48.15\n",
      "namul2/pretrain_open_source\n",
      "WhiteboardLLM/opensource_12000\n",
      "WhiteboardLLM/llama1B_opensource_with_prompt\n",
      "opensourcerelease/DeepSeek-V3-Base-bf16\n",
      "mlfoundations-dev/stackexchange_opensource\n",
      "mradermacher/stackexchange_opensource-GGUF\n",
      "MSheng-Lee/geox-open-source\n",
      "opensourcerelease/DeepSeek-R1-bf16\n",
      "opensourcerelease/DeepSeek-R1-Zero-bf16\n",
      "twm-opensource/Qwen2.5-1.5B-Open-R1-Distill-v1-0317\n",
      "twm-opensource/Qwen2.5-1.5B-Open-R1-Distill-v2-0318\n",
      "twm-opensource/gemma-3-1b-it\n",
      "twm-opensource/Qwen2.5-7B-Open-R1-Distill-v1-0317\n",
      "twm-opensource/evals_logs\n",
      "RichardErkhov/OpenSourceEnjoyer_-_LLaMA-3.1-8B-Function-Calling-FP16-8bits\n",
      "RichardErkhov/OpenSourceEnjoyer_-_LLaMA-3.1-8B-Function-Calling-FP16-awq\n",
      "yyj1205/DeepSeek-R1-OpenSource-Soft-Recommendation-COT\n",
      "mradermacher/DeepSeek-R1-OpenSource-Soft-Recommendation-COT-GGUF\n",
      "gcuomo/open-source-ai-t5-liar-lens\n",
      "henokyemam/open_source\n",
      "Chitra137/my-open-source-model\n",
      "openSourcerer9000/ImpSeg\n",
      "omunaman/Open_Source_GPT_OSS_20B\n",
      "OpensourceThai/Wilai\n",
      "ChenWu98/openthoughts3_math_teachers_source_condition_32768_qwen3_4b_instruct_2507\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_32768_0_qwen3_4b_instruct_2507\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_16384_0_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_0_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_condition_17000_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_1_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_20000_0_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_condition_17000_20000_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_20000_1_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_condition_17000_5000_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_5000_0_qwen2_5_7b_instruct\n",
      "ChenWu98/openthoughts3_math_teachers_source_split_17000_5000_1_qwen2_5_7b_instruct\n",
      "ComfyAudio/stable-audio-open-1.0-Source\n",
      "ComfyAudio/stable-audio-open-small-Source\n",
      "Saved 68 model IDs to open_source_models_names.txt\n"
     ]
    }
   ],
   "source": [
    "# %% config\n",
    "QUERY = \"open source\"           # your search phrase\n",
    "MODE = \"full_text\"              # \"api\" or \"full_text\"\n",
    "SAVE_TO = \"open_source_models.txt\"  # set None to skip saving\n",
    "VERBOSE = True\n",
    "\n",
    "# Provide your HF token (optional but boosts API limits). Will prompt if not in env.\n",
    "import os\n",
    "from getpass import getpass\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or (getpass(\"Enter your HF token (hf_..., or leave blank): \").strip() or None)\n",
    "\n",
    "# %% imports\n",
    "import time, random, re\n",
    "from typing import Optional, Iterable, List\n",
    "from urllib.parse import quote_plus, urlparse, parse_qs, urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "# %% generic retry/backoff (works for both requests and hf_hub)\n",
    "def _retry_wait(attempt, base=1.0, backoff=2.0, max_delay=60.0, jitter=0.25):\n",
    "    wait = min(max_delay, base * (backoff ** attempt)) + random.uniform(0, jitter)\n",
    "    return wait\n",
    "\n",
    "def with_retry_requests(session: requests.Session, max_retries=8, verbose=True):\n",
    "    def _get(url, **kw):\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            r = session.get(url, **kw)\n",
    "            if r.status_code in (429, 502, 503, 504) and attempt < max_retries:\n",
    "                ra = r.headers.get(\"Retry-After\")\n",
    "                wait = float(ra) if ra else _retry_wait(attempt)\n",
    "                attempt += 1\n",
    "                if verbose:\n",
    "                    print(f\"[retry] GET {url} => {r.status_code}; sleep {wait:.1f}s ({attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "    return _get\n",
    "\n",
    "def iterate_list_models_with_retry(api: HfApi, search: str, *, full=False, direction=-1, max_retries=8, verbose=True):\n",
    "    \"\"\"\n",
    "    Resilient generator around api.list_models(search=...).\n",
    "    If pagination hiccups, restart and dedupe.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            for m in api.list_models(search=search, full=full, direction=direction):\n",
    "                mid = getattr(m, \"modelId\", None)\n",
    "                if not mid or mid in seen:\n",
    "                    continue\n",
    "                seen.add(mid)\n",
    "                yield m\n",
    "            return\n",
    "        except HfHubHTTPError as e:\n",
    "            code = getattr(getattr(e, \"response\", None), \"status_code\", None)\n",
    "            if code in (429, 502, 503, 504) and attempt < max_retries:\n",
    "                attempt += 1\n",
    "                wait = _retry_wait(attempt)\n",
    "                if verbose:\n",
    "                    print(f\"[retry] list_models => {code}; sleep {wait:.1f}s ({attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# %% helpers\n",
    "def extract_repo_ids_from_fulltext_html(html: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse /search/full-text HTML and extract model repo ids like 'owner/name'.\n",
    "    We look for anchors that look like '/owner/repo' (models) and dedupe.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    repo_ids = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        # Only plain model repo paths (exclude /datasets/, /spaces/, /orgs, etc.)\n",
    "        if re.match(r\"^/[A-Za-z0-9][A-Za-z0-9_\\-\\.]*/[A-Za-z0-9][A-Za-z0-9_\\-\\.]*$\", href):\n",
    "            repo_ids.add(href.strip(\"/\"))\n",
    "    return sorted(repo_ids)\n",
    "\n",
    "def find_next_page_url(current_url: str, html: str) -> Optional[str]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    next_link = None\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        if a.text.strip().lower() == \"next\":\n",
    "            next_link = a.get(\"href\")\n",
    "            break\n",
    "    if not next_link:\n",
    "        return None\n",
    "    # Handle relative links\n",
    "    base = \"https://huggingface.co\"\n",
    "    return urljoin(base, next_link)\n",
    "\n",
    "# %% main\n",
    "def get_models_api(query: str, token: Optional[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    API mode: uses /api/models search (matches repo IDs + usernames).\n",
    "    This does NOT do full-text across README. (Smaller result set.)\n",
    "    \"\"\"\n",
    "    api = HfApi(token=token)\n",
    "    out = []\n",
    "    for m in iterate_list_models_with_retry(api, search=query, full=False, direction=-1, verbose=VERBOSE):\n",
    "        out.append(m.modelId)\n",
    "        if VERBOSE and len(out) % 100 == 0:\n",
    "            print(f\"...{len(out)} (API)\")\n",
    "    return out\n",
    "\n",
    "def get_models_full_text(query: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Full-text mode: scrapes /search/full-text?type=model&q=... (what the website shows).\n",
    "    We paginate until 'Next' disappears.\n",
    "    \"\"\"\n",
    "    base = \"https://huggingface.co/search/full-text\"\n",
    "    session = requests.Session()\n",
    "    getter = with_retry_requests(session, verbose=VERBOSE)\n",
    "    url = f\"{base}?q={quote_plus(query)}&type=model\"\n",
    "    all_ids, seen_pages = [], set()\n",
    "\n",
    "    while url and url not in seen_pages:\n",
    "        seen_pages.add(url)\n",
    "        r = getter(url, timeout=30)\n",
    "        repo_ids = extract_repo_ids_from_fulltext_html(r.text)\n",
    "        all_ids.extend(repo_ids)\n",
    "        if VERBOSE:\n",
    "            print(f\"...{len(repo_ids)} on page; total {len(set(all_ids))}\")\n",
    "        url = find_next_page_url(url, r.text)\n",
    "\n",
    "    # de-dupe and sort\n",
    "    return sorted(set(all_ids))\n",
    "\n",
    "if MODE == \"api\":\n",
    "    models = get_models_api(QUERY, HF_TOKEN)\n",
    "else:\n",
    "    models = get_models_full_text(QUERY)\n",
    "\n",
    "print(f\"\\nFound {len(models)} models for '{QUERY}' in mode={MODE}\\n\")\n",
    "for mid in models:\n",
    "    print(mid)\n",
    "\n",
    "if SAVE_TO:\n",
    "    with open(SAVE_TO, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(models))\n",
    "    print(f\"\\nSaved to {SAVE_TO}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
