{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c926b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20069\n"
     ]
    }
   ],
   "source": [
    "#Getting started, making sure we're getting results\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "#Was getting only 68 results with 'open source' so trying 'open' and then filtering later\n",
    "models = api.list_models(\n",
    "    search=\"open\"\n",
    ")\n",
    "\n",
    "#See how many models we get back, we'll do further filtering down the road\n",
    "models = list(models)\n",
    "print(len(models))\n",
    "#20069 models!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a4826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28101319",
   "metadata": {},
   "source": [
    "# Grabbing Model Data\n",
    "- Name\n",
    "- ID\n",
    "- Model type\n",
    "- License\n",
    "    - Always prioritize LICENSE file\n",
    "    - Then YAML on Readme.md file\n",
    "- Date released\n",
    "- Date last modified\n",
    "- No. Downloads\n",
    "- Author\n",
    "- License file content\n",
    "- Readme file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c95337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retry] model_info mradermacher/openthoughts3_1k_llama3-GGUF -> 429; sleeping 169.1s (attempt 1/8)\n",
      "[retry] model_info OpenMed/OpenMed-NER-GenomeDetect-TinyMed-82M -> 429; sleeping 217.3s (attempt 1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retry] model_info mradermacher/openthoughts2_1k_32B-i1-GGUF -> 429; sleeping 221.2s (attempt 1/8)\n",
      "[retry] model_info fh1628/MNLP_M3_open_model_test -> 429; sleeping 220.4s (attempt 1/8)\n",
      "[retry] model_info open-unlearning/unlearn_tofu_Llama-3.2-1B-Instruct_forget10_RMU_lr1e-05_layer5_scoeff10_epoch5 -> 429; sleeping 221.4s (attempt 1/8)\n",
      "[retry] model_info DevQuasar/nvidia.OpenMath-Nemotron-14B-Kaggle-GGUF -> 429; sleeping 223.5s (attempt 1/8)\n",
      "[retry] model_info Godreign/gemma-2-2b-it-openvino-int8-model -> 429; sleeping 225.0s (attempt 1/8)\n",
      "[retry] model_info viethq5/Qwen2.5-1.5B-Open-R1-Distill -> 429; sleeping 220.0s (attempt 1/8)\n",
      "[retry] model_info caijanfeng/Qwen2.5-1.5B-Open-R1-Distill-repeat -> 429; sleeping 226.2s (attempt 1/8)\n",
      "[retry] model_info OzzyGT/opensketch -> 429; sleeping 224.2s (attempt 1/8)\n",
      "[retry] model_info furmaniak/openalex_pretrain_model_qwen2.5_vX -> 429; sleeping 246.4s (attempt 1/8)\n",
      "[retry] model_info oodeh/openshift-qe-r32-a16-epoch10-merged-model -> 429; sleeping 246.3s (attempt 1/8)\n",
      "[retry] model_info saran3026/OPENSLR_Whisper_small_TAMIL-5 -> 429; sleeping 248.4s (attempt 1/8)\n",
      "[retry] model_info LiteLLMs/OpenELM-1_1B-Instruct-GGUF -> 429; sleeping 245.3s (attempt 1/8)\n",
      "[retry] model_info ulkaa/starcoder2-3b-OpenVINO-asym-int8 -> 429; sleeping 246.1s (attempt 1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[retry] model_info houghtonweihu/openchat_gemma_adapter_1 -> 429; sleeping 243.5s (attempt 1/8)\n",
      "[retry] model_info huangyt/falcon-7b-Open-Platypus_2.5w-r16-query_key_value -> 429; sleeping 248.3s (attempt 1/8)\n",
      "[retry] model_info maddes8cht/openlm-research-open_llama_7b_v2-gguf -> 429; sleeping 240.4s (attempt 1/8)\n",
      "[retry] model_info madh34rt/open-reverse-proxy -> 429; sleeping 246.5s (attempt 1/8)\n",
      "[retry] model_info openclimatefix/power_perceiver -> 429; sleeping 243.5s (attempt 1/8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid model-index. Not loading eval results into CardData.\n",
      "Invalid model-index. Not loading eval results into CardData.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote 20070 rows to hf_models_open_test-2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from typing import Optional, List\n",
    "import re, time, random\n",
    "\n",
    "from huggingface_hub import HfApi, model_info\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "\n",
    "SEARCH_TERM = \"open\"\n",
    "OUT_CSV = \"hf_models_open_raw.csv\"\n",
    "HF_TOKEN = \"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\"  # optional: set your token in env for higher limits\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "def iso(dt) -> str:\n",
    "    if dt is None:\n",
    "        return \"\"\n",
    "    if isinstance(dt, str):\n",
    "        return dt\n",
    "    try:\n",
    "        return dt.isoformat()\n",
    "    except Exception:\n",
    "        return str(dt)\n",
    "\n",
    "def resolve_license_from_metadata(info, tags: Optional[List[str]] = None) -> str:\n",
    "    cd = getattr(info, \"cardData\", None)\n",
    "    if isinstance(cd, dict):\n",
    "        for key in (\"license\", \"licenses\", \"license_id\", \"license_name\"):\n",
    "            val = cd.get(key)\n",
    "            if val:\n",
    "                if isinstance(val, (list, tuple)):\n",
    "                    return \", \".join([str(x) for x in val if x]).strip()\n",
    "                return str(val).strip()\n",
    "    cfg = getattr(info, \"config\", None)\n",
    "    if isinstance(cfg, dict):\n",
    "        lic = cfg.get(\"license\")\n",
    "        if lic:\n",
    "            return str(lic).strip()\n",
    "    for t in (tags or []):\n",
    "        if isinstance(t, str) and t.lower().startswith(\"license:\"):\n",
    "            return t.split(\":\", 1)[1].strip()\n",
    "    return \"\"\n",
    "\n",
    "# -------- minimal rate-limit helpers --------\n",
    "def _status_code(exc) -> Optional[int]:\n",
    "    try:\n",
    "        return getattr(getattr(exc, \"response\", None), \"status_code\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _retry_after_seconds(exc) -> Optional[float]:\n",
    "    try:\n",
    "        hdrs = getattr(getattr(exc, \"response\", None), \"headers\", {}) or {}\n",
    "        ra = hdrs.get(\"Retry-After\")\n",
    "        return float(ra) if ra not in (None, \"\") else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _parse_rate_limit(headers: dict):\n",
    "    \"\"\"\n",
    "    RateLimit: \"api|pages|resolvers\";r=1234;t=108\n",
    "    RateLimit-Policy: \"fixed window\";\"api|pages|resolvers\";q=5000;w=300\n",
    "    -> returns (remaining:int|None, reset_sec:int|None)\n",
    "    \"\"\"\n",
    "    rl = headers.get(\"RateLimit\", \"\") or \"\"\n",
    "    rem = None\n",
    "    reset = None\n",
    "    m_r = re.search(r\"r=(\\d+)\", rl)\n",
    "    m_t = re.search(r\"t=(\\d+)\", rl)\n",
    "    if m_r: rem = int(m_r.group(1))\n",
    "    if m_t: reset = int(m_t.group(1))\n",
    "    return rem, reset\n",
    "\n",
    "def _sleep_from_headers(exc, attempt: int, max_backoff: float = 60.0) -> float:\n",
    "    # 1) Retry-After header wins\n",
    "    wait = _retry_after_seconds(exc)\n",
    "    if wait is not None:\n",
    "        return wait + random.uniform(0, 0.5)\n",
    "    # 2) RateLimit header t=seconds until reset\n",
    "    headers = getattr(getattr(exc, \"response\", None), \"headers\", {}) or {}\n",
    "    _, reset_sec = _parse_rate_limit(headers)\n",
    "    if reset_sec is not None:\n",
    "        return float(reset_sec) + random.uniform(0, 0.5)\n",
    "    # 3) fallback exponential backoff with jitter\n",
    "    return min(max_backoff, (2 ** attempt)) + random.uniform(0, 0.25)\n",
    "\n",
    "# Simple retry for model_info\n",
    "def safe_model_info(repo_id: str, token: Optional[str], max_retries: int = 8):\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return model_info(repo_id, token=token)\n",
    "        except HfHubHTTPError as e:\n",
    "            code = _status_code(e)\n",
    "            if code in (429, 502, 503, 504) and attempt < max_retries:\n",
    "                attempt += 1\n",
    "                wait = _sleep_from_headers(e, attempt)\n",
    "                print(f\"[retry] model_info {repo_id} -> {code}; sleeping {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# Resilient iterator for list_models (very simple)\n",
    "def iter_models_with_retry(api: HfApi, *, search: str, full: bool, sort: str, direction: int, max_retries: int = 8):\n",
    "    seen = set()\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            for m in api.list_models(search=search, full=full, sort=sort, direction=direction):\n",
    "                mid = getattr(m, \"modelId\", None)\n",
    "                if not mid or mid in seen:\n",
    "                    continue\n",
    "                seen.add(mid)\n",
    "                yield m\n",
    "            return\n",
    "        except HfHubHTTPError as e:\n",
    "            code = _status_code(e)\n",
    "            if code in (429, 502, 503, 504) and attempt < max_retries:\n",
    "                attempt += 1\n",
    "                wait = _sleep_from_headers(e, attempt)\n",
    "                print(f\"[retry] list_models -> {code}; sleeping {wait:.1f}s (attempt {attempt}/{max_retries})\")\n",
    "                time.sleep(wait)\n",
    "                # loop restarts and continues; 'seen' avoids duplicates\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"name\",\n",
    "    \"id\",\n",
    "    \"model_type\",\n",
    "    \"license\",\n",
    "    \"date_released\",\n",
    "    \"date_last_modified\",\n",
    "    \"downloads\",\n",
    "    \"author\",\n",
    "]\n",
    "\n",
    "count = 0\n",
    "written = 0\n",
    "\n",
    "with open(OUT_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for summary in iter_models_with_retry(api, search=SEARCH_TERM, full=True, sort=\"last_modified\", direction=-1):\n",
    "        mid = getattr(summary, \"modelId\", None)\n",
    "        if not mid:\n",
    "            continue\n",
    "        count += 1\n",
    "\n",
    "        try:\n",
    "            info = safe_model_info(mid, token=HF_TOKEN)\n",
    "        except HfHubHTTPError:\n",
    "            continue\n",
    "\n",
    "        repo_id = info.modelId\n",
    "        name = repo_id.split(\"/\")[-1] if \"/\" in repo_id else repo_id\n",
    "        author = getattr(info, \"author\", \"\") or (repo_id.split(\"/\")[0] if \"/\" in repo_id else \"\")\n",
    "        model_type = getattr(info, \"pipeline_tag\", \"\") or \"\"\n",
    "        license_val = resolve_license_from_metadata(info, tags=getattr(summary, \"tags\", None))\n",
    "        date_released = iso(getattr(info, \"created_at\", None))\n",
    "        date_last_modified = iso(getattr(info, \"lastModified\", None))\n",
    "        downloads = getattr(info, \"downloads\", \"\") or \"\"\n",
    "\n",
    "        row = {\n",
    "            \"name\": name,\n",
    "            \"id\": repo_id,\n",
    "            \"model_type\": model_type,\n",
    "            \"license\": license_val,\n",
    "            \"date_released\": date_released,\n",
    "            \"date_last_modified\": date_last_modified,\n",
    "            \"downloads\": downloads,\n",
    "            \"author\": author,\n",
    "        }\n",
    "        writer.writerow(row)\n",
    "        written += 1\n",
    "\n",
    "        # tiny, optional pacing to avoid hammering\n",
    "        if written % 500 == 0:\n",
    "            time.sleep(1.0)\n",
    "\n",
    "print(f\"Done. Wrote {written} rows to {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
